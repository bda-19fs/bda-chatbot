{"id": "3058626", "url": "https://de.wikipedia.org/wiki?curid=3058626", "title": "Blue Gene Watson", "text": "Blue Gene Watson\n\nBlue Gene Watson (BGW) ist ein Supercomputer von IBM, der sich im Thomas J. Watson Research Center in Yorktown Heights, New York, befindet. Blue Gene Watson war bei der erstmaligen LINPACK-Messung im Juni 2005 der zweitschnellste Computer auf der TOP500-Liste.\nSeine Leistung liegt bei 91,29 Teraflops. Die maximale Rechenleistung des Supercomputers beträgt etwa 114 Teraflops (peak).\n\nBlue Gene Watson besteht aus 20 Racks, deren Hardware der des BlueGene/L gleicht. Jedes dieser Racks beinhaltet 1024 Knoten. Jeder Knoten besteht aus zwei 700-MHz-\"Power-440\"-Prozessoren und 512 MB Arbeitsspeicher (RAM). Die kompletten 20 Racks sind in fünf Reihen mit jeweils vier Racks aufgeteilt.\n\nDie Benutzer interagieren mit BGW durch seine vier Front-End-Knoten, von denen jeder ein IBM-p655-Server ist, der mit SUSE Linux Enterprise Server 9 läuft.\n\nJedes dieser Racks hat 16 I/O-Knoten, die für die Daten-Ein- und Ausgabe verwendet werden. Die I/O-Knoten kommunizieren über eine interne Gigabit-Ethernet-Verbindung. Die primäre Datenablage besteht aus 60 Terabyte SAN-basierenden Festplatten, die das \"General Parallel File System\" nutzen. Zusätzlich wird eine 500 Terabyte große \"IBM 3494 Tape-Library\" verwendet, um die Daten von der Festplatte zu sichern.\n\nDie primäre Aufgabe des Blue Gene Watson sind wissenschaftliche Berechnungen, die auf schwächeren Rechnern nicht die gewünschten Erfolge zeigten, zu berechnen und auszuwerten. Das System läuft ununterbrochen, und führt in 90 % der Zeit wissenschaftliche Berechnungen (aufgeteilt in 40 % Proteinfaltung, 40 % andere biologische Simulationen, 20 % andere wissenschaftliche Berechnungen) durch, die restlichen 10 % werden für \"„zukunftsweisende Computerforschung“\" verwendet. In einer frühen Entwicklungsphase der ebenfalls Watson genannten Software zur Beantwortung von in natürlicher Sprache gestellten Fragen gab es Versuche, diese ebenfalls auf der Blue Gene-Architektur zu betreiben. Letztlich entschieden die Entwickler sich jedoch für das Serversystem Power 750, das mit leistungsstärkeren Prozessoren ausgestattet ist.\n\n"}
{"id": "3060418", "url": "https://de.wikipedia.org/wiki?curid=3060418", "title": "Windows Kontakte", "text": "Windows Kontakte\n\nWindows Kontakte ist das Nachfolgeprogramm des \"Windows-Addressbuchs (WAB)\" in Windows Vista.\n\nEs interagiert mit Windows Mail. Das Programm benutzt ein neues XML-basiertes Schemaformat, in dem jeder Kontakt als eigene *.contact-Datei angelegt wird. Es speichert allgemeine Informationen des Kontaktes; jedem Kontakt kann auch ein Bild zugeordnet werden. Für die Zusammenarbeit mit anderen Programmen und das Speichern der Informationen existieren APIs. Das alte codice_1-Format und die offenen Standards codice_2 (vCard) und codice_3 (CSV) werden ebenfalls unterstützt. Windows Kontakte bietet die meisten Funktionen des Windows-Adressbuchs.\n\n\n"}
{"id": "3061126", "url": "https://de.wikipedia.org/wiki?curid=3061126", "title": "Nibble (Zeitschrift)", "text": "Nibble (Zeitschrift)\n\nNibble war eine US-amerikanische Computerzeitschrift für den Apple-II-Computer. Hauptthema des Magazins war die Programmierung des Rechners. Der Zeitschriftentitel bedeutet \"halbes Byte\" oder \"vier Bits\", der Slogan lautete \"The Magazine for Apple II Enthusiasts\". Die meisten Artikel von Nibble enthielten das Listing eines Programms, das entweder eine Anwendung, ein Utility für den Rechner oder ein Spiel war. Daneben beschrieb der Artikel die genaue Funktionsweise des Programms.\n\nDie Zeitschrift wurde erstmals im Januar 1980 von Mike Harvey herausgebracht. Ursprünglich wurde sie achtmal jährlich veröffentlicht. Ab 1984 war das Magazin so erfolgreich, dass der Verlag es sich erlauben konnte, die Zeitschrift monatlich zu veröffentlichen. Sie wurde für weitere zwölf Jahre herausgebracht und die letzte Ausgabe erschien im Juli 1992. Eine Erweiterung der Zeitschrift war die Macintosh-Publikation \"Nibble Mac\", welche anfangs ein Teil der Original-Zeitschrift war und später separat veröffentlicht wurde. Die meisten Ausgaben der Zeitschrift, inklusive der Ausgaben \"Nibble Mac\", sind mittlerweile über die Webseite des Verlegers abrufbar.\n\n"}
{"id": "3067557", "url": "https://de.wikipedia.org/wiki?curid=3067557", "title": "PCLinuxOS", "text": "PCLinuxOS\n\nPCLinuxOS – kurz PCLOS – ist eine US-amerikanische Linux-Distribution, die ursprünglich auf Mandrake zurückgeht und vor allem für den Desktop-Einsatz zugeschnitten ist. Sie gehörte laut einer Umfrage aus dem Jahr 2007 zu den zehn meistgenutzten Distributionen und wird weiterhin auf der Vergleichsseite Distrowatch als eine der Hauptdistributionen aufgeführt.\n\nLaut verschiedener Tests zeichnet sich PCLinuxOS durch seine Einfachheit und Benutzerfreundlichkeit sowie die überdurchschnittlich gute Hardwareerkennung aus. Von Heimanwendern oft benutzte Software wie Codecs für Multimediadateien, Flash und Java sind bei PCLinuxOS vorinstalliert. PCLinuxOS kann mit verschiedenen Desktop-Umgebungen installiert werden, die auf die Bedürfnisse und vorhandene Hardware der Nutzer zugeschnitten sind.\n\nBill Reynolds (in der Entwicklerszene bekannt als \"Texstar\"), heute der Hauptentwickler von PCLinuxOS, bot ab 2000 auf einer eigenen Quelle namens \"PCLinuxOnline\" selbstkompilierte RPM-Pakete für Mandrake Linux an, um diese Distribution zu verbessern.\n\n2003 entschied sich Reynolds aus Unzufriedenheit mit der Politik von Mandrake einen Fork der Version 9.2 von Mandrake zu erstellen, die er PCLinuxOS nannte. Die ersten Versionen, die als Previews bezeichnet wurden, ähnelten noch stark dem Original, seitdem hat sich die neue Distribution jedoch immer weiter vom Mandriva Linux entfernt. Der Großteil der Pakete, zum Beispiel der Kernel, die GNU Compiler Collection oder die KDE Software Compilation 4 wurden unabhängig von Mandriva kompiliert, daher hat PCLinuxOS auch sein eigenes Repository. Des Weiteren hat PCLinuxOS eigene Grafiken und Icons sowie eine eigene Menü-Struktur.\n\nDie Version PCLinuxOS 2007 mit der internen Versionsnummer .94, deren stabiles Release am 21. Mai 2007 veröffentlicht wurde, war eine vollkommene Neugestaltung des Systems, weshalb ein normales Upgrade von früheren Versionen nicht möglich war. Zudem wurden kosmetische Verbesserungen, wie ein neues Logo und eingebaute 3D-Desktopeffekte via Beryl und Compiz eingefügt. Alle weiteren Versionen sollen auf .94 aufbauen, da seitdem das Entwicklungsmodell eines \"Rolling Release\", also einer kontinuierlichen Weiterentwicklung aller in der Distribution enthaltenen Programme, eingeführt wurde.\n\nAnfang November 2007 wurde die erste größere Bearbeitung der 2007er Version vorgenommen, die unter anderem einen neuen Kernel beinhaltete. Im Januar 2008 wurde eine neue \"MiniMe\"-Version veröffentlicht, die nur die wichtigsten Basis-Programme enthält und etwa 300 MB groß ist.\n\nDie am 11. März 2009 erschienene Version 2009.1 basierte weiterhin auf K Desktop Environment 3, da die Entwickler eine vergleichbare Funktionalität mit KDE Plasma Desktop 4 nicht erreicht hatten. Die Version basiert weiterhin auf der Codebasis der Version 2007, ist also von dieser aus per Paketverwaltung aktualisierbar.\n\nAm 5. Mai 2010 wurde die Version 2010.1 mit KDE Plasma Desktop 4 veröffentlicht. Alle nachfolgenden Veröffentlichungen sind Schnappschüsse der aktualisierten Version 2010.1.\n\nPCLinuxOS erscheint in den aktuellen Versionen 2016.03 ausschließlich als 64-Bit-Ausführung und kommt mit Kernel 4.4.4 LTS, grub2 sowie voller UEFI-Unterstützung.\n\nPCLinuxOS ist eine für den einfachen Desktop-Gebrauch gedachte Linux-Distribution, während der Server-Einsatz eine eher geringe Rolle spielt. Die Live-CD bietet die Möglichkeit persönliche Dateien und Konfigurationsdateien auf einen USB-Stick abzulegen. Das System kann aber auch auf eine Festplatte installiert werden; die CD enthält, wenn dekomprimiert, etwa zwei Gigabyte an Software, wie KDE Software Compilation 4, LibreOffice, GIMP und viele weitere Applikationen.\n\nDie Distribution zählt zu den ersten, die das von Debian stammende Paketmanagement-Programm APT in Verbindung mit dem RPM-Paketsystem in der Variante APT-RPM und dessen grafischem Aufsatz Synaptic nutzt.\n\nPCLinuxOS legt großen Wert auf Kompatibilität, so sind zum Beispiel NDISwrapper oder Codecs für zahlreiche proprietäre Multimediaformate (z. B. MPEG) vorinstalliert, was bei anderen Distributionen aufgrund der daraus resultierenden unklaren Rechtslage unterlassen wird. Das umstrittene Paket libdvdcss ist zwar nicht vorinstalliert, aber im offiziellen Repository vorhanden.\n\nWeiterhin ist in PCLinuxOS das Skript \"mklivecd\" integriert, das dem Benutzer erlaubt, einen “Schnappschuss” von seiner Installation zu machen (mit allen Einstellungen, Programmen, Lesezeichen, Dokumenten, Bildern, etc.) der dann zu einem ISO-CD-Image komprimiert wird. Dadurch ist es sehr einfach, Backups zu machen sowie seine eigene PCLinuxOS-basierte Distribution zu erstellen.\n\nAb der Version 2007 nutzt die Distribution ein \"Rolling-Release\"-Versionssystem. Upgrades erfolgen über die Paketverwaltung und es gibt keinen Feature-Freeze, von allen Programmen wird also möglichst die neueste Version angeboten.\n\nUm die Installation von PCLinuxOS auf Festplatte zu erleichtern, bietet die deutsche PCLinuxOS-Gemeinschaft eine bebilderte Installationsanleitung an. Des Weiteren kann PCLinuxOS nach erfolgreicher Installation mit dem sogenannten Lokalisierungs-Manager (addlocale) in eine von 85 Sprachen mit nur wenigen Mausklicks komplett umgestellt werden, unter anderem auch auf \"Deutsch\".\n\nFür die deutschsprachigen Nutzer steht außerdem eine inoffizielle Version mit der Desktop-Umgebung LXDE, die bereits komplett auf Deutsch umgestellt wurde, zum Download bereit.\n\nPCLinuxOS ist mit verschiedenen Desktop-Umgebungen erhältlich, hierzu zählen:\n\n\nMATE ist eine beliebte Multi-Plattform-Desktop-Umgebung und hat sich zum Ziel gesetzt, das klassische Desktop-Konzept von GNOME 2 fortzuführen. Der Schwerpunkt von MATE liegt in der Benutzerfreundlichkeit und der Stabilität. MATE bietet alles was der Computer-Anwender von einer modernen IT-Umgebung erwartet.\n\n\nKDE ist eine beliebte Desktop-Umgebung für den Computer und hat eine große Ähnlichkeit mit dem Windows-Betriebssystem. Der KDE-Schwerpunkt ist die Benutzerfreundlichkeit, Stabilität und Barrierefreiheit und bietet alle gängigen Programme die Computer-Anwender von einer modernen IT-Umgebung erwarten, wie beispielsweise E-Mail, Web-Browsing, Dateiverwaltung, Multimedia und Spiele.\n\n\nKDE FullMonty basiert auf der KDE-Umgebung und verfolgt ein neues Konzept. Es bietet sechs vorkonfigurierte virtuelle Arbeitsoberflächen, die jeweils Software für eine spezifische Aufgabe bereithalten. So wechselt man per Mausklick zwischen einer Auswahl von Büroprogrammen und einem Desktop, der für Aufgaben im Netz optimiert ist.\nDieses Desktop-Layout wurde entworfen um den täglichen Anforderungen und Bedürfnissen der Anwender gerecht zu werden und diese zu erleichtern.\n\"FullMonty-Desktop\" enthält eine Vielzahl zusätzlicher Programme und Codecs.\n\nIn den Foren der PCLinuxOS-Communitys werden auch inoffizielle Derivate – die mit dem Skript \"mklivecd\" produziert wurden – zum Download bereitgestellt. Bei diesen Derivaten sind unter anderem aktuelle Updates bereits eingespielt oder es werden weitere Desktop-Umgebungen angeboten.\n\nFerner gibt es auch abgespeckte Varianten der Distribution, sie richten sich an fortgeschrittene Benutzer, die die Software-Vorgaben der Standardvariante nicht wünschen und stattdessen ihr System weitestgehend selbst einrichten und konfigurieren möchten.\n\nIm Unterschied zum Original setzen einige dieser Derivate auf Desktop-Umgebungen die eher für Cloud-Computer mit geringen Hardware-Spezifikationen, Netbooks oder ältere Computer konzipiert sind, wie zum Beispiel: Xfce, LXDE, LXQt und Trinity.\n\nIm Unterschied zu anderen größeren Linux-Distributionen wird PCLinuxOS von einer unabhängigen Entwicklergemeinde ohne nennenswerte kommerzielle Interessen vertrieben, neben Bill Reynolds ist dies vor allem eine als \"Ripper-Gang\" bekannte Programmierergruppe.\n\nAls Nachteil des Rolling-Release-Versionssystems wird genannt, dass die Release-Zyklen unregelmäßig erfolgen und vergleichsweise viel Zeit zwischen den einzelnen Versionen vergeht.\n\nAußerdem existiert bei der Installationsroutine noch keine Mehrsprachen-Unterstützung.\n\n"}
{"id": "3069705", "url": "https://de.wikipedia.org/wiki?curid=3069705", "title": "Power Macintosh G3 Tower", "text": "Power Macintosh G3 Tower\n\nDer Power Macintosh G3 Tower („Beige“, Codename „Gossamer“) ist ein Personal Computer des Unternehmens Apple und gehört zur Power-Macintosh-Serie.\nDer im November 1997 eingeführte PowerMacintosh G3 war zur Zeit seines Erscheinens der schnellste Mac. Er war mit dem neu entwickelten PPC-750-Prozessor ausgestattet, der in Zusammenarbeit von Motorola mit IBM entwickelt worden war. Der 750er war der erste Prozessor mit Level-II-Cache, was die Geschwindigkeit des Datentransfers deutlich steigerte.\n\nVom beigen Power Mac G3 gab es die Revisionen A, B und C, die sich im System-ROM und im Grafik-Chip unterscheiden. Erst ab der Revision B war an der IDE-Schnittstelle ein Master/Slave-Betrieb (bedingt durch das neuere ROM) möglich, d. h. es wurden vier statt vorher zwei Geräte unterstützt. Revision A hatte einen ATI Rage II Grafik-Chip, ab Revision B wurde ein Rage Pro mit 3D-Beschleunigung eingebaut. Die Revision B hat eine fehlkonstruierte „Personality Card“, was dazu führt, dass das Bild am Röhrenmonitor dunkel und unscharf ist.\n\nDie drei Revisionen lassen sich durch die Endung der ROM-Versionsnummer unterscheiden, ablesbar z. B. im Dienstprogramm „Apple System Profiler“ unter dem Reiter „Systemprofil“ – Produktionsinformation:\n\nDa 1997/1998 Iomega-ZIP-Laufwerke sehr gefragt waren, konnte der linke Schacht neben dem CD-Laufwerk mit einem internen ZIP100-Laufwerk bestückt werden. Bei der Revision A war das ZIP-Laufwerk mittels SCSI (ID 5), bei den darauffolgenden Revisionen per IDE-Bus angebunden (dadurch bedingt, dass in der Revision A die beiden zur Verfügung stehenden IDE-Kanäle durch die System-Festplatte und das CD-ROM-Laufwerk bereits belegt waren).\n\n1999/2000 war es dann erstmals möglich, die bis dahin nur für Windows-Rechner konzipierte Voodoo-Grafikkarte „Voodoo3 2000“ der Firma 3dfx Interactive Inc. in den G3 einzubauen. Diese Karten galten in der zweiten Hälfte der 1990er Jahre als legendär in der Auflösung und waren auf Spiele spezialisiert.\n\nDie IDE-Schnittstellen unterstützen den Transfermodus \"Multiword-DMA\" mit 16 Megabytes/s und eine maximale Festplattengröße von 128 GiByte, entsprechend 137 GByte. Der G3 als sogenannter „Old-World-Mac“ erfüllt alle Vorgaben für den Betrieb von Mac OS X 10.2 „Jaguar“, jedoch unter der Voraussetzung, dass mindestens 128 MB Arbeitsspeicher zur Verfügung stehen. Die neueste noch offiziell unterstützte Betriebssystem-Version ist 10.2.8. Mac OS 10.3 und auch Mac OS X Tiger können jedoch mit Hilfe der Shareware \"XPostFacto\" installiert werden. Mac OS X Leopard kann aufgrund des fehlenden G4-Prozessors nicht mehr installiert werden.\n\nDas Gehäuse ist sehr gut zugänglich und die diversen Steckplätze ermöglichen ein vielfältiges Aufrüsten.\n\n\"Apple Power Macintosh G3 Tower („Beige“)\"\n\n"}
{"id": "3075591", "url": "https://de.wikipedia.org/wiki?curid=3075591", "title": "System-Locked Preinstallation", "text": "System-Locked Preinstallation\n\nSystem-Locked Preinstallation (SLP) bezeichnet eine Form der Windows-Installation durch einen OEM-Computerhersteller, bei der für den Endkunden die sonst seit Windows XP obligatorische Produktaktivierung entfällt.\n\nEine SLP-Installation von Windows sucht beim Start nach bestimmten Informationen im BIOS des Computersystems, die den jeweiligen OEM-Hersteller identifizieren. Werden diese Informationen gefunden, wird Windows automatisch aktiviert. Fehlen die entsprechenden Daten, so verhält sich Windows wie eine normale Retail-Version und muss innerhalb von 30 Tagen bei Microsoft aktiviert werden.\n\nAufgrund dieser Tatsache ist es möglich, auf OEM-Systemen mit SLP-Installationen von Windows praktisch alle Hardwarekomponenten zu wechseln, ohne dadurch den Aktivierungsstatus zu verlieren. Selbst ein Austausch der Hauptplatine ist möglich, solange der Ersatz vom jeweiligen OEM-Hersteller stammt und die notwendigen Daten im BIOS enthält.\n\n"}
{"id": "3077040", "url": "https://de.wikipedia.org/wiki?curid=3077040", "title": "MACup", "text": "MACup\n\nMACup war eine Computerzeitschrift der Verlagsgruppe Ebner Ulm, die vom Januar 1985 bis 1. Juni 2011 monatlich im Verlag der Neue Mediengesellschaft Ulm mbH erschien.\n\nIhren redaktionellen Fokus legte die MACup auf die Themen Medienproduktion, Grafik, DTP, PDF, Video, Web, Audio sowie Animation und richtete sich demnach vorwiegend an Macintosh-Anwender, Grafik-Designer und Fachleute aus dem Verlagswesen (Publishing Professionals), aber auch an Musiker und Film-/Video-Produzenten. Dem Heft lag i. d. R. eine CD oder DVD mit Software, Workshops, Tutorials, Demos usw. bei.\n\nDie Zeitschrift wurde 1984 gegründet und war somit nach eigener Aussage\n„Europas erstes Magazin für Apple-Anwender“. Im Mai 2001 fusionierten MACup und das im gleichen Verlag herausgegebene macmagazin.\nBeide Magazine beschäftigten sich mit Hard- und Softwareentwicklungen rund um den Mac.\n\nDie Publikation hatte zuletzt mit drastisch sinkenden Verkaufszahlen zu kämpfen. Im ersten Quartal 2011 lag die durchschnittliche monatliche (verbreitete) Auflage nach IVW bei 17.603 Exemplaren. Noch im zweiten Quartal 2010 lag die durchschnittliche monatliche (verbreitete) Auflage nach IVW bei 21.382 Exemplaren. Im Vergleichsquartal des Vorvorjahres waren es nach IVW 21.888 Exemplare.\n\nIm Jahr 2010 war die MACup noch Deutschlands drittgrößte Mac-Zeitschrift nach der Macwelt (durchschnittliche monatliche Auflage im zweiten Quartal 2010 nach IVW: 37.206) und nach der Mac Life (durchschnittliche monatliche Auflage im zweiten Quartal 2010 nach IVW: 34.252). Der letzte Heftpreis lag bei 5,90 € in Deutschland.\n"}
{"id": "3081453", "url": "https://de.wikipedia.org/wiki?curid=3081453", "title": "Aptosid", "text": "Aptosid\n\nAptosid [] (Eigenschreibweise \"aptosid\", bis September 2010 bekannt unter dem Namen Sidux [], Eigenschreibweise \"sidux\") war eine auf Debian unstable aufbauende, nicht kommerzielle Linux-Distribution für Desktop-Computer und Notebooks.\n\nSie ist als Live-CD für die Architekturen i686 und AMD64 verfügbar und lässt sich mit einem grafischen Installationsassistenten installieren. Die Distribution wird in der Fachpresse vor allem aufgrund ihrer Aktualität hervorgehoben.\nNeben der vollständigen Fassung, die alle Komponenten umfasst, gibt es eine „Light“-Variante mit verringertem Paketumfang. Die Standard-Desktop-Umgebung von Aptosid ist KDE.\n\nBesonderer Wert wird bei der Entwicklung von Aptosid auf Hardwareerkennung und Systemgeschwindigkeit gelegt. Die Distribution ist vollständig kompatibel zu Debian/unstable „sid“, so dass sämtliche Pakete des Debian-Repositories nutzbar sind. Aufgrund der Optimierung auf i686 und Ausrichtung auf aktuelle Hardware (AMD ab K7, VIA ab C7) ist eine Verwendung mit älteren Prozessoren (zum Beispiel bis AMD K6 oder VIA C3) nicht ohne weiteres möglich, da diese den Befehlssatz nicht voll unterstützen.\n\nAptosid enthält ausschließlich DFSG-konforme Open-Source-Software. Aptosid stellt für verschiedene Anwendungsbereiche Skripte und Meta-Pakete zur gebündelten Installation der benötigten Software oder auch von unfreien Paketen bereit. Die Aptosid Live-CD beinhaltet die Sprachen Englisch und Deutsch; weitere Sprachpakete können nachinstalliert werden. Die DVD beinhaltet jedoch alle unterstützten Sprachen.\n\nDurch die Kopplung an Debian/unstable ist die Distribution immer auf einem aktuellen Stand, man spricht dabei auch von einem Rolling Release; dies geht jedoch teilweise auf Kosten der Stabilität. Die Live-CD ist jeweils ein Snapshot des aktuellen Standes von Aptosid zum Zeitpunkt der Veröffentlichung der jeweils aktuellen Ausgabe (ausgenommen die LiveCD-spezifischen Elemente, wie den Installer). Für die Aktualisierung des Systems nutzt Aptosid das apt-Werkzeug. Das ursprünglich von Kanotix stammende und bis zum Release 2008-01 über einen sogenannten „Stub-Installer“ in Sidux integrierbare eigenständige Skript \"smxi\", das mittels einer Sperrliste die in Debian/unstable vorhandenen fehlerhaften Pakete von der Installation ausschließt, ist nicht Teil der Distribution.\n\nDer Namensbestandteil \"sid\" verweist auf den technischen Ursprung „Sid“ (\"still in development\"), wie Debian/unstable intern genannt wird. Sidux folgte mit seiner Auslautung auf ux der Tradition anderer Unix-Derivate. Aptosid thematisiert einerseits \"apt\" als den Namen der für die Durchführung des \"Rolling-Release\" verwendeten Debian-Paketverwaltung. Weitere beabsichtigte Interpretationen entstammen dem lateinischen Ausdruck \"apto\" von \"aptus\" = \"passend (gemacht)\", sowie dem griechischen Ausdruck \"απ'το\" (\"ap'to\") = \"(kommend) von\". Die Bezeichnungen der einzelnen veröffentlichten Versionen sind der griechischen Mythologie entlehnt und werden durch das Jahr und eine jährliche Versionsnummer ergänzt.\n\nSidux führte die ehemalige Konzeption von Kanotix fort und ist aus dieser Distribution hervorgegangen, nachdem sich deren Entwickler Jörg Schirottke einer stärker kommerziell orientierten Linie zuliebe von Debian/unstable als Basis abgewandt hatte. Damit übernahm Sidux die bisherige Funktion von Kanotix innerhalb der Linux-Distributionen und war die einzige Distribution, welche unmittelbar und ausschließlich auf Debians Entwicklerzweig „Sid“ basiert. Diese Tradition gilt nach wie vor für Aptosid. Gegründet wurde Sidux am 24. November 2006 durch frühere Kanotix-Entwickler.\n\nSeit dem 26. November 2006, lange bevor Sidux als brennfähiges ISO-Abbild für CD und DVD veröffentlicht wurde, konnte es als funktionsfähiges Betriebssystem auf dem PC installiert werden, indem eine vorhergehende Kanotix-Installation per Script mit Quellen aus dem Repository zu Sidux aktualisiert wurde. Die erste Sidux Live-CD mit dem Namen „Χάος“ (chaos) und der Bezeichnung 2007-01 wurde offiziell am 22. Februar 2007 veröffentlicht.\n\nIm Februar 2007 wurde Sidux von Distrowatch ausgezeichnet und erhielt im Rahmen dessen eine Spende über 350 $.\n\nIn den folgenden Monaten war Sidux immer wieder Inhalt von Tests durch Linux-Sonderhefte von PC-Zeitschriften, so eines Tests im Dezember 2007 durch die Chip, in dem der Kompromiss aus Aktualität und Stabilität erklärt wird. Die \"PC-Welt Linux 2/2008\" beinhaltet Sidux neben diversen anderen Linux-Distributionen auf der DVD. \"Linux Life\", ein Ableger des PC Magazins, hat in Ausgabe 02/08 ebenfalls einen Bericht.\n\nSeit dem 9. Dezember 2009 ist Sidux ein der SPI angeschlossenes Projekt.\n\nNachdem es zu Streit zwischen dem Sidux e.V. und den Entwicklern kam, wurde in einer Mitgliederversammlung am 14. August 2010 eine Trennung zwischen Entwicklung und Verein beschlossen. Das Entwicklungsteam entwickelt unabhängig davon die Distribution weiter. Um Markenrechtsstreitigkeiten zwischen Entwicklungsteam und Verein aus dem Weg zu gehen, wurde die Distribution in Aptosid umbenannt. Nach einem knappen Monat Überarbeitungszeit wurde Aptosid am 11. September 2010 veröffentlicht.\n\nWegen Differenzen im Umgang mit der Nutzergemeinde entstand im Sommer 2011 ein Fork von aptosid, der sich Siduction nennt. Dieser bleibt ebenso ein Derivat von Debian Sid, möchte aber die Endbenutzer stärker in die Entwicklung einbinden als aptosid es tut.\n\nZur Unterstützung und Finanzierung der Distribution wurde am 1. April 2007 der gemeinnützige Verein „Sidux e. V.“ mit Sitz in Berlin gegründet. Die Satzung des Vereins definiert die Ziele, die über die ausschließliche Förderung und Verbreitung von Sidux hinausgehen. Nach der Trennung von Verein und Entwickler-Community plant der Verein, zukünftig allgemein freie Software zu fördern.\n\nBis ins Jahr 2010 wurde planmäßig jeweils etwa alle drei Monate eine neue Version veröffentlicht.\nEs gab üblicherweise 4 verschiedene CDs pro Version, jedoch werden aus Platzgründen ab Version 2007-04.5 eine voll ausgestattete DVD und zwei CDs angeboten.\nSeit 2011 gibt es keinen Zeitplan mehr für neue Versionen. Zumindest erscheint seither einmal jährlich eine aktualisierte Fassung. Seit 2013 wurde keine neue Version mehr veröffentlicht, somit scheint diese Distribution nicht mehr weiterentwickelt zu werden.\n\nWie bei vielen Linux-Distribution existieren auch von Sidux Abwandlungen. Hierbei handelt es sich in diesem Falle vor allem um Versionen mit einer anderen Lokalisation, zum einen \"celtux\" mit einer irischen (gälischen) Lokalisation, zum anderen \"Deb-on-Air\" mit einer französischen.\n\nEin weiteres Derivat ist \"Seminarix\", welches in Version 1.0 noch auf Kubuntu basierte, seit \"sidux-2008.1-seminarix\" jetzt aber aus Geschwindigkeitsgründen auf Sidux basiert. Diese Distribution wurde vom Studienseminar Neuss für die Lehrerausbildung Sekundarstufe I und II entwickelt und speziell an die Anforderungen im Bildungsbereich angepasst. Anders als bei Edubuntu wurde das Betriebssystem nicht „verniedlicht“. Im Vordergrund stand eine gute Mischung zwischen Lernsoftware für alle Fächer mit wichtiger Büro- und Graphiksoftware zu schaffen. Sidux-Seminarix unterstützt in einer speziellen Version auch ältere Hardware (zum Beispiel Prozessoren mit Befehlssatz vor i686).\n\n\n"}
{"id": "3084048", "url": "https://de.wikipedia.org/wiki?curid=3084048", "title": "S3 Texture Compression", "text": "S3 Texture Compression\n\nS3 Texture Compression (S3TC, manchmal auch DXTn oder DXTC) ist ein ursprünglich für die Savage 3D entwickeltes Texturkomprimierungssystem von S3 Graphics. Es eignet sich im Gegensatz zu Bildkompressionsalgorithmen wie JPEG für hardwarebeschleunigte Computergrafik, da es eine feste Datenkompressionsrate besitzt und nur einen Speicherzugriff pro Texel benötigt. Durch die Aufnahme in DirectX 6.0 wurde S3TC schnell herstellerübergreifend akzeptiert und ist heute der vorherrschende Standard. Die DXT-Formate werden in OpenGL als Extension unterstützt.\n\nS3TC besteht aus fünf Formaten, die nach den ihnen in DirectX zugewiesenen FourCC-Identifikation als DXT1 bis DXT5 benannt wurden und sich in der Handhabung des Alphakanals unterscheiden. DXT2 und DXT4 werden kaum verwendet und sind im Gegensatz zu den anderen drei Formaten auch nicht Teil der OpenGL-Erweiterung für S3TC.\n\nWie jeder andere verlustbehaftete Kompressionsalgorithmus versucht S3TC sichtbare Artefakte trotz hoher Datenpackrate zu minimieren. So kann bei gleichem Speicherbedarf eine Textur mit deutlich höherer Auflösung verwendet werden, was insgesamt zu einem besseren Ergebnis führt. Wie die meisten modernen Algorithmen zur Bildkompression legt S3TC nur fest, wie die Daten entpackt werden und lässt damit Spielraum für verschiedene Ansätze bei der Kompression. Trotzdem fallen Lizenzgebühren an, da die grundlegende Implementation von einem Patent abgedeckt wird.\n\nMit Direct3D 10 wurden die fünf DXT-Stufen als veraltet (\"deprecated\") eingestuft. Der Unterschied zwischen vorher und nachher multiplizierten Alpha-Werten wird nicht mehr gemacht. Aus DXT1 wird BC1, aus DXT2 und DXT3 wird BC2, aus DXT4 und DXT5 wird BC3.\n\nInsgesamt wurden fünf Algorithmen entwickelt, die auf demselben Prinzip basieren, aber für verschiedene Typen von Bilddaten ausgelegt sind. Die Textur wird zunächst in 4×4-Texel-Blöcke zerlegt. Aus den 16 Farbwerten werden zwei 16 Bit RGB-565-Farben berechnet. Die einzelnen Algorithmen berechnen aus diesen beiden weitere Farbwerte und speichern sie in einer Lookup-Tabelle. Wie in einer Grafik mit Farbpalette wird für jedes Texel nur der Index des am besten passenden Farbwertes in der Tabelle gespeichert und nicht die Farbe des Texels selbst. Die Einträge dort können mit nur wenigen Bits adressiert werden, so dass sich bei 32 Bit RGBA-Texturen Kompressionsraten von 8:1 bei DXT1 und 4:1 bei allen anderen Codecs ergeben.\n\nFür jeden 4×4 Textur-Block werden neben den beiden 16-Bit-Farbwerten pro Texel ein 2-Bit-Index berechnet. Insgesamt werden also 64 Bit pro Block benötigt und die Lookup-Tabelle kann maximal 4 Einträge enthalten. \n\nFalls der erste Farbwert größer als der zweite ist, lauten die beiden Anderen:\n\nAnsonsten gilt:\n\nDXT1 komprimiert im Vergleich zu den anderen Algorithmen doppelt so stark, da hier keine Alpha-Werte gespeichert werden.\n\nDXT3 komprimiert die Farbwerte wie DXT1, jedoch wird nicht zwischen den beiden Schemata unterschieden, sondern immer die erste Variante mit vier opaken Farben verwendet, Transparenz wird durch einen zusätzlichen 4-Bit-Alphakanal ermöglicht. Insgesamt werden 128 Bit pro Block benötigt. DXT3 eignet sich vor allem für Texturen mit harten Übergängen zwischen transparenten und opaken Bereichen.\n\nDXT5 komprimiert die Farbwerte wie DXT1. Es werden zwei 8-Bit-Alphawerte gespeichert, sowie pro Pixel ein 3-Bit-Wert, der zum Interpolieren zwischen den Alphawerten dient. Falls der erste Alphawert größer als der zweite ist, werden durch lineares Interpolieren 8 Alphawerte erhalten. Ansonsten werden nur 6 Alphawerte durch Interpolieren erzeugt, während die anderen beiden 0 und 1 sind. Es werden 128 Bit pro Block benötigt.\n\nIn der Praxis haben sich vor allem Cartoon-artige Zeichnungen und Normal Maps als problematisch erwiesen. Der von ATI entwickelte 3Dc-Algorithmus setzt auf S3TC auf, komprimiert Normals Maps aber deutlich effizienter. Auch Id Software beschäftigte sich bei der Entwicklung von Doom 3 mit dem Thema. Die Programmierer umgingen das Problem zumindest teilweise, indem sie den roten Farbkanal mit dem Alpha-Kanal vor und nach der Kompression vertauschten.\n\nUm S3TC mit Mesa 3D nutzen zu können, muss die Bibliothek \"libtxc_dxtn\" installiert sein.\n\n"}
{"id": "3084099", "url": "https://de.wikipedia.org/wiki?curid=3084099", "title": "Texturkomprimierung", "text": "Texturkomprimierung\n\nTexturkomprimierung wird in der Computergrafik verwendet, um den Speicher- und Bandbreitenbedarf von Texturen zu senken.\n\nDie Anforderungen an Texturkomprimierungssysteme unterscheiden sich von denen an gewöhnliche Bildkomprimierung, da schneller Zugriff auf zufällige Texel nötig ist:\n\n\nIn der Anfangszeit wurde Vektorquantisierung, insbesondere Indizierte Farben eingesetzt, das aber wegen der dabei auftretenden Indirektion durch andere Verfahren verdrängt wurde. Indizierte Farben wurden noch lange im Bereich eingebetteter Systeme eingesetzt und waren Bestandteil von OpenGL ES 1.x.\n\nAm weitesten verbreitet auf dem Desktop ist die in DirectX aufgenommene und in OpenGL über eine Extension unterstützte S3 Texture Compression (S3TC).\n\nBei Highend-Smartphones ist das von Ericsson entwickelte ETC verbreitet.\n\nFXT1 war ein Texturkomprimierungsystem von 3Dfx. Es wird heute nur noch von Intel unterstützt.\n\n"}
{"id": "3089146", "url": "https://de.wikipedia.org/wiki?curid=3089146", "title": "Mikes neues Auto", "text": "Mikes neues Auto\n\nMikes neues Auto ist ein computeranimierter Kurzfilm der Pixar Animation Studios aus dem Jahr 2002. Der Film wurde im Kino nach dem Hauptfilm \"Die Monster AG\" gezeigt. Er zeigt dieselben Hauptfiguren: Mike Glotzkowski und Sulley. Der Kurzfilm wurde unter anderem auch auf der DVD zu \"Die Monster AG\" veröffentlicht.\n\nMike hat sich ein neues Auto gekauft und präsentiert es Sulley. Doch der ist nicht sehr beeindruckt. Nach dem Einsteigen provoziert er durch intensives Spielen mit der außerordentlich flexiblen Sitzverstellung einen ersten Wutanfall von Mike. Nach dem Anlassen des Motors hat Mike, im Gegensatz zu Sulley, Probleme mit dem Gurt, fällt beim Ziehen daran aus dem Auto heraus und sperrt sich aus. Sulley öffnet dann versehentlich per Knopfdruck die Motorhaube und klemmt Mike beim Schließen selbiger die Finger ein. Nachdem Sully die Motorhaube wieder per Knopfdruck geöffnet hat, fällt Mike auf den Motor, bekommt diverse Stromschläge und die Motorhaube fällt wieder zu und sperrt Mike im Motorraum ein. Sulley gelingt es, die Motorhaube wieder zu öffnen, sodass Mike den Motorraum verlassen und die Motorhaube wieder von außen schließen kann. \n\nMikes Stimmung hat sich aufgrund dieser Vorkommnisse nicht verbessert. Beim anschließenden Versuch loszufahren aktiviert Mike versehentlich diverse Servicefunktionen gleichzeitig, u. a. Scheibenwischer, Radio, Gebläse und Sitzverstellung. Insbesondere letztere demonstriert nun eindrucksvoll ihre enorme Leistungsfähigkeit und es gelingt Mike erst nach einiger Zeit, die Funktionen wieder abzustellen. Da Sulley nun auch noch den verstellten Innenspiegel versehentlich abbricht, wird er vom immer noch wütenden Mike aus dem Auto geworfen. Als Mike dann wutentbrannt zurücksetzen will, aber per Kickstart losfährt, setzt er das Auto gegen ein Hindernis. Sulley ist etwas besorgt, weil der Airbag bei der Kollision nicht aktiviert wurde. Dies geschieht jedoch wenige Sekunden später, so dass Sulley den an ihm vorbeifliegenden Mike auffangen kann. Mike wünscht sich nun doch sein altes Auto zurück.\n\nDer Film wurde 2003 für den Oscar in der Kategorie Bester animierter Kurzfilm nominiert.\n"}
{"id": "3097369", "url": "https://de.wikipedia.org/wiki?curid=3097369", "title": "Vorschlagssuche", "text": "Vorschlagssuche\n\nDie Vorschlagssuche (oder auch: Suchvervollständigung) ist eine Variante der Suchfunktion von Rechnerprogrammen. Dabei werden bereits beim Eintippen des Suchbegriffs mögliche Vervollständigungen oder Fundstellen angezeigt.\n\nDer Vergleich von eingetippter Suchanfrage und der Datenbasis von Vervollständigungen kann unterschiedlich flexibel und unterschiedlich intelligent sein. Bei der Flexibilität kann man grob unterscheiden, ob Vorschläge immer nach rechts vervollständigen oder auch nach links. Die meisten Programme zur Suchvervollständigung operieren ohne Berücksichtigung der Semantik der Dokumente, d. h., sie schlagen auch Vervollständigungen vor, die zwei inhaltlich nicht zusammenhängende Wörter enthalten, die nur zufällig in den Dokumenten direkt nebeneinander stehen. Neuartige semantische Suchvorschläge im Kontext von bedeutungsorientierten Suchmaschinen hingegen achten darauf, dass die Begriffe in der Suchvervollständigung auch in einem inhaltlichen Zusammenhang stehen. Dabei können auch weit auseinander stehende Wörter zu einem Suchvorschlag kombiniert werden, wenn der Zusammenhang z. B. durch automatische Sprachverarbeitung nachgewiesen werden konnte.\n\nEin Beispiel für eine einfache Vorschlagssuche ist die Suchfunktion des Browsers Firefox, bei der bereits mit jedem eingetippten Buchstaben eine passende Fundstelle angezeigt wird. Auch mit Google Suggest kommt die Vorschlagssuche zum Einsatz. Vorschlagssuchfunktionen auf Internetseiten nutzen die AJAX-Technik – auf diese Weise können Daten mit dem Dienstanbieter ausgetauscht werden, ohne dass die Seite neu geladen werden muss.\n"}
{"id": "3101156", "url": "https://de.wikipedia.org/wiki?curid=3101156", "title": "Conduit (GNOME)", "text": "Conduit (GNOME)\n\nConduit ist eine freie Software zum Datenabgleich für den GNOME-Desktop. Es erlaubt dem Nutzer, Daten aus verschiedenen Quellen (PCs, Online-Diensten oder anderen Geräten, wie beispielsweise Mobiltelefonen) zu synchronisieren.\n\nDie Synchronisation erfolgt über eine Vielzahl von Schnittstellen. Diese können mit unterschiedlichen Datentypen (Text, Grafik, binär) umgehen und kann als Ein-Wege- oder Zwei-Wege-Synchronisation dienen.\n\nSo ist es möglich, Notizen aus Tomboy mit Google Notebook zu synchronisieren oder Daten zwischen zwei Computern auf einem einheitlichen Stand zu halten.\n\nDurch Conduit soll es Benutzern ermöglicht werden, all ihre Informationen synchron zu halten – unabhängig wo diese gespeichert sind. Dies soll dadurch erreicht werden, dass Conduit lediglich als allgemeines Framework fungiert und somit die aktuellen Probleme beseitigt, dass vorhandene Synchronisierungslösungen meist nur für spezifische Geräte oder Webseiten funktionieren.\n\nConduit verfügt über eine Sammlung von Datenanbietern und -wandlern. Datenanbieter repräsentieren alle möglichen Ressourcen wie angeschlossene MP3-Player, Webseiten oder installierte Programme. Sie besitzen ein Dateiformat (z. B. Bild, Kontakt, Notiz) und sind entweder eine Quelle, eine Senke, oder beides. Wenn der Benutzer versucht, einen Datenanbieter als Quelle mit einem Datenanbieter als Senke zu verbinden, wird Conduit versuchen diese Datenverbindung unter Benutzung der verfügbaren Datenwandler herzustellen. Es existiert eine Reihe grundlegender Datenformate so dass eine Wandlung nur ein einziges Mal hergestellt werden muss und von allen Datenanbietern wiederverwendet werden kann, die die gleichen Datenformate nutzen.\n\n"}
{"id": "3107908", "url": "https://de.wikipedia.org/wiki?curid=3107908", "title": "SystemRescueCd", "text": "SystemRescueCd\n\nSystemRescueCd (vom Englischen für \"System-Rettungs-CD\") ist eine bis Version 5 auf Gentoo basierende, seit Version 6 auf ArchLinux Linux-Distribution, die sich als Live-System direkt von einer CD oder einem USB-Stick starten lässt. Sie wurde als kleines Rettungssystem entworfen (startfähiges Notfallsystem), um ein nicht mehr startbares Betriebssystem zu reparieren und Daten wiederherzustellen.\n\nDas in Frankreich gestartete Projekt \"SystemRescueCd\" basiert auf der Gentoo Live CD und beinhaltet eine Reihe nützlicher Systemtools (siehe unten) und Basisanwendungen (Browser, Editor, Midnight Commander, Netzwerktools). Zusätzlich zur Konsole ist auch ein X Window System mit dem Fenstermanager Xfce4 (bis Version 1.1.7 JWM, davor GNU Window Maker) verfügbar.\n\nDie Distribution bietet somit einfache Möglichkeiten, um Administrationsaufgaben wie zum Beispiel das Erstellen oder Ändern von Partitionen vorzunehmen. So werden viele Systemprogramme und einfache Werkzeuge mitgeliefert. Außerdem werden die wichtigsten Dateisysteme (ext2/ext3/ext4, btrfs, reiserfs, reiser4, xfs, jfs, vfat, ntfs, iso9660) und Netzwerksysteme (samba, nfs) unterstützt.\n\n\n\n"}
{"id": "3108069", "url": "https://de.wikipedia.org/wiki?curid=3108069", "title": "IBM 305 RAMAC", "text": "IBM 305 RAMAC\n\nDer IBM 305 RAMAC (Abkürzung für \"random-access method of accounting and control)\" war der erste kommerziell erfolgreiche Computer, der über eine Festplatte verfügte. Er konnte bis zu zwei Festplatten des Modells \"350 Disk Storage Unit\" beherbergen und Daten in Echtzeit speichern.\n\nDie Festplatte hatte eine Kapazität von 5 Millionen 7-Bit-Zeichen (entspricht 4,375 Millionen Oktetten, also 4,375 Megabyte im heutigen Sprachgebrauch). Sie hatte ein Gewicht von einer Tonne und eine Größe von 60×68×25 Zoll (152,4×172,72×63,5 cm). Die Daten wurden auf 50 mit Eisenoxid beschichteten Platten gespeichert. Die Speicherkapazität war in 50.000 Blöcke zu jeweils 100 alphanumerischen Zeichen aufgeteilt. Bei einer Rotationsgeschwindigkeit von 1200 Umdrehungen pro Minute wurden bis zu 100 Bit pro Zoll gespeichert und eine Zugriffszeit von 600 Millisekunden erreicht. Ein anderes Modell der Festplatte hatte 10 Millionen Zeichen Speicherkapazität.\n\nDer Computer wurde im Mai 1955 von IBM angekündigt und am 14. September 1956 erstmals der Öffentlichkeit präsentiert. Er wurde von 1956 bis 1961 hergestellt. Mehr als 1000 der Rechenanlagen mit Vakuumröhren wurden produziert und ab 1957 zu jeweils 3200 US-Dollar monatlich vermietet. 1969 wurden die 305 RAMACs von IBM zurückgenommen.\n\n"}
{"id": "3109875", "url": "https://de.wikipedia.org/wiki?curid=3109875", "title": "Conditional Random Field", "text": "Conditional Random Field\n\nEin Conditional Random Field (CRF) ist ein Typ von ungerichtetem probabilistischem Modell, der im maschinellen Lernen (einem Teilgebiet der künstlichen Intelligenz) eingesetzt wird. Häufig wird mit dem Begriff auf eine spezielle Form mit linearer Struktur verwiesen, das \"Linear-Chain Conditional Random Field\". Dieses wird typischerweise zur Segmentierung von Sequenzen verwendet. Das bedeutet, das CRF erhält eine Sequenz formula_1 als Eingabe und gibt eine gleich lange Sequenz formula_2 aus. Im Unterschied zu Hidden-Markov-Modellen (HMMs; ein anderes, jedoch gerichtetes Modell für sequentielle Daten) kann ein CRF an jeder Stelle auf die komplette Information der Eingabesequenz zugreifen, wohingegen ein HMM nur die aktuelle Eingabe sieht. Hierdurch können komplexe Merkmalsmengen verwendet werden.\n\nWie alle Modelle des maschinellen Lernens müssen CRFs \"trainiert\" werden, d. h., ihre Parameter müssen aus Daten abgeschätzt werden. Hierfür existieren verschiedene Lernverfahren, etwa das Gradientenverfahren oder das Quasi-Newton-Verfahren. Dabei werden einige Sequenzen vorgegeben, von denen sowohl die Eingabe als auch die gewünschte Ausgabe bekannt ist (es handelt sich also um überwachtes Lernen). Das Lernverfahren versucht dann die Parameter im CRF so anzupassen, dass für möglichst viele Sequenzen in den Trainingsdaten die richtige Ausgabesequenz vorhergesagt wird.\n\nCRFs wurden erfolgreich auf verschiedene Probleme angewandt, wie zum Beispiel:\n\n\n"}
{"id": "3112383", "url": "https://de.wikipedia.org/wiki?curid=3112383", "title": "NBZ80", "text": "NBZ80\n\nDer Nanocomputer NBZ80 ist ein von der italienischen Elektronikfirma SGS-ATES hergestellter 8-Bit-Einplatinencomputer für Unterrichtszwecke. Er wurde 1980 in den Markt eingeführt und ist mit einem auf 2,4756 MHz getakteten Zilog Z80 Hauptprozessor und mit einem Arbeitsspeicher von 4 kB ausgestattet. Der Arbeitsspeicher lässt sich auf maximal 16 KB erweitern. Des Weiteren besitzt er einen ROM von 2 KB. Außerdem ist der NBZ80 mit einer 30-Tasten Tastatur und einem einzeiligen 8-Zeichen LED-Display ausgestattet.\n\nDie Nachfolger sind der \"NBZ80 Super\" und der \"NBZ80 NF\".\n\nDer NBZ80-B verfügte über die Nanocomputer Hauptplatine, 2 KB ROM sowie Dialog- und Stromversorgungs-Peripherie. Das Modell NBZ80-S besaß außerdem eine Experimentierplatine. In der NBZ80-HL Ausstattung waren ein in BASIC programmierbarer, 8 KB großer ROM, ein 16 KB großer RAM, sowie eine Tastatur und eine Videoschnittstelle enthalten. Zusätzlich war für die HL-Variante ein Monitor (\"TVZ80\") verfügbar.\n\nEs war möglich, das \"B\"-Modell zu einem \"S\"-Modell aufzurüsten und dieses wiederum zu einem \"HL\"-Modell.\n\n"}
{"id": "3112869", "url": "https://de.wikipedia.org/wiki?curid=3112869", "title": "EnGarde Secure Linux", "text": "EnGarde Secure Linux\n\nEnGarde Secure Linux ist eine eigenständig entwickelte Linux-Distribution für Server. Sie wurde als besonders sicheres Betriebssystem entworfen und setzt dabei auf SELinux.\n\nDas System kann optional komplett über eine SSL-verschlüsselte Web-Schnittstelle konfiguriert werden. Es stellt relativ niedrige Systemanforderungen, die Entwickler geben lediglich einen Intel-Pentium-Prozessor, 32 MiB Arbeitsspeicher und eine 2-GB-Festplatte an.\n\nDas System wird seit 1999 entwickelt, eine erste Version erschien 2001. Nach eigener Aussage ist es die weltweit erste Open-Source-Sicherheitslösung.\n\nUrsprünglich basierte EnGarde Secure Linux auf Red Hat Linux, heute wird es jedoch eigenständig weiterentwickelt.\n\n"}
{"id": "3115013", "url": "https://de.wikipedia.org/wiki?curid=3115013", "title": "HyperTerminal", "text": "HyperTerminal\n\nHyperTerminal ist ein Kommunikationsprogramm, das beginnend mit der Version 2.0 mit dem Windows-Betriebssystem mitgeliefert wurde. HyperTerminal basiert auf dem Programm HyperACCESS und wurde von der Firma Hilgraeve an Microsoft lizenziert.\n\nHyperTerminal kann verwendet werden, wenn mit einem Modem eine Einwahlverbindung zu einem Großrechner oder einem Mailbox-Server aufgebaut werden soll. Das Programm erfüllt dann die Funktion eines Terminalemulationsprogramms.\n\nDarüber hinaus ist es auch möglich, mit HyperTerminal Verbindungen zwischen Computern herzustellen, die über die seriellen Schnittstellen direkt miteinander verbunden sind (etwa zwischen einem Desktop-PC und einem tragbaren Computer). Weiter ist es möglich, mittels HyperTerminal über die serielle Schnittstelle externe Geräte zu steuern (etwa wissenschaftliche Instrumente und Roboter).\n\nAb Windows Vista ist HyperTerminal nicht mehr Bestandteil von Windows. Es besteht allerdings die Möglichkeit eines kostenpflichtigen Downloads zum Nachrüsten des Programmes. Alternativ dazu gibt es auch kostenlose Opensource-Terminalprogramme, wie zum Beispiel PuTTY, mit denen man einen ähnlichen Funktionsumfang erhält.\n\n"}
{"id": "3120216", "url": "https://de.wikipedia.org/wiki?curid=3120216", "title": "Code Lyoko", "text": "Code Lyoko\n\nCode Lyoko ist eine französische computeranimierte Fernsehserie, die von 2003 bis 2007 auf den Sendern France 3 und Canal J, aber auch auf dem amerikanischen Cartoon Network ausgestrahlt wurde. \n\nDer Computervirus X.A.N.A. versucht von der virtuellen Welt Lyoko aus, die reale Welt zu besetzen. Nur die drei Freunde Ulrich, Odd und Yumi können als „Krieger von Lyoko“ mithilfe ihres Freundes Jeremie, eines Supercomputers und eines Scanners nach Lyoko gelangen, um dort X.A.N.A. aufzuhalten. Dabei steht ihnen Aelita zur Seite, ein Mädchen das sich vor langer Zeit ebenfalls nach Lyoko begeben hat, dort aber, bis zum Ende der ersten Staffel, nicht mehr weg kann. In Lyoko besitzen die Kinder eine Kampfausrüstung, Waffen und besondere Fähigkeiten, wie zum Beispiel Telekinese.\n\n\n\n\n"}
{"id": "3123921", "url": "https://de.wikipedia.org/wiki?curid=3123921", "title": "MicroBee", "text": "MicroBee\n\nMicroBee (Micro Bee) war eine Marke für eine Reihe von Heimcomputern der Firma Applied Technology, die sich später in den Namen MicroBee Systems umbenannte.\n\nDer Original MicroBee Computer wurde in Australien entwickelt. Daran beteiligt waren unter anderem auch Owen Hill und Matthew Starr. Der Rechner basierte auf Eigenschaften der Prozessorkarten DG-Z80 und DG-640 S-100, die von David Griffith entwickelt worden waren, der TCT-PCG S-100 Karte, entwickelt von TCT Micro Design, und der MW6545 S-100 Karte, entwickelt von Dr. John Wilmshurst. Der Rechner war ursprünglich als Zwei-Board-Einheit aufgebaut worden. Grundlage war das \"Mainboard\", das die Tastatur, den Zilog Z80 Mikroprozessor, den Synertek 6545 CRT Controller, 2K \"Bildschirm\" RAM, 2K Zeichensatz ROM (128 Zeichen) und 2K PCG (Programmable Character Graphics) RAM (128 Zeichen) enthielt. Jedes Byte im Bildschirm RAM adressierte jeweils ein Zeichen im Zeichensatz ROM oder im PCG RAM. Ein zweites Board, mit dem Namen \"Coreboard\", enthielt den Speicher und spätere Modelle waren auch mit einem Floppy Disk Controller ausgestattet.\n\n"}
{"id": "3124308", "url": "https://de.wikipedia.org/wiki?curid=3124308", "title": "Eboy", "text": "Eboy\n\nEboy (auch eboy) ist eine 1998 gegründete Gruppe von deutschen Künstlern und freiberuflichen Grafikdesignern, die sich auf Pixel-Art spezialisiert hat und seltener auch Vektorgrafiken herstellt. Gründungsmitglieder der in Berlin-Wedding arbeitenden Gruppe sind Steffen Sauerteig, Svend Smital und Kai Vermehr; der zwischenzeitlich hinzugestoßene Peter Stemmler (New York City) verließ die Gruppe im Januar 2007 wieder, um sich auf sein Projekt \"Quickhoney\" zu konzentrieren. \n\nDie Arbeiten der Designer umfassen unter anderem Werbeaufträge von MTV, Adidas und SAP, Illustrationen für Publikationen wie Die Woche, Die Zeit oder The Face sowie das Titelbild des „Spiegel“ vom 10. August 2009 und den Entwurf einer Spielzeugserie namens \"Peecol\". Die größte Verbreitung fanden ihre isometrischen Stadtansichten, zum Beispiel von London oder Tokio. Ihre Bilddatenbank brachte eboy 2002 in dem Band \"Hello Eboy\" heraus. Die Gruppe zeigte Drucke ihrer Werke auf verschiedenen Ausstellungen, zum Beispiel \"Pixelesque\" im Mai/Juni 2004 in der Maxalot Gallery in Barcelona oder als Teil von \"Digital Aesthetic 2\" im Vereinigten Königreich im Frühjahr 2007. Sie trat auf Konferenzen wie der \"Typo Berlin 2006\" oder der \"Pictoplasma Conference\" des Projekts Pictoplasma auf.\n\n\n"}
{"id": "3124524", "url": "https://de.wikipedia.org/wiki?curid=3124524", "title": "Time Machine (Apple)", "text": "Time Machine (Apple)\n\nTime Machine (englisch für \"Zeitmaschine\") ist eine Datensicherungssoftware von Apple, die seit Mac OS X Leopard (10.5) in das Betriebssystem macOS integriert ist. Das Programm kann für die automatische Sicherung aller Computerdaten und Einstellungen benutzt werden.\n\nDie Time Machine erstellt im Betrieb Snapshots des aktuellen Dateisystems und speichert sie in einem mitwachsenden Sparsebundle. Am Beginn steht eine Urfassung der zu sichernden Verzeichnisse, die gespiegelt werden. Für jedes Backup wird dann daneben ein neues Verzeichnis angelegt, das nach dem Datum und der Uhrzeit benannt ist. Alle Dateien werden innerhalb der Time Machine nur einmal abgelegt, um Speicherplatz zu sparen; zu weiterhin aktuellen Versionen von Dateien werden aus späteren Backups heraus Hardlinks angelegt.\n\nDie Sicherung wird so lange durchgeführt, bis das Speichermedium für die Time Machine voll ist. Um weiter arbeiten zu können, löscht die Time Machine automatisch die ältesten Versionen durch Bereinigung des Backups. Time Machine erstellt daher kein vollständiges Archiv, sondern nur eine Reihe von Abbildern eines Dateisystems im zeitlichen Ablauf. Sie funktioniert nur zuverlässig, wenn ihre Integrität unbeschädigt ist, insbesondere solange alle Systemlinks, die sie erstellt hat, intakt sind.\n\nDie Sicherung kann im Dauerbetrieb ständig „nebenher“ erfolgen oder punktuell, wenn das Ziellaufwerk hierfür nur zeitweise angeschlossen wird.\n\nTime Machine kann Daten auf interne Festplatten oder externe Speichermedien, die über USB, FireWire oder Thunderbolt mit dem Computer verbunden sind, sichern. Hierfür wird eine HFS+-formatierte Festplatte oder Partition vorausgesetzt. Von diesem Volume darf der Mac nicht gebootet worden sein, um im selben Arbeitsgang Daten auf ihm ablegen zu können.\n\nFür die Datensicherung über das Netzwerk bot Apple von 2008 bis 2018 die Apple Time Capsule (mit eingebauter Festplatte) an. Alternativ kann ein gewöhnlicher Router mit angeschlossener USB-Festplatte oder ein NAS verwendet werden, zumal die meisten Consumer-Router- und -NAS-Hersteller Unterstützung für die Time Machine anbieten.\n\nTime Machine kann eine Datensicherung nur für Laufwerke durchführen, die unmittelbar am Mac angeschlossen sind. Die Sicherung kann also neben der eingebauten Festplatte, auf der das Betriebssystem läuft, auch weitere direkt an den Mac angeschlossene externe Laufwerke umfassen. Im Umkehrschluss bedeutet dies, dass sich Netzlaufwerke wie SMB-Shares nicht mit Time Machine sichern lassen.\n\nWelche Laufwerke gesichert werden sollen, hat der Benutzer in der Time-Machine-Konfiguration zu konfigurieren.\n\nWenn Daten beschädigt oder verlorengegangen sind, kann der Benutzer diese über Time Machine wiederherstellen. Als erstes muss dazu der Benutzer das jeweilige Programm aufrufen, in welchem er Daten vermisst. Das kann zum Beispiel eine Datei im Finder sein, aber auch eine E-Mail in Apple Mail oder ein Kontakteintrag im Apple-Adressbuch. Im zweiten Schritt startet der Benutzer das Time-Machine-Programm.\n\nJetzt werden alle Fenster, bis auf das Fenster, in dem der Benutzer seine Daten vermisst (Finder, Mail …), ausgeblendet. Der Hintergrund wechselt in ein visuell aufwendig gestaltetes 3D-Universum mit einer chronologischen Reihenfolge hintereinander liegender Fenster mit einem Zeitstrahl, auf dem alle Sicherungszeitpunkte ersichtlich sind. Durch Auswahl eines Zeitpunktes kann der jeweilige vormalige Datenzustand betrachtet werden. Wenn die vermissten Daten bzw. die richtige Version wiedergefunden wurde, kann alles oder nur ein Teil aus der Vergangenheit mit einem Klick in die aktuelle (neue) Version zurückgeholt werden.\n\nWelches Fenster im 3D-Stapel dargestellt wird, hängt von der Time Machine-Integration der jeweiligen Anwendung ab. Bei Apple Mail, Kontakte oder in älteren Systemen bei iPhoto kann das jeweilige Programm mit den in der Vergangenheit vorhandenen Daten (die früheren Mails, Notizen, Adressen, Fotos usw.) angezeigt werden. Andere Programme bieten keine so tiefe Time-Machine-Integration – teils gilt das auch für Anwendungen von Apple, wie etwa Aperture oder iTunes. Hier erscheint dann das Finder-Fenster, und man muss anhand der Dateinamen die richtige Information hervorholen. Bei Office-Programmen, in denen der Benutzer ständig mit Dateinamen arbeitet und alles bewusst im Finder ablegt, erfolgt die Darstellung des Finders aufgabenorientiert. Bei Anwendungen, bei denen die Software die Dateiverwaltung übernimmt und der Benutzer dateinamenlos arbeitet (wie etwa bei iTunes, Aperture oder VMware), kann sich das Wiederfinden etwas schwieriger gestalten. Die Softwarehersteller bauten die Integration der Time-Machine-Kompatibilität über die Jahre immer mehr aus. Ein Beispiel ist etwa VMware Fusion 4.0, das, im Gegensatz zu früheren Versionen, für das einfache Wiederfinden der richtigen Version aus der Vergangenheit statt eines Finder-Fensters ein richtiges GUI anzeigt.\n\nIst das ganze System wiederherzustellen, beispielsweise nach einem Festplattenwechsel, so ist der Computer im macOS-Wiederherstellungmodus zu booten, anschließend kann der ganze Time-Machine-Backup zurückgespielt werden. Die Migration auf einen anderen Computer lässt sich mit dem Migrationsassistenten durchführen, hierbei lässt sich als Quelle ein Time-Machine-Backup nutzen.\n\nDer Zugriff auf Time-Machine-Backups ist von jedem macOS-System aus möglich, sei es über das Time-Machine-Dienstprogramm oder durch das manuelle Mounten des Sparsebundles, in dem das Backup abgelegt ist.\n\nWie bei jeder anderen differenziellen Datensicherung wird bei einer Änderung der Datei die ganze Datei neu gesichert. Daher ist Time Machine beispielsweise für die Sicherung von virtuellen Maschinen, die aus einer einzigen großen Datei bestehen, weniger gut geeignet. Gleiches gilt für E-Mail-Clients, bei denen alle Mails standardmäßig in einer einzigen großen Mailbox-Datei gespeichert werden, wie etwa Mozilla Thunderbird. Für solche Fälle wäre eine inkrementell arbeitende Datensicherung besser geeignet. Paket-Dateien (im Finder präsentieren sich diese für den Benutzer wie eine Datei) werden hingegen von Time Machine als Ordner behandelt und dann nur die darin veränderten Dateien bei Veränderungen gesichert.\n\nFolgende Kritikpunkte wurden über mehrere Generationen von neuen Mac-OS-Versionen schrittweise behoben:\n\n"}
{"id": "3126552", "url": "https://de.wikipedia.org/wiki?curid=3126552", "title": "Windows Developer", "text": "Windows Developer\n\nDas Magazin Windows Developer (Eigenschreibweise: windows.developer; bis 2012 dot.net magazin) ist ein unabhängiges IT-Fachmagazin für den Bereich .NET-Technologien und wird von Software & Support Media in Frankfurt am Main herausgegeben. Inhalte des Magazins sind Informationen für Entwickler, die u. a. mit der .NET-Plattform arbeiten.\n\n\"Windows Developer\" erscheint monatlich im deutschsprachigen Raum mit einer Druckauflage von 5.000 Exemplaren. Eine CD-ROM oder DVD mit Quellcodes liegt jeder Ausgabe bei. Außerdem erscheinen in unregelmäßigen Abständen Sonderhefte mit aktuellem Bezug. Redaktionssitz ist seit Januar 2009 in Frankfurt am Main; zuvor war es in Unterhaching angesiedelt.\n\nDer Chefredakteur ist derzeit Sebastian Meyen. Vertretungsberechtigter Geschäftsführer ist Pouya Kamali, der auch den Verlag leitet.\n\nIm vierten Quartal 2017 lag die durchschnittliche monatlich verbreitete Auflage nach IVW-Angaben bei 2.530 Exemplaren. Das sind 1,02 % (= 26 Hefte) weniger Hefte als im Vergleichsquartal des Vorjahres. Die Abonnentenzahl nahm im gleichen Zeitraum um 135 Abonnenten (= 7,51 %) ab.\n\n"}
{"id": "3136020", "url": "https://de.wikipedia.org/wiki?curid=3136020", "title": "Windows Anytime Upgrade", "text": "Windows Anytime Upgrade\n\nWindows Anytime Upgrade war ein Geschäftsmodell von Microsoft zum Vertrieb von Windows Vista und Windows 7 über das Internet. Die Anwender erhalten für alle Editionen eine identische Installations-DVD und installieren die jeweils passende Edition nach Eingabe ihres Lizenzschlüssels. Unterschiedliche Datenträger sind nur noch für die 32-Bit- und die 64-Bit-Version notwendig. Microsoft sowie die Benutzer haben den Vorteil, nicht für jede Edition eine andere DVD herstellen bzw. bereithalten zu müssen.\n\nMittlerweile (Stand 2016) ist Anytime Upgrade unter Windows Vista und Windows 7 nicht mehr nutzbar.\n"}
{"id": "3137999", "url": "https://de.wikipedia.org/wiki?curid=3137999", "title": "Micky Maus Wunderhaus", "text": "Micky Maus Wunderhaus\n\nMicky Maus Wunderhaus (im Original: \"Mickey Mouse Clubhouse\") ist eine US-amerikanische Trickserie der Walt Disney Company, die an Kinder im Vorschulalter gerichtet ist. Sowohl in den USA als auch in Deutschland läuft die Serie auf dem Disney Channel. Zudem lief sie auf Super RTL.\n\nHauptcharaktere der Serie sind bekannte Disneyfiguren wie Micky Maus, Minnie Maus und Donald Duck, Goofy, Daisy und Pluto. Dabei sind die Disney-Charaktere erstmals in computer-animierter Form zu sehen.\nIn jeder Folge erleben Micky und seine Freunde Abenteuer, in deren Verlauf sie Probleme lösen müssen. Dabei soll sowohl das Lösen von praktischen Problemen als auch Zählen und Rechnen geübt werden.\n\nÜbersicht der Staffeln und Episoden in Deutschen Fernsehen im Vergleich zum Original:\n\nDie Aufzählung der Episoden erfolgt landesspezifisch nach Erstausstrahlung, wobei in einigen Ländern die Spezialfolgen mit Überlängen (28:50 oder über 40 Minuten) einsortiert sind.\n\n\"Die Wunderhaus-Eisenbahn (Choo-Choo Express)\" (hier Episode 2.33) ist im Original (in englischsprachigen Ländern) eine Episode mit ca. 42 Minuten Dauer (Spezial). Sie wurde zum einfacheren Senden in zwei Standard-Episoden mit 23 Minuten geteilt und mit einer kurzen Überleitung zum abschließenden Mauske-Tanz bzw. einer kurzen Einleitung versehen. Der zweite Teil wurde hierzu in \"Der Wunderhaus Bahnhof (Mickey's Train Station)\" (hier Episode 3.33) umbenannt.\n\nDie Deutsche Film- und Medienbewertung FBW in Wiesbaden verlieh folgenden Episoden/Veröffentlichungen das Prädikat wertvoll:\n\n- Micky Maus Wunderhaus Micky rettet den Weihnachtsmann sowie\n\n- Muska, Micky Maus \"Micky Maus Wunderhaus\" Meeska.\n\nDas Titellied wird von der Band They Might Be Giants gesungen.\n\n\n"}
{"id": "3146374", "url": "https://de.wikipedia.org/wiki?curid=3146374", "title": "HTC Universal", "text": "HTC Universal\n\nDas HTC Universal, in Deutschland hauptsächlich als T-Mobile MDA Pro bekannt, ist ein PDA-Phone auf Basis von Windows Mobile 5.0, hergestellt von HTC Corporation. Es war sowohl das erste 3G/UMTS-fähige PDA-Phone als auch das erste, das mit Windows Mobile 5 ausgeliefert wurde.\n\nIm Gegensatz zu neueren Modellen des taiwanischen Herstellers HTC wurde das Gerät nicht vom Hersteller selber, sondern nur von alternativen Anbietern bzw. Providern vertrieben. So wurde es unter Namen wie O2 XDA Exec, Orange SPV M5000, Dopod 900, Qtek 9000, T-Mobile MDA Pro, I-mate JasJar, Vodafone v1640 (auch VPA IV), E-Plus PDA IV und Grundig GR980 verkauft. Alle Modelle haben die gleiche Hardware und unterscheiden sich nur in Gehäusefarbe und Branding.\n\nDas auffallendste Merkmal des Geräts ist das um 180° drehbare Display, das einen schnellen Wechsel zwischen Hoch- und Querformat erlaubt. Die GUI wechselt automatisch beim Drehen bzw. Klappen in den gewünschten Modus, nicht jedoch über einen Schwenksensor wie beim iPhone.\n\n\nDer interne Flashspeicher ist in fünf „Partitionen“ aufgeteilt:\n\nIm Gegensatz zu den Modellen von Qtek und i-mate sind die Modelle, die von Providern wie T-Mobile vertrieben werden, per Netlock geschützt und können gegen Zahlung von 100 € oder nach 24 Monaten Wartezeit davon befreit werden. Es besteht allerdings auch die Möglichkeit, es mit einem kostenlosen Tool von buzz_lightyear bei buzzdev.net freizuschalten. Damit geht die Garantie verloren.\n\nAndere Hersteller bieten weit leistungsfähigere Akkus für das Gerät an, die in Laufzeit und Entladungszeit den werksseitig verbauten 1600 mAh weit übertreffen. Diese Akkus erreichen Werte bis hin zu 4800 mAh, was Gespräche von über 12 Stunden Länge und „Schlummerzeiten“ von über zwei Wochen ermöglicht. Stärkere Akkus sind allerdings auch schwerer und erhöhen die Gesamtmasse auf über 350 Gramm (ohne Akku: ca. 100 Gramm).\n\n\nIm Gegensatz zu einigen PDA-Herstellern bietet HTC für seine Geräte keine Upgrade-Möglichkeit auf neue Betriebssysteme. Es haben sich jedoch einige Entwickler-Gemeinschaften gebildet, die das Betriebssystem Windows Mobile 6 auf Basis inoffizieller Builds auf dem HTC Universal installieren. Die Gemeinschaft \"XDA-Developers\" arbeitet ständig an Problemlösungen und entwickelt neue Applikationen und Möglichkeiten für das HTC Universal.\n\nDas erfolgreichste deutsche ROM mit Windows Mobile 6.1 liegt seit dem 24. Dezember 2008 in der Version 6.0 vor und bringt viele Features und Programme mit. Das Projekt wird allerdings nicht mehr weiterentwickelt.\n\nAuch das vom Microsoft stark für die Fingerbedienung optimierte Windows Mobile Version 6.5 ist verfügbar. \n\nEinige Entwickler der \"XDA-Developers\" portieren ein Linux-Betriebssystem auf das HTC Universal. Es hat gewisse Einschränkungen und erkennt Teile der Hardware, wie beispielsweise die eingebaute Kamera, nicht.\n\nMitglieder der \"XDA-Developers\" portieren auch das mobile Betriebssystem \"Android\" auf das HTC Universal.\n\n"}
{"id": "3153352", "url": "https://de.wikipedia.org/wiki?curid=3153352", "title": "Functional Digital Mock-Up", "text": "Functional Digital Mock-Up\n\nFunctional Digital Mock-Up (FDMU), auch Functional Mock-Up (FMU) genannt, ist die Fortentwicklung des Digital Mock-Up. Während beim DMU nur geometrische Aspekte von am Rechner konstruierten dreidimensionalen Baugruppen beleuchtet werden, versucht man beim FDMU auch Funktionen dieser Baugruppen zu simulieren. An ein CAD-System gekoppelte Steuersoftware kann auf diese Art und Weise gemeinsam mit dem 3D-Modell getestet werden.\n\n"}
{"id": "3156718", "url": "https://de.wikipedia.org/wiki?curid=3156718", "title": "Algorithmus von Sutherland-Hodgman", "text": "Algorithmus von Sutherland-Hodgman\n\nDer Algorithmus von Sutherland-Hodgman ist ein nach Ivan Sutherland und Gary W. Hodgman benannter Algorithmus der Computergrafik zum Clipping von Polygonen.\n\nMit dem Sutherland-Hodgman-Algorithmus kann man mit jedem konvexen Polygon jedes andere Polygon (konvex oder konkav) clippen. Für jede Fensterkante wird die Begrenzungsstrecke zu einer Gerade erweitert, an der sämtliche (relevanten) Polygonkanten gekürzt werden.\n\nDer folgende Pseudo-Code clippt ein Polygon nach dem Sutherland-Hodgman-Algorithmus:\n\nClipping eines Polygons bzgl. eines beliebigen konvexen Polygons. Beschreibung des Polygons durch seine Ecken formula_1 und Kanten von formula_2 nach formula_3 bzw. von formula_4 nach formula_5. Nun wird in formula_6 Teilschritten die Liste der Ecken durchlaufen formula_7 und eine Liste mit formula_8 Polygonecken formula_9 ausgegeben. Beim Übergang formula_10 sind vier Fälle möglich.\n\n\n\n"}
{"id": "3160865", "url": "https://de.wikipedia.org/wiki?curid=3160865", "title": "Corel Presentations", "text": "Corel Presentations\n\nCorel Presentations ist ein Präsentationsprogramm der Firma Corel und Bestandteil des Pakets WordPerfect Office.\n\nDie aktuelle Version von Presentations ist X7. Sie erschien mit der Paketversion WordPerfect Office X7 2014, ist jedoch nie so weit verbreitet gewesen wie das Präsentationsprogramm Microsoft PowerPoint. Seit der 2008 erschienenen Version X4 ist Presentations nur noch in englischer Sprache verfügbar.\n\n"}
{"id": "3161466", "url": "https://de.wikipedia.org/wiki?curid=3161466", "title": "GDI+", "text": "GDI+\n\nGDI+ (GDI steht für Graphics Device Interface) ist der Nachfolger von Microsofts Grafiksystem GDI und ist seit Windows XP ins Betriebssystem integriert, kann aber auf älteren Microsoft-Betriebssystemen (zumindest Windows 2000, nicht Windows NT) über Windows Update nachinstalliert werden. In Windows Vista hat GDI+ das ältere GDI fast vollständig verdrängt, so dass veraltete GDI-Grafik nur noch an den Desktop Window Manager weitergeleitet wird. Aus diesem Grund ist auch GDI im Gegensatz zu GDI+ seit Windows Vista nicht mehr hardwarebeschleunigt.\n\nNeue Features von GDI+ beinhalten die native Darstellung von JPEG und PNG, verbessertes Pfad-Management (für Vektorgrafiken) und Farben in ARGB. Dies vereinfacht die Implementierung von PDF, SVG und verwandten Technologien. Vergleichbare Grafikbibliotheken sind Quartz für Apples macOS und Cairo für Linux.\n\n"}
{"id": "3162421", "url": "https://de.wikipedia.org/wiki?curid=3162421", "title": "Das Planetarium 1900–2100", "text": "Das Planetarium 1900–2100\n\nDas Planetarium 1900–2100 ist ein Astronomieprogramm, das in der Ansichtsgenauigkeit auf den Zeitraum von 1900 bis 2100 beschränkt ist.\n\nDas Programm ist für den direkten Vergleich mit dem nächtlichen Himmel per bloßem Auge oder mit Fernglas geeignet – der eigene Standort lässt sich dafür anpassen. Auch bietet es Details zu den Jupitermonden, Sonne und Mond sowie einen Almanach.\n\n3D-Darstellungen der Planeten geschehen mittels OpenGL und DirectX.\n\nDas Programm besteht aus mehreren Teilmodulen, welche auch einzeln auf der Website verfügbar sind:\n\n"}
{"id": "3164708", "url": "https://de.wikipedia.org/wiki?curid=3164708", "title": "AutoHotkey", "text": "AutoHotkey\n\nAutoHotkey, kurz AHK, ist eine Skriptsprache und ein zugehöriger Interpreter, mit denen man die Windows-Benutzeroberfläche steuern kann. Mit Hilfe von AutoHotkey kann man sich wiederholende Arbeitsaufgaben unter Windows automatisieren. AutoHotkey ist eine freie Software.\n\nAutoHotkey läuft unter Windows 2000 oder höher. Unter Windows 95/98/Me funktioniert es mit einigen Einschränkungen.\n\nAutoHotkey ist auch für Programmieranfänger leicht zu erlernen. Die zur Programmiersprache mitgelieferte englische Dokumentation enthält ein Tutorial und zu jedem Befehl komplette Programmbeispiele oder zumindest konkrete Anwendungsfälle. Die zugehörige Hilfedatei AutoHotkey.chm kann durch die aktuelle deutsche Version ersetzt werden.\n\nAutoHotkey ist dafür gedacht, sich wiederholende Arbeitsaufgaben zu automatisieren. Mit AHK-Skripten kann man beispielsweise\n\nMit AutoHotkey lassen sich sowohl systemweit gültige als auch programmspezifische Tastenkombinationen und Funktionen zuweisen. So kann man z. B.\n\nFortgeschrittene können mit AutoHotkey grafische Benutzeroberflächen (GUI) erstellen, mit Regulären Ausdrücken arbeiten (PCRE) oder direkt auf DLLs von Windows oder Anwendungsprogrammen zugreifen.\n\nEs ist möglich, AutoHotkey-Skripte in eigenständig lauffähige Programme zu kompilieren. Diese Programme können somit auch auf Windows-Rechnern ohne AutoHotkey-Installation ausgeführt werden. Ein passender Compiler ist Bestandteil des AutoHotkey-Programmpakets. Skripte wie auch kompilierte Programme unterliegen der vom Entwickler vorgesehenen Lizenz und dürften somit auch als eigenständige Programme verkauft werden. Beim Kompilierungsvorgang lässt sich ein Passwort angeben (bis Version 1.1). Damit lässt sich das Programm vor dem einfachen Rückumwandeln in Quelltext (Dekompilieren) schützen, solange das Passwort nicht bekannt ist. Der so erreichte Schutz entspricht jedoch nicht dem eines binär kompilierten Programms. Um diesen falschen Eindruck nicht zu erwecken, unterstützen neuere Versionen (ab Version 1.1) den Passwortschutz nicht mehr.\n\nIm Programmpaket finden sich noch die Tools\n\nDas Skript gibt \"Hallo Welt!\" in einer MessageBox aus:\nMsgBox Hallo Welt!\n\nDas Skript startet das Programm Notepad, wartet, bis das Fenster aktiv ist und tippt dort \"Hallo Welt!\" ein:\n\nRun, \"notepad.exe\"\nWinWaitActive, ahk_class Notepad\nDurch das Drücken der Tastenkombination ++ wird ein markiertes Wort bei Wikipedia nachgeschlagen:\n\n^!w::\nReturn\nDie Abkürzung \"MfG\" wird durch einen Hotstring nach der Eingabe zu \"Mit freundlichen Grüßen\":\n\nDeaktiviert auf dem Desktop das Mausrad und damit versehentliches zoomen:\n\n *WheelDown::Return\nAusgabe einiger Systeminfos in einer selbst definierten Oberfläche: \n\nReturn\nGuiClose:\nGuiEscape:\nDer Programmierer Chris Mallett begann die Arbeit an AutoHotkey im Jahr 2003. Grund dafür war, dass die von ihm gewünschte Hotkey-Unterstützung für das damals unter der GNU General Public License stehende Programm AutoIt2 zu diesem Zeitpunkt nicht existierte und auch nicht geplant war.\n\nVersion 1.0 von AutoHotkey wurde im Februar 2004 veröffentlicht. AutoHotkey kann AutoIt2-Skripte ausführen, und circa 40 AutoHotkey-Befehle basieren direkt auf dem Quellcode von AutoIt. Einige AutoHotkey beigelegte Hilfsprogramme (u. a. der Compiler) stammen ebenfalls aus AutoIt.\n\nAutoIt ist inzwischen kein Open-Source-Programm mehr.\n\nAm 10. Oktober 2010 erklärte der Entwickler Chris Mallet, dass er AutoHotkey nicht mehr aktiv weiterentwickeln werde. Er habe das Interesse verloren, weil er, im Gegensatz zu anderen aktiven Entwicklern, die Skriptsprache nie zu einer voll ausgestatteten Programmiersprache ausbauen wollte. Auf der Download-Seite wird nun eine direkte Weiterentwicklung, die von einigen Mitgliedern der Community entwickelt wurde, als Download für AutoHotkey angeboten. Dieser offizielle Nachfolger trägt den Namen \"AutoHotkey 1.1\" und bietet neben 64-Bit-Unterstützung auch Unterstützung für Unicode, Arrays und Objekte. Die Originalversion, nun \"AutoHotkey Basic\" genannt, verfügt nicht über diese erweiterten Funktionalitäten.\n\n2012: AutoHotkey_L wurde die offizielle Version. „Chris“ übergibt die Besitzrechte der Domain www.autohotkey.com an „polyethene“.\n\n24. April 2014: Die \"AutoHotkey Foundation LLC\" wird gegründet, neue offizielle AutoHotkey-Webseite ist ahkscript.org.\n\n22. September 2015: Einigung mit „polyethene“. Übergabe der Domain autohotkey.com. Diese wird vollständig in einen neuen Server migriert.\n\n\n\n"}
{"id": "3170290", "url": "https://de.wikipedia.org/wiki?curid=3170290", "title": "Electronic Organizer", "text": "Electronic Organizer\n\nAls Electronic Organizer (bzw. Organiser) bezeichnet man handliche Computer, die mittels einer Personal-Information-Manager-Funktionalität (PIM) das Speichern und Organisieren von Daten, Adressen sowie Terminen in einer elektronischen Datenbank ermöglichen. Electronic Organizer verfügen über eine Tastatur (ab ca. 1987 über eine QWERTZ-Tastatur), die eine schnelle Dateneingabe ermöglicht, besitzen allerdings keine Mobilfunk- oder Internet-Funktionalität. Eine energiesparende Bauweise ermöglicht „Electronic Organizern“ einen wochenlangen Betrieb mit nur einem Satz Trockenbatterien; eine Lithium-Puffer-Batterie stellt darüber hinaus bei leeren Betriebsbatterien einen Datenerhalt über mehrere Jahre sicher.\n\nAnfang der 1980er Jahre erschienen am Markt leistungsfähige programmierbare Taschenrechner, die zum Teil mit QWERTZ-Tastaturen ausgerüstet waren (Sharp, HP, Psion, Casio usw.). Als Mikrocomputer konnten diese auch zur elektronischen Termin-, Adress- und Aufgabenverwaltung eingesetzt werden, und ersetzten daher oft einen Papier-Organizer. Diese Geräte wurden vom Handel daher nicht mehr als programmierbare Taschenrechner, sondern im Deutschen Sprachraum als Electronic Organizer (Organiser) vermarktet. (Psion Organizer als Psion). \n\nIm Zuge der Markteinführung der Personal Digital Assistants (PDAs) im Jahr 1993, die nicht mit Tastatur, sondern nur über einen berührungsempfindlichen Bildschirm bedient wurden, und schließlich auch durch den Markterfolg der Geräte vom Hersteller Palm mit einer erstmals alltagstauglichen Handschriftenerkennung, verschwanden Electronic Organizer in kurzer Zeit weitgehend vom Markt.\n\nDa die herkömmlichen Organizer von Blinden meistens nicht zu bedienen sind, haben Hilfsmittelanbieter spezielle Geräte für blinde Nutzer entwickelt.\n\nDie Produktpalette reicht vom kleinen Notizgerät, das nur über eine Sprachausgabe verfügt, bis zum Modell mit Notebook-Tastatur und integrierter Braillezeile.\n\n"}
{"id": "3171180", "url": "https://de.wikipedia.org/wiki?curid=3171180", "title": "PicPick", "text": "PicPick\n\nPicPick ist ein vor allem im asiatischen Raum verbreitetes Programm für Windows-Betriebssysteme zur Erstellung und Bearbeitung von Screenshots. Nach der Installation residiert es im Benachrichtigungsfeld, von dem aus alle Funktionen per Kontextmenü zugänglich sind.\n\nPicPick ist für die private Nutzung kostenlos. Kommerzielle Anwender müssen eine Lizenz erwerben, die bezüglich der Funktionalität jedoch unverändert ist.\n\nDie Erstellung von Screenshots ist auf zwei Arten möglich:\nPicPick kann zum einen die „Druck“-Taste übernehmen (ein so genannter \"Hook\"), so dass ein entsprechender Tastendruck automatisch einen Screenshot des Desktops oder, in Verbindung mit der \"Alt\"-Taste, des aktuellen Fensters in PicPick öffnet. Zum anderen bietet das Tray-Kontextmenü entsprechende Menüpunkte an, mit denen es auch möglich ist, einen frei wählbaren Bereich des Desktops zu „fotografieren“. Dies geschieht mittels der aus Grafikprogrammen wie IrfanView bekannten „Freihandauswahl“ oder „Rechteckauswahl“. Mit der Option „Fensterobjekt“ können einzelne Teile eines Fensters fotografiert werden. Damit ist es zum Beispiel möglich, Webseiten mit Bildlauf in einem einzigen Screenshot vollständig zu speichern.\n\nFür die mit PicPick erstellten Screenshots, aber auch beliebige andere Grafikdateien, stehen grundlegende Bearbeitungsfunktionen zur Verfügung (siehe Screenshot). So ist bspw. die „Verpixelung“ von Bildbereichen möglich, und es kann Text hinzugefügt oder bearbeitet werden.\n\nNeben den erwähnten Funktionen zur Erstellung und Bearbeitung von Screenshots stellt PicPick einige Sonderfunktionen für die allgemeine Grafikbearbeitung zur Verfügung, zum Beispiel ein Bildschirmlineal und einen Farbwähler, der den Farbwert einer beliebigen Stelle auf dem Bildschirm anzeigt. Auch ein Zeichenstift (Whiteboard), um den Bildschirm zu bemalen und Bereiche zu markieren, ist eingebaut. Seit Version 2.2 können erstellte Grafiken via Facebook oder Twitter veröffentlicht werden.\n\nUnterstützt werden die Formate mit den Dateinamenserweiterungen \".bmp,\" \".emf,\" \".gif,\" \".ico,\" \".jpg,\" \".png\" und \".wmf.\"\n\n"}
{"id": "3172149", "url": "https://de.wikipedia.org/wiki?curid=3172149", "title": "TV Computer", "text": "TV Computer\n\nDer TV Computer (kurz TVC) der ungarischen Firma Videoton (kurz VT) war ein Kleincomputer auf Basis einer mit 3,125 MHz getakteten Z80-CPU, der 1988 vor allem von staatlichen Stellen wie etwa Schulen eingekauft wurde. Bei dem Gerät handelte es sich um eine lizenzierte Anpassung des Enterprise-Heimcomputers, für das die Rechte bereits im Jahr 1984 erworben und gesichert wurden.\n\nDas Gehäusedesign ähnelte den anderen Systemen seiner Zeit, z. B. dem C64. Das Gerät gab es in zwei Grundversionen mit 32 oder 64 kB RAM, sowie später als fehlerbereinigte Version, den TVC 64k+, der für den Betrieb mit einem Diskettenlaufwerk vorbereitet war. Vom ersten Grundmodell wurden ca. 3000 produziert, vom zweiten Grundmodell waren es ca. 9000.\n\nAlle Geräte kamen mit HF-Modulator für die Ausgabe über ein TV-Gerät sowie einem RGB-Ausgang, mit dem man z. B. VGA-Monitore ansteuern konnte. Bei 16 kB Video-RAM waren Textmodi mit 64×24 Zeichen mit je 8×10 Pixeln möglich oder Grafikmodi mit 128×240 Pixeln bei 16 Farben bis hin zu 512×240 Pixeln bei 2 Farben. Das Betriebssystem bot eine Grafikschnittstelle an, die permanent eine Auflösung von 1024×960 Pixeln vorgaukelte.\n\nErgänzt wurde das Systemdesign durch Anschlüsse für Bandlaufwerk, Centronics-Drucker (ein solcher Drucker wurde von Videoton selbst angeboten), zwei Joysticks sowie verschiedene Erweiterungsports für ROM-Module und Zusatzkarten.\n\nIn der Grundversion war das Gerät mit 20k ROM ausgestattet, das ein TVC-BASIC V1.2 sowie ein Betriebssystem enthielt. Für die Plus-Version war TVC BASIC V2.2 beigepackt sowie ein Betriebssystem namens VT-DOS, das einen MS-DOS kompatiblen Befehlssatz hatte und in der Lage war, das FAT12-Diskettenformat für 360 bis 720 kB-Formate zu nutzen. Eine vollständige Kompatibilität mit dem PC-Format war aufgrund der Umsetzung von nationalen Sonderzeichen nicht gegeben. Programme für IBM-kompatible PCs waren prinzipiell nicht lauffähig. Über die nationalen Vertriebswege wurden ca. 40 bis 50 Programme aller Art als Zubehör angeboten. Eines der beliebtesten und bekanntesten Spiele war Hungaroring.\n\nDas Gerät wurde über einen Zeitraum von rund drei Jahren produziert, danach wurde wegen der hohen Preise und mangelnder Effizienz stattdessen der Commodore Plus4 als preiswertes Ersatzobjekt aus dem westlichen Ausland importiert.\n\nFür das Gerät existieren heute mehrere System-Emulatoren verschiedener Technologie und Reife, z. B. für die PC-Plattform oder für die Java-VM.\n\n"}
{"id": "3174794", "url": "https://de.wikipedia.org/wiki?curid=3174794", "title": "Prozesssimulation", "text": "Prozesssimulation\n\nDie Prozesssimulation ist ein Hilfsmittel zur Entwicklung und Optimierung der technischen Prozesse in verfahrenstechnischen oder chemischen Anlagen.\n\nDie Prozesssimulation ist im Wesentlichen ein Abbild chemischer Prozesse und Grundoperationen in Computerprogrammen. Für die Modellierung ist eine Reihe von Kenntnissen notwendig:\n\nDie Prozesssimulation sorgt dafür, dass die Stoff- und Wärmebilanzen stimmen und in ein stabiles Gleichgewicht gebracht werden. Meist werden die Prozesse gleichzeitig visualisiert.\n\nDie ersten Versuche, Prozesse elektronisch zu simulieren, wurde bereits in den 1950er Jahren auf elektronischen Analogrechnern vorgenommen. Diese Lösungsansätze wurden jedoch sehr schnell zugunsten von Simulationen auf Digitalrechnern aufgegeben.\n\nErste Entwicklungen in der digitalen Prozesssimulation chemischer Anlagen wurden in den 1970er Jahren begonnen, da hier das erste Mal geeignete Hardware und Software (hier insbesondere die Programmiersprache Fortran) zur Verfügung standen. Die Modellierung physikalischer Eigenschaften ist bereits wesentlich früher begonnen worden, hier sind beispielsweise kubische Zustandsgleichungen (siehe etwa Van-der-Waals-Gleichung) und Korrelationen (siehe etwa Antoine-Gleichung) zu nennen, die bereits im 19. Jahrhundert entwickelt wurden und heute teilweise noch verwendet werden. Auch Untersuchungen zur Kinetik chemischer Umsetzungen und zu Reaktionsmechanismen waren weit fortgeschritten. Apparateeigenschaften waren ebenfalls bereits weitgehend modelliert worden, so dass alle Werkzeuge zur Verfügung standen, vollständige chemische Prozesse in silico (ausschließlich mittels Computern) zu modellieren und zu berechnen.\n\nGleichzeitig hat die Entwicklung der Prozesssimulation die Fortentwicklung der diversen Modelle für die Abschätzung von Stoffeigenschaften, von Reaktionsmechanismen, deren Kinetik, von Apparateeigenschaften etc., aber insbesondere auch die Entwicklung von Faktendatenbanken stark beschleunigt. Faktendatenbanken dienen heute dazu, Abschätzverfahren und Korrelationen weiterzuentwickeln.\n\nUrsprünglich wurde die Prozesssimulation nur auf stationäre Anlagen angewandt. Dabei erhält man eine vollständige Massenbilanz und Energiebilanz eines stationären Zustandes auf der Basis von Modellen. Diese \"statische\" Simulation wird heute durch die \"dynamische\" Simulation ergänzt. Dynamisch bedeutet in diesem Zusammenhang, dass zeitlich abhängige Ergebnisse berechnet werden. Im Prinzip wird das Fließschema infinitesimal betrachtet und als Differenzialgleichungssystem numerisch berechnet. Dieses Verfahren benötigt eine wesentlich höhere Rechenleistung, erlaubt aber auch den Übergang zur Kontrolle und Führung chemischer Anlagen in Echtzeit. Ein einfaches Beispiel ist das Füllen oder Entleeren eines Behälters. Bei der dynamischen Simulation lassen sich insbesondere Regelvorgänge (PID-Regler), Holdups und chemische Reaktionen sehr realistisch darstellen.\n\nDas häufigste Phasengleichgewicht ist das Dampf-Flüssig-Gleichgewicht (meist mit VLE für Vapor-Liquid Equilibrium abgekürzt), welches insbesondere bei Gaswäschen und bei der Rektifikation von Bedeutung ist. Aber auch bei der Berechnung von Siede- und Tautemperaturen wird es angewandt. Bei idealen Stoffen, wie etwa Alkanen, genügt als Modell das Raoult-Daltonsche Gesetz, welches auf der Definition des Partialdrucks beruht. Bei nichtidealen Gemischen wird in der Flüssigphase der Aktivitätskoeffizient und in der Gasphase der Fugazitätskoeffizient berechnet und damit das Raoult-Daltonsche Gesetz korrigiert. Während der Fugazitätskoeffizient gut aus der Zustandsgleichung (häufig Soave-Redlich-Kwong) eines jeden einzelnen Stoffes in einem Gemisch berechnet werden kann, ist der Aktivitätskoeffizient von den binären Wechselwirkungen abhängig. In einem Gemisch mit z. B. 10 Inhaltsstoffen existieren 45 binäre Wechselwirkungen. Daher müssten in diesem Fall 45 VLE vermessen werden. VLE-Messungen sind in Datenbanken z. B. der DETHERM oder der DDB und in der Literatur wie z. B. DECHEMA Data Collection zu finden. Darin findet man auch die zugehörenden Parameter geeigneter Modelle wie z. B. Non-Random-Two-Liquid-Modell (NRTL). Für viele binäre Gemische, die nicht vermessen wurden, lassen sich die Modellparameter mittels der UNIFAC-Methode abschätzen. Das UNIFAC-Modell ist u. a. im VDI-Wärmeatlas beschrieben.\n\nJe stärker die Aktivitätskoeffizienten von eins abweichen, umso deutlicher unterscheidet sich das xy-Diagramm (x bezeichnet die Zusammensetzung der Flüssigkeit, y die Dampfzusammensetzung) von dem eines idealen VLE, bis es schließlich die Diagonale schneidet bzw. eine S-Kurve bildet, was das Zeichen für Azeotropie und ggfs. eine Mischungslücke ist. Dies kann man leicht am Portermodell demonstrieren.\n\nLetztendlich ist es auch möglich, mit dem Non-Random-Two-Liquid-Modell (NRTL) ein Liquid-Liquid-Equilibrium (LLE) zu berechnen, vorausgesetzt, die Parameter sind bekannt. Näherungsweise kann man mit VLE-NRTL-Daten durchaus ein LLE berechnen. Je größer die Mischungslücke ist, z. B. Benzol-Wasser, desto geringer ist der Fehler. Beim System N-Butanol-Wasser mit geringerer Mischungslücke ist die Näherung nicht akzeptabel. Mit geeigneten Daten können sogar komplexe LLE wie 3-Methylpyridin-Wasser mit elliptischen Gleichgewichtslinien berechnet werden.\n\nAuf der Basis geeigneter Daten für die Schmelzwärme kann man mit dem NRTL-Modell sogar Feststofflöslichkeiten (Fest-Flüssig-Gleichgewicht, kurz SLE für Solid-Liquid-Equilibrium) berechnen. Bei vielen Stoffen wie z. B. den sehr engsiedenden Stoffen 1-Methyl-Naphthalin und 2-Methyl-Naphthalin ergibt sich unmittelbar ein Eutektikum, dessen Lage eine gute Näherung zur Realität darstellt.\n\nDie in der Prozesssimulation verwendeten Stoffe werden aus einer Datenbank ausgewählt. Die Datenbank enthält Gase, Flüssigkeiten, Feststoffe. Polymere und Elektrolyte. Sie kann mit eigenen Stoffen und Daten per Regression erweitert werden. Die Datenbank bietet temperaturunabhängige Daten wie z. B. kritischer Druck und Temperaturfunktionen für bspw. den Dampfdruck, spezifische Wärmekapazität usw. Bekannte Datenbanken sind DETHERM und die Dortmunder Datenbank, die im Wesentlichen experimentelle Daten für Reinstoffe und Gemische enthalten, und die DIPPR-Datenbank, die im Wesentlichen Parameter für Gleichungen für reine Stoffe enthält. Mit Hilfe von Mischungsregeln lassen sich die Stoffdaten von Gemischen aus bekannten Reinstoffdaten näherungsweise berechnen. Für Stoffe, die nicht in den genannten Datenbanken enthalten sind, werden die Stoffdaten oft mit Inkrementmethoden wie z. B. Joback generiert.\n\nDie Rektifikation, häufig auch Destillation genannt, ist eine der zentralen Grundoperationen in der Prozess Simulation aber auch in der chemischen Verfahrenstechnik. Das ältere FUG Modell (Fenske-Underwood-Gilliland), welches für ideale Gemische eine schnelle und gute Näherung darstellt, spielt kaum noch eine Rolle. Vielmehr hat sich das Simultaneous Correction System (s. Perry's Chemical Engineering Handbook) durchgesetzt, welches nahezu alle Arten von Rektifikation gut modellieren kann wie z. B. Azeotrop-, Extraktions-, Reaktivdestillation, Trennblechkolonne, Gaswäsche, Absorption, Desorption, Elektrolyte, Seitenkolonne. Für petrochemische Destillationen ist auch durchaus noch das Modell Inside-Out in Gebrauch, da es schnell konvergiert und das Gemisch überwiegend aus Alkanen besteht.\n\nAuch die Batchdestillation lässt sich simulieren. Dabei werden meist die Algorithmen der kontinuierlichen Destillation verwendet. Mit der Batchdestillation kann ein Mehrstoffgemisch in zeitlicher Reihenfolge in einzelne Fraktionen aufgeteilt werden. Die mathematische Beschreibung der Batchdestillation erfolgt mit Hilfe der Rayleigh-Verteilung.\n\nDie bekanntesten Modelltypen sind der stöchiometrische, der Gleichgewichts- und der kinetische Reaktor. Der Gleichgewichtsreaktor lässt sich einerseits nach der Gibbs’schen Theorie als auch nach van’t Hoff modellieren. Für den kinetischen Reaktor verwendet man meist das Modell von Arrhenius. In Kombination mit VBA lassen sich kinetische Ansätze beliebig darstellen, z. B. für Bioreaktionen. Im Batchreaktor, auch diskontinuierlicher Rührreaktor genannt, werden Reaktionen zeitabhängig durch Lösen von Differenzialgleichungen simuliert.\n\nZur optimalen Bedienung der Prozesssimulation dienen Schnittstellen wie z. B. Excel zur Datenübertragung in eine Projektdatenbank oder eine Anlage. Mit Hilfe der MS-COM-Technik kann die Prozesssimulation sogar von Excel aus gesteuert, d. h. gestartet werden. Dadurch ist sogar eine Online-Simulation möglich, bei der Daten aus laufenden Anlagen der Prozesssimulation kontinuierlich zugeführt werden. Die Ergebnisse dienen der optimalen Prozessführung.\n\nSimulationsprogramme gibt es in großer Anzahl, die bedeutenderen sind bspw.\nGrößere Firmen haben oft auch Eigenentwicklungen in Benutzung, die ausschließlich firmenintern verwendet werden. Zwei Beispiele sind\n\nWährend die oben genannten Systeme in der Regel vorzugsweise zur Simulation reiner Fluidprozesse eingesetzt werden können, kann das Simulationsprogramm SolidSim sowie HSC speziell zur Simulation von Feststoffprozessen angewendet werden.\n\n"}
{"id": "3174820", "url": "https://de.wikipedia.org/wiki?curid=3174820", "title": "Deluge", "text": "Deluge\n\nDeluge (ehemals \"gTorrent\") ist ein freier BitTorrent-Client für Linux, macOS und Windows.\n\nUrsprünglich wurde das Programm von Zach Tibbitts und Alon Zakai, zwei Mitgliedern von ubuntuforums.org, als \"gTorrent\" („G“ für GNOME), mit der Absicht einen vollwertigen BitTorrent-Client für die Gnome-Desktop-Umgebung zu entwickeln, ins Leben gerufen. Noch vor der ersten Veröffentlichung wurde es umbenannt, um die Trugschlüsse zu vermeiden, dass Deluge nur unter GNOME liefe oder ein Programm aus dem GNOME-Projekt sei.\n\nMit der komplett neu geschriebenen Version 0.5 wurde am 18. März 2007 das Betastadium verlassen. Der zweiten „stabilen“ Veröffentlichung 0.5.1.1 wurden wichtige Funktionen, wie die Protokollverschlüsselung, Quellenaustausch und UPnP, hinzugefügt. Ab Version 0.5.4.1 ist Deluge auch für Mac OS (über MacPorts) und Windows verfügbar. Version 0.5.8 brachte einen anonymisierenden Webbrowser, um die Blockadeversuche von BitTorrent-feindlichen Internetdienstanbietern umgehen und unbeobachtet aus dem Programm heraus nach Torrent-Dateien suchen zu können.\n\nDie Version 1.0 sollte eigentlich als 0.6 erscheinen, wurde jedoch nach über einem Jahr eigenständiger Entwicklung als Version 0.9.01 in den Release-Candidate für die Version 1.0 umbenannt. Dabei handelt es sich um eine komplette Neuerstellung. Als wesentliches Merkmal sind ab der Version 1 die Benutzerschnittstellen zur Steuerung und Kontrolle vom Programmkern welcher die eigentlich Funktionen wie Up- und Download beinhaltet getrennt. Dazu ist der Programmkern () als Daemon ausgelegt, welcher je nach Konfiguration beispielsweise bei dem Start des Servers automatisch und ohne Benutzerinteraktion im Hintergrund gestartet werden kann. Er ist ohne Benutzeroberfläche als sogenannter lauffähig. Die Voreinstellung ist, ihn mit der Benutzerschnittstelle gemeinsam zu starten und beenden zu lassen. Dieser Modus wird bei Deluge in Erinnerung an die Version 0.x als bezeichnet.\n\nDer Thin Client kann sich auch auf einen physisch anderen Rechner als auf dem Rechner mit der Benutzerstelle befinden und ermöglicht so die Trennung von Front-End und Back-End. So kann beispielsweise der Programmkern auf einen externen Server in einem Rechenzentrum mit guter Internetanbindung betrieben werden, während sich die Steuerung und die Benutzeroberfläche, die auch nicht permanent aktiv sein muss, auf einen oder mehreren davon getrennten und externen Benutzerrechnern wie Laptops befinden kann.\n\nNeben dem optionalen grafischen GTK-Frontend wird auch ein Web-Frontend zur Steuerung mittels Webbrowser angeboten. Daneben besteht noch eine reine textorientierte Steuermöglichkeit mittels Kommandozeile in Form der die die Steuerung direkt über einen SSH-Kommandozeilen Zugang erlaubt.\n\nDeluge ist in Python geschrieben, hat eine grafische Benutzeroberfläche auf Basis von GTK+ (über PyGTK) und baut auf der C++-Programmbibliothek \"libtorrent\" auf, die mit Python-Anbindung versehen wurde. Das Kernprogramm verfügt über eine Schnittstelle für Module. Als Modul angelegte Funktionen lassen sich deaktivieren, womit sich der Funktionsumfang des Programms nach der Installation auch verringern lässt. Bereits jetzt sind einige der Module von projektexternen Entwicklern beigesteuert, werden aber alle mit Deluge ausgeliefert.\n\nFür grundlegende Funktionalität greift Deluge auf die Basis \"libtorrent\" zurück. Neben der Abfrage von Trackern wird mit Peers über eine verteilte Hashtabelle Quellenaustausch betrieben, um zusätzliche Gegenstellen zu finden. Dies kann sowohl nach dem im BitTorrent-Protokoll vorgesehenen \"Mainline-DHT\" (nach dem gleichnamigen Referenz-Client) als auch einen mit µTorrent kompatiblen Modus geschehen. Zudem lassen sich HTTP-Server als Quelle verwenden (Webseed). IPv6 wird unterstützt und Deluge kann Gegenstellen in lokalen Netzwerken erkennen.\n\nDie Benutzbarkeit verbessern Merkmale wie das Öffnen benötigter Ports hinter einer Firewall aus dem Programm heraus (über UPnP oder NAT-PMP). Die Internet-Nutzung lässt sich von der Tageszeit abhängig beschränken. Statt Dateien manuell zu verschieben, kann Deluge sie selbst verschieben und erübrigt so zeitaufwändiges Einlesen. Ebenso kann man Dateien automatisch verschieben lassen, sobald sie vollständig heruntergeladen wurden.\n\nZum Schutz der Privatsphäre können die Nutz- und Protokolldaten verschlüsselt werden. Anhand von IP-Bannlisten kann Deluge störende Gegenstellen ignorieren. Torrents können über einen anonymisierten Browser gesucht werden. Eine Webschnittstelle ermöglicht das Steuern des Clients über einen Webbrowser, jedoch wird bislang eine grafische Oberfläche zum Betreiben des Clients benötigt. Die Entwickler setzen für die Benutzung des Webschnittstelle Mozilla Firefox oder einen ebenfalls auf der Gecko-Engine basierenden Browser voraus.\n\nWeitere Module umfassen für BitTorrent-Clients übliche Funktionalitäten, wie das Anzeigen von ausführlichen Statistiken, das grafische Aufbereiten der benutzten Bandbreite oder die Benachrichtigung mittels Pop-up beim Beenden eines Downloads. Das Setzen von erwünschten Verteilungsverhältnissen und Geschwindigkeiten sowie Auswahl der erwünschten Dateien passiert für jedes Torrent individuell. Mittlerweile üblicher geworden sind ein integrierter Assistent zum Erstellen eigener Torrents und ein Broadcatcher zum automatisierten Herunterladen und Verteilen von Torrents nach Web-Feeds.\n\n\n"}
{"id": "3175046", "url": "https://de.wikipedia.org/wiki?curid=3175046", "title": "Snagit", "text": "Snagit\n\nSnagit ist primär eine Software des Herstellers TechSmith zur Erstellung und Verbesserung von Screenshots (Bildschirmfotos), kann aber auch zur Videoaufnahme des Bildschirms verwendet werden.\n\nDie erste Version kam 1990 auf den Markt als Windows-Version. Sie erweitert die Bildschirmfoto-Funktionalität des Betriebssystems (Taste „Druck“, engl. \"Print Screen\") um weitere Möglichkeiten wie direkter Bildschirmausschnitt-Auswahl (so dass ein Nachbearbeitungsschritt nach dem Erzeugen des Bildschirmfotos entfällt) oder der direkten anschließenden Kommentierung des Bildschirmfotos mit Pfeilen und Texten oder des vereinfachten Ausschneidens und Überlegens von nicht erwünschten Bildschirmbereichen.\n\nWesentlicher Unterschied zur inzwischen auch verbesserten Betriebssystem-Bildschirmfotofunktion von Windows Vista ist das seit vielen Jahren vorhandene „Fotografieren“ einer ganzen Internetseite auch von dem Bereich der nur über Bildschirm-Scrollen erreichbar ist, so dass man nicht mühselig mehrere Bildschirmfotos nachträglich zusammenfügen muss.\n\nNeuere Versionen (aktuell Version 12) legen den Schwerpunkt darauf, die vielen Funktionen über eine bessere Benutzeroberfläche dennoch so einfach bedienbar zu machen, als wenn sie nur Grundfunktionen böten.\n\nSeit Dezember 2009 gibt es auch eine Version Snagit for Mac. Die GUI der Mac-Version ist für eine Mac-Version typisch aufgeräumter als eine Win-Version. Innovativ ist, dass die in vielen Wettbewerberprodukten mit getrennt aufrufbaren unterschiedlichen Bildschirmfoto-Aufnahmemodi alle mit dem gleichen Aufruf erreichbar sind und je nachdem, wo sich der Mauszeiger befindet, es mal ein Bereichsfoto, ganzer Bildschirm, oder wenn man auf einen eingeblendeten Pfeil klickt, ein Scrollen stattfindet.\n\nFür Snagit wird entweder das Betriebssystem Microsoft Windows 10, Windows 8, oder Windows 7 (32-bit und 64-bit) empfohlen. Zur Installation werden 125 MB Festplattenspeicher benötigt. Als Prozessor wird mindestens eine Single-Core-CPU mit 2,4 GHz benötigt, für Videoaufnahmen wird ein 2,4 GHz Dual Core Prozessor empfohlen. RAM-Speicher wird mindestens 1 GB benötigt, empfohlen werden 2 GB oder mehr für Videos. Weitere Anforderungen: Internet Explorer 8.0 oder höhere Version; .NET 4.0 oder höhere Version für Video-Capture; Windows 7 kompatible Video- und Audiohardware zur Videoaufnahme auf Windows 7; Office 2000, Office XP für Snagit Add-Ins für Microsoft Word, Excel, PowerPoint und Outlook; Active Accessibility 2.0 für Captures von Webseiten.\n\nDie erste Voraussetzung ist ein Mac Computer mit Intel-Prozessor (Dual Core 2,0 GHz oder schneller). Als Betriebssystem wird Mac OS X 10.6.8 (Snow Leopard), 10.7 (Lion) oder eine höhere Version empfohlen, sowie 2 GB RAM oder mehr. Es ist ein Upgrade von Version 1.0.3 auf Version 1.0.4 erforderlich, bevor Mac Version 2.0 installiert wird. Eine Lizenz ist aus dem Mac App Store oder direkt beim Herstellershop erhältlich.\n\n"}
{"id": "3175680", "url": "https://de.wikipedia.org/wiki?curid=3175680", "title": "NonVisual Desktop Access", "text": "NonVisual Desktop Access\n\nNonVisual Desktop Access (NVDA) ist ein kostenloser, portabler und quelloffener Screenreader, der blinden Menschen die Nutzung von Computern mit dem Betriebssystem Windows ermöglicht.\n\nDas Projekt wurde 2006 vom Australier Michael Curran begonnen, der wie die meisten freiwilligen Mitarbeiter selbst blind ist. NVDA ist in Python programmiert. Von der Version 2010.2 Beta1 an wird der Bildschirminhalt auch mittels Display-Hooking ausgelesen. Jedoch verwendet es standardmäßig Accessibility-Frameworks wie Microsoft Active Accessibility (MSAA), das neuere User Interface Automation oder die Java Access Bridge (JAB). Das Projekt verwendet die GNU General Public License.\n\nNVDA enthält den integrierten Sprachsynthesizer eSpeak und unterstützt zusätzlich weitere Sprachsynthesizer wie u. a. SAPI-Synthesizer. Die Ausgabe auf Braillezeilen ist von der Version 0.6p3 an offiziell möglich.\n\nNeben den generellen Windows-Funktionalitäten arbeitet NVDA mit WordPad, Notepad, Outlook Express und dem Internet Explorer. Es unterstützt die grundlegenden Funktionen von Microsoft Word und Excel 2000/XP/2003/2007/2010. NVDA unterstützt sehr gut die freien Bürosoftwarepakete LibreOffice und Apache OpenOffice (beide mittels der JAB) sowie den Webbrowser Mozilla Firefox (Version 2 oder höher); Letzterer ist der empfohlene Webbrowser für maximale Barrierefreiheit mit NVDA. ARIA wird ebenfalls unterstützt, so dass Webanwendungen künftig wesentlich besser von blinden Menschen genutzt werden können.\n\nNVDA läuft unter Windows XP (bis Version NVDA 2017.3) 7 und 8. Anfängliche Unterstützung für Windows 10 ist von NVDA Version 2015.3 an enthalten; insbesondere die Unterstützung von Microsoft Edge ist im Moment aber noch recht experimentell.\n\nDie Entwicklung und der Support finden statt im Rahmen der gemeinnützigen Gesellschaft NV Access, die 2007 in South East Queensland, Australien gegründet wurde. Das Projekt wird durch Spenden finanziert. Das Ziel von NV Access ist die kostengünstige Möglichkeit sehbehinderten und blinden Menschen aus allen sozialen Schichten den Zugang zu Computern zu ermöglichen. NV Access hat verschiedenste Sponsoren wie: Adobe Systems, The Nippon Foundation und Google.\n\n"}
{"id": "3176901", "url": "https://de.wikipedia.org/wiki?curid=3176901", "title": "Pocket PC Magazin", "text": "Pocket PC Magazin\n\nDas Pocket PC Magazin war eine zweimonatlich erscheinende deutsche Computerzeitschrift, die auf Themen rund um Pocket PCs spezialisiert war.\nZwischen den Ausgaben 11/05 und 03/08 erschien das Magazin monatlich, dies wurde dann wieder auf den Zwei-Monats-Takt geändert.\n\nHerausgegeben wurde die Zeitschrift von der Bikini Verlag GmbH. Der Herausgeber und Chefredakteur Gerhard Bauer ist gleichzeitig Gesellschafter der Bikini Verlag GmbH.\n\nDie Zeitschrift erschien seit dem Jahr 2000; frühe Ausgaben aus dem Jahr 2000 erschienen noch unter dem Titel „Windows-CE-Magazin“.\n\nDie letzte Ausgabe des Magazins war 1/09. Mit dieser Ausgabe wurde das Heft eingestellt. Die Themen sollten in die ebenfalls im Bikini Verlag erscheinende \"NOTEBOOK\nOrganizer & Handy\" eingegliedert werden, was auch in der Ausgabe 3–4/2009 getan wurde. Diese Ausgabe war aber gleichzeitig die letzte Ausgabe des Magazins.\n"}
{"id": "3179148", "url": "https://de.wikipedia.org/wiki?curid=3179148", "title": "Sunbelt Personal Firewall", "text": "Sunbelt Personal Firewall\n\nDie Sunbelt Personal Firewall (SPF) war eine Personal Firewall für Microsoft Windows. Die Software wurde früher unter dem Namen Kerio Personal Firewall und Sunbelt Kerio Personal Firewall vertrieben und wurde im Mai 2011 letztlich eingestellt.\n\nDie Sunbelt Personal Firewall steht für Windows XP (Home, Professional und Media Center Edition), Windows 2000 Professional sowie seit Version 4.6 auch für Windows Vista zur Verfügung. Die Betriebssysteme Windows 98 und ME wurde seit Version 4.2 nicht mehr unterstützt. Ebenfalls nicht unterstützt wurden 64-Bit-Versionen von Windows. Das Programm wurde als Shareware vertrieben, die für die ersten 30 Tage ohne Funktionsbeschränkungen eingesetzt werden konnte. Danach konnte die Software entweder käuflich erworben werden oder als limitierte Edition weiter genutzt werden. In der kostenlosen Version fehlten Content Filter, hostbasiertes IDS, Fernwartungszugang sowie die Möglichkeit, die Firewall auf einem Router einzusetzen.\n\nDie Firewall entstammte der \"Tiny Personal Firewall\". Die Firma \"Kerio Technologies Inc\". erwarb die Rechte an dem Produkt, entwickelte es weiter und veröffentlichte im März 2002 die endgültige Version 2.1 unter dem Namen \"Kerio Personal Firewall\". Die Software war Freeware. Mit Version 4 wurde 2003 der Funktionsumfang von Kerio stark erweitert und eine kommerzielle Version eingeführt. Im September 2005 stellte Kerio die Entwicklung des Produkts ein. Kerio-Mitarbeiter begründeten dies im Produkt-Forum damit, dass die Firewall nicht profitabel sei. Ein Firmensprecher erklärte, Kerio wolle sich auf die Entwicklung des \"Kerio-MailServers\" und der \"WinRoute Firewalls\" konzentrieren. Im Dezember 2005 wurde die \"Kerio Personal Firewall\" von der Firma \"Sunbelt Software\" übernommen. Bei Sunbelt war die Software zunächst unter dem Namen \"Sunbelt Kerio Personal Firewall\" erhältlich. Im April 2007 wurde der Name auf \"Sunbelt Personal Firewall\" geändert.\nSunbelt vertreibt eine geänderte Version seit 2013 unter dem Namen Vipre Internet Security.\n\n"}
{"id": "3182085", "url": "https://de.wikipedia.org/wiki?curid=3182085", "title": "Windows-1256", "text": "Windows-1256\n\nWindows-1256 ist eine 8-Bit-Zeichenkodierung des Windows-Betriebssystems. Sie basiert auf Windows-1252 und deckt im Gegensatz zu ISO 8859-6 Arabisch, Persisch und Urdu vollständig ab und enthält auch alle Vokalzeichen. Ebenso enthält der Zeichensatz zahlreiche Kleinbuchstaben zur Schreibung des Französischen, vor allem um die Kommunikation im Maghreb-Raum zu erleichtern. Seit Juli 2016 verwenden 0,1 % aller Websites Windows-1256, ISO-8859-6 wird von weniger als 0,1 % verwendet.\n\nDie folgende Tabelle zeigt das Repertoire von Windows-1256. Pink gefärbte Felder stellen Vokalzeichen dar.\n\n"}
{"id": "3187638", "url": "https://de.wikipedia.org/wiki?curid=3187638", "title": "K9Copy", "text": "K9Copy\n\nK9Copy ist eine freie DVD-Authoring-Software für Linux und wird allgemein als Äquivalent zum Windows-basierten DVD Shrink gesehen.\n\nEs komprimiert den Inhalt einer Dual-Layer Video-DVD (DVD-9), um ihn auf eine Single-Layer-DVD (DVD-5) brennen zu können. Dies wird erreicht, indem die Videospur neu enkodiert wird oder Inhalte wie etwa Audiospuren einzelner Sprachfassungen, Bonusmaterial, Untertitel, Menüs weggelassen werden.\n\nDas Programm ermöglicht es, DVDs als ISO-Abbild auf die Festplatte abzuspeichern. Dieses Abbild kann dann durch die Verwendung eines Brennprogrammes wie beispielsweise Brasero oder K3b auf eine DVD gebrannt werden. Ab der Version 2 setzt K9Copy auf KDE4.\n\nK9Copy ist bei einigen populären Linux-Distribution wie z. B. Ubuntu direkt aus den Standard-Paketquellen installierbar.\n\nIm Juli 2011 wurde bekanntgegeben, dass keinerlei Weiterentwicklung mehr stattfindet.\n\nUnter der Bezeichnung \"K9copy-Reloaded\" wurde das Projekt aber ebenfalls schon 2011 durch einen anderen Entwickler bei sourceforge wieder aufgenommen und seitdem fortgeführt.\n\n"}
{"id": "3192340", "url": "https://de.wikipedia.org/wiki?curid=3192340", "title": "XQuartz", "text": "XQuartz\n\nXQuartz, vormals X11 (auch codice_1), ist Apples Umsetzung des X Window System (X11) für macOS (bis 2016 „OS X,“ davor bis 2012 „Mac OS X“). Apples Umsetzung von X11 basiert auf dem X.Org-Server und erweitert diesen unter anderem um die hardwarebeschleunigte 2D-Darstellung Quartz, hardwarebeschleunigtes OpenGL und integriert ihn besser in Aqua, die grafische Benutzeroberfläche von macOS.\n\nX11 war für Mac OS X 10.2 („Jaguar,“ 2002) in einer Beta-Version erhältlich und wurde später zu einem Standardpaket für Mac OS X Panther (10.3, 2003). Es kann bei der Installation als optionales Paket gewählt, nachträglich von den Installationsmedien installiert oder von Apples Homepage heruntergeladen werden.\n\nBeim optionalen Installationspaket von Mac OS X Tiger (10.4, 2005) wurde X11 Version 6.6 (X11R6.6) als Basis verwendet. Diese Implementation beinhaltete einen auf XFree86 4.4 basierenden X-Server, einen Fenstermanager, der „rootless“ war, und grundlegende Hilfsprogramme wie beispielsweise xterm. „Rootless“ bedeutete, dass die X-Anwendungen auf dem Quartz-Desktop erschienen, wobei sie wie jede andere Anwendung in einem ganz normalen Fenster liefen und nicht in einer eigenen Umgebung wie zum Beispiel bei Virtual PC.\n\nIn Mac OS X Leopard (10.5, 2007) bis Mac OS X Lion (10.7, 2011) wurde X11 standardmäßig mitinstalliert. Es wurde umgeschrieben und basierte nun auf dem populäreren X.Org-Server (X11R7.2) anstatt auf XFree86.\n\nSeit OS X Mountain Lion (10.8, 2012) ist X11 kein Bestandteil des Betriebssystems mehr. Apple unterstützt stattdessen die Weiterentwicklung von XQuartz und verweist auf dieses externe Projekt.\n\n"}
{"id": "3196122", "url": "https://de.wikipedia.org/wiki?curid=3196122", "title": "Strigi", "text": "Strigi\n\nStrigi ist ein freier Daemon zur Indizierung von Informationen zur Desktopsuche.\n\nDie Software ist portabel, hat einen geringen Speicherverbrauch und ist sehr schnell.\n\nDer in C++ geschriebene Daemon indiziert mit einem Crawler, der mit niedrigster Priorität (also bei Untätigkeit) im Hintergrund durch Dateien im Dateisystem der Festplatte, in Archivdateien, gespeicherte E-Mails, Chatlogs etc. läuft. Als Index-Backend können derzeit wahlweise hyperestraier, sqlite3, xapian oder üblicherweise CLucene, der schnellste der vier, verwendet werden. CLucene ist eine C++-Portierung von Lucene und basiert auf den JStreams-Klassen, die dem Durchsuchen von in Dateien enthaltenen Informationen dienen.\n\nZu jeder Datei wird eine SHA-1-Prüfsumme gespeichert, um identische Inhalte effizient erkennen zu können.\n\nIns Leben gerufen wurde Strigi von Jos van den Oever, der die JStreams-Klassen geschrieben hat, die dann in CLucene integriert wurden. Dieser Crawler sollte ursprünglich in Kat integriert werden, wobei er dann jedoch wegen schleppender Entwicklungen beim Kat-Projekt einen eigenen kleinen Daemon dafür geschrieben hat, der dann damit indiziert. Der Name kommt von den Strigiformes, dem lateinischen Namen für die biologische Ordnung der Eulen.\n\nDer Strigi-Daemon stellt nur das Backend für eine Suchfunktionalität dar. Die Frontends werden separat von Arbeitsumgebungen und Anwendungsprogrammen gebildet. KDE 4 nutzt standardmäßig die Strigi-Schnittstelle zur Dateiindizierung und kombiniert dies mit der NEPOMUK-Datenbasis, deren Meta-Daten im Strigi-Index mitverwendet werden. Benutzergerichtete Suchfunktionen werden einerseits direkt von KIO-Slaves in den Dateimanagern Dolphin und Konqueror sowie allen Datei-Dialogen angeboten, es existieren aber auch dedizierte grafische und kommandozeilenorientierte Clients.\n\nDie Desktop-Umgebung Gnome nutzt nach Beagle hauptsächlich die eigene Lösung \"Meta Tracker\" zur Indizierung und Suche, es existiert aber ein Plug-in zur Integration der Strigi-Funktionalität in die Such- und Navigationsleiste \"Deskbar\".\n\n"}
{"id": "3200121", "url": "https://de.wikipedia.org/wiki?curid=3200121", "title": "Docking (Chemie)", "text": "Docking (Chemie)\n\nMolekulares Docking (kurz Docking, dt.: Ankuppeln, Einpassen) ist ein bioinformatisches/chemoinformatisches Verfahren, mit dem der Bindungsmodus und idealerweise auch die Bindungsenergie zweier aneinanderbindender Biomoleküle vorhergesagt wird. Docking wird typischerweise in der molekularbiologischen und der pharmazeutischen Forschung eingesetzt. Ein Hauptanwendungsgebiet ist die Suche nach Wirkstoffkandidaten für ein pharmazeutisch relevantes Problem. Mit Hilfe von Dockingmethoden können große Substanzmengen virtuell auf Bindung an ein Zielmolekül getestet werden. Diese \"in silico\" Bindungsstudien sind deutlich schneller und kostengünstiger als vergleichbare Verfahren im Nasslabor, jedoch in aller Regel von spürbar geringerer Genauigkeit. \n\nDie Vorhersage findet auf Basis der bereits bekannten chemischen und räumlichen Struktur der beiden Ausgangsmoleküle statt. Die Struktur und die frei werdende Bindungsenergie des Komplexes, der aus den beiden Molekülen gebildet wird, ist vor der Berechnung unbekannt. Als Lösung erhält man eine möglichst gute Näherung der Komplexstruktur und je nach Methode auch eine Abschätzung der Bindungsenergie des Komplexes. \n\nDie beteiligten Moleküle sind in der Regel hochkomplex, weswegen eine A-priori-Berechnung des gebundenen Zustandes derzeit aufgrund der dafür notwendigen enormen Rechenkapazitäten nicht möglich ist. Daher verwenden alle relevanten Algorithmen starke bis sehr starke Näherungen der zugrundeliegenden Physik bzw. Chemie, um eine gute Abschätzung des Bindungsmodus zu berechnen. \n\nEs gibt verschiedene Ansätze, um die Komplexität des Problems im Zaum zu halten. Der einfachste strukturelle Ansatz ist das starre Docking, das auf der vereinfachenden Annahme aufbaut, dass sich die beteiligten Moleküle während des Bindungsvorgangs räumlich nicht verändern. Weiterhin gibt es Ansätze, die einen der Bindungspartner starr halten, während der zweite (teil-)flexibel modelliert wird. Letzteres findet häufig beim Ligandendocking Anwendung, da hier kleine, wirkstoffähnliche Moleküle betrachtet werden, deren vergleichsweise geringe räumliche Komplexität eine flexible Betrachtung handhabbar macht. \n\nDa es sich um dreidimensionale Objekte handelt, ist die gegenseitige Positionierung und Orientierung der Moleküle im gebundenen Zustand, d. h. im Zustand der geringsten Energie, von Interesse. Zusätzlich können die Moleküle selbst beim Prozess Änderungen in ihrer Konformation erfahren. Die Parameter dieses unbekannten Endzustands könnten zwar auch experimentell bestimmt werden, dies ist jedoch zu aufwändig; daher werden computergestützte Methoden verwendet, um die virtuelle Räumlichkeit des biochemischen Komplexes zu berechnen und dabei realen Verhältnissen möglichst nahezukommen.\n\nDocking unterteilt sich je nach Art der Bindungspartner in die Untergebiete\n\n\nDiese Unterteilung ist notwendig, da die Eigenschaften der jeweils am Komplex beteiligten Bindungspartner die Verwendung spezieller Algorithmen verlangen.\n"}
{"id": "3208549", "url": "https://de.wikipedia.org/wiki?curid=3208549", "title": "LittleCMS", "text": "LittleCMS\n\nLittleCMS (LCMS) ist eine plattformübergreifend verfügbare freie Software für Farbmanagement.\nDie Software besteht aus einer (mit einem Speicherbedarf von unter 100k) sehr schlanken Programmbibliothek (liblcms). Dazu gibt es mit \"LPROF\" (oder \"LCMS Profiler\") ein grafisches Werkzeug, um Farbprofile zu erstellen, und mehrere Kommandozeilenwerkzeuge. Zu ihnen gehören \"tifficc\" und \"jpegicc\", um ICC-Profile in TIFF- beziehungsweise JFIF-Dateien einzubetten sowie \"icc2ps\" für PostScript-Dateien und \"icclink\", um DeviceLink-Profile zu erzeugen.\n\nLCMS wurde 1998 von Marti Maria ins Leben gerufen, war eines der ersten freien Farbmanagement-Systeme und ist das einzige mit einer grafischen Oberfläche. Anfang 2004 stellte Maria die Unterstützung von LPROF ein. Im August 2005 wurde es zu einem SourceForge-Projekt, das jetzt von anderen weiter betreut wird. Im Rahmen der Freigabe von Java als freie Software wurde die unfreie Farbmanagement-Komponente des JDK durch LittleCMS ersetzt.\n\nAuf Linux-Systemen ist es das verbreitetste seiner Art. Es wird unter anderem in Programmen wie Scribus, Firefox, CinePaint, Krita, Picasa und ImageMagick eingesetzt.\n\n"}
{"id": "3213329", "url": "https://de.wikipedia.org/wiki?curid=3213329", "title": "MemoQ", "text": "MemoQ\n\nMemoQ ist ein Übersetzungstool (CAT), das ähnlich wie Déjà Vu, Trados oder Wordfast Pro über eine eigene Programmoberfläche verfügt und datenbankgestützte Übersetzungen ermöglicht.\n\nInitiator und Entwickler des Programms ist die ungarische Firma Kilgray.\n\nSeit 2009 organisiert Kilgray in jedem Frühjahr eine Konferenz in Budapest unter dem Titel \"memoQfest\", die sich mit verschiedenen wirtschaftlichen und technischen Aspekten der Übersetzungsbranche befasst.\n\nDie verschiedenen Versionen von memoQ umfassen jeweils mehrere Unterversionen, so dass z. B. auch von einer Version 5.0.56 gesprochen wird. Die Aktualisierungen erfolgen unregelmäßig, es gibt also keine größeren \"Service Packs\" wie bei anderen Produkten. Diese Aktualisierung kann optional automatisch nach dem Beenden des Programms erfolgen. \n\nWenige Wochen vor der Freigabe neuer Versionsstufen ist eine freiwillige Teilnahme am Betaprogramm möglich.\n\nSämtliche Hinweise in diesem Abschnitt beschränken sich auf die Client-Version.\n\nVersion 2.0 ist die erste „offizielle“ Version. Sie wurde im Januar 2007 veröffentlicht und verfügt über ein QA-Add-in zur Qualitätssicherung.\n\nVersion memoQ 2.1 vom Mai 2007 verfügt über kontextsensitive TM-Einträge oder so genannte „101%-Treffer“, wobei Kontext das vorhergehende oder nachfolgende Segment ist. Segmente in der Übersetzungsspalte können nun gesperrt werden, zudem ist ein automatischer Import/Export von Ordnerstrukturen einschließlich Unterordnern möglich. Exportregeln zum Festlegen von Dateinamen können festgelegt werden. \n\nNeben der Unterstützung zweisprachiger RTF-Dokumente kann die im November 2007 auf den Markt gebrachte Version 2.2 \"Ansichten\" erstellen, das heißt mehrere Dokumente (auch unterschiedlichen Formats) können in einer \"Ansicht\" zusammengefügt und gemeinsam übersetzt werden, ohne diese Dokumente einzeln öffnen zu müssen. Version 2.3 (März 2008) erweitert das Programm um eine Live-Vorschau des Übersetzungsdokuments (bei bestimmten Formaten).\n\nIm September 2008 wurde Version 3.0 veröffentlicht. Neu war das Aktualisieren von Dokumenten dank zweisprachigem tabellarischem Rich-Text-Format. Damit können Dokumente in Word korrigiert und erneut importiert werden. Zu weiteren Neuheiten gehörten die vollständige XLIFF-Unterstützung, die Rechtschreibprüfung mit Hunspell (als Alternative zur Rechtschreibprüfung von Microsoft Office), eine neue, erweiterte Termdatenbank und Filtern nach Segmentstatus. \n\nVersion 3.2 (November 2008) verbesserte das Einfügen von Fragmenten und bietet Auto-Korrektur-Listen für die automatische Korrektur von Tippfehlern. Wenige Monate später (März 2009) führte Version 3.5 die automatische Konkordanzsuche LSC, sowie automatische Platzhalter bei der Konkordanzsuche ein. Hinzu kommen ein STAR-Transit- sowie ein PPTX-Filter und die XML-Vorschau.\n\nDie letzte Version dieser Reihe (3.6 vom September 2009) unterstützt 64-bit-Betriebssysteme und bietet neben einem DOCX-Filter auch Fließtext-Import aus PDF-Dateien.\n\nVersion 4.0 wurde am 1. Februar 2010 veröffentlicht. Das QA-Add-in wurde nun integraler Bestandteil des Programms. Weitere wesentliche Änderungen sind neben der neu gestalteten Benutzeroberfläche und dem neu programmierten Texteditor erweiterte Serverfunktionen (Weitergabe von Parametern und Einstellungen über Dokumente, TM und Terminologie hinaus).\n\nAm 22. Oktober 2010 kam Version 4.5 heraus. Neben dem neu programmierten TM-Engine lag der Schwerpunkt auf dem verbesserten Alignment bereits bestehender Dokumentenpaare und der LiveDocs-Funktion für den Einsatz ein- und zweisprachiger Referenzdokumente.\n\nVersion 5.0 wurde am 8. September 2011 veröffentlicht. Zu den Besonderheiten dieser Version zählen unter anderem:\n\nVersion 6.0 wurde am 3. Juli 2012 veröffentlicht.\n\n\nVersion 8.0 (damals noch memoQ Adriatic genannt) wurde am 22. Februar 2017 veröffentlicht.\n\nMemoQ kann unter anderem folgende Formate verarbeiten:\nÜber die eigentlichen Importfilter hinaus können bestimmte Filter verkettet werden, so dass beispielsweise HTML aus XML zu extrahieren, was für die Übersetzung von CMS-Exporten hilfreich sein kann. Jeder Filter kann außerdem mit einem einfachen Regulärer Ausdruck-Tagger verknüpft werden, um Variablen, Entitäten und Tags gesondert zu schützen. \n\nSeit Version 5.0 ist memoQ sowohl zu Dokumentformaten von Trados 2007 und 2009 (ttx- und sdlxliff-Dateien) als auch zu Wordfast (TXML) vollständig kompatibel. Installationen in virtuellen Umgebungen wie z. B. Parallels werden vom Hersteller offiziell unterstützt. Zudem wird Wert auf eine Kompatibilität zu Spracherkennungssoftware gelegt.\n\nDurch die Offenheit gegenüber externen Austauschformaten konnte memoQ vor allem bei Freiberuflern und kleinen Agenturen große Marktanteile erobern.\n\nEine Besonderheit ist das zweispaltige Rich-Text-Format. Damit werden Übersetzungsdokumente und Ansichten in ein Format exportiert, das mit Office-Programmen angezeigt, bearbeitet und gedruckt werden kann. Mit diesem Format können Übersetzer in Projekte eingebunden werden, die kein CAT-Programm verwenden. \n\nDéjà Vu bietet bereits länger eine ähnliche Funktion an. Bei SDL Trados kann dieser Ablauf – allerdings umständlicher – über ein Plug-in nachvollzogen werden.\n\nDie Serverversion war in frühen memoQ-Versionen ein Alleinstellungsmerkmal; mittlerweile werden diese Funktionen zumindest teilweise auch von anderen Programmen angeboten. Es gibt für den Benutzer des Clients verschiedene Arten von Serverprojekten:\n\n\nIn allen Fällen kann der Übersetzer ergänzend eigene, lokale Ressourcen nutzen (die bei einer Rückgabe als mqback-Datei nicht übermittelt werden). Bei vollständigen Online-Projekten (3. Fall) kann der Kunde den Dateizugriff (Export von Dokumenten und TM) beschränken.\n\nEine Lizenz für memoQ kostet derzeit 620 € und umfasst drei Jahre Support und Upgrades auf alle neuen Programmversionen. Eine Verlängerung kostet pro Jahr 124 €. Neben der kostenpflichtigen Version \"translator pro\" wird eine kostenlose Version \"4free\" mit eingeschränktem Funktionsumfang angeboten.\n\nAnders als bei anderen Anbietern gibt es bei Kilgray keine Zertifizierungsprogramme für Anwender. Dagegen werden auf der Unternehmenswebsite Schulungsvideos und aufgezeichnete Webinare kostenlos zur Verfügung gestellt.\n\nEine zeitnahe Unterstützung wird neben dem Support des Herstellers über eine Yahoo!-Diskussionsgruppe angeboten (auf Englisch), auf der Anwender und Mitarbeiter des Herstellers aktiv sind.\n\nZu den Kritikpunkten an memoQ gehörten bei früheren Versionen die vergleichsweise späte Einführung der Filter für OpenOffice sowie docx- und pptx-Dateien. Zudem mussten bei Serverprojekten die Programmversion des Servers und der darauf zugreifenden lokalen Einzelplatzversion übereinstimmen. Mittlerweile kann für die Übersetzung auch ein um eins höhere oder niedrige Version des Clients verwendet werden. Für die Projektverwaltung auf dem Server wird weiterhin eine identische Hauptversion benötigt.\n\n"}
{"id": "3214108", "url": "https://de.wikipedia.org/wiki?curid=3214108", "title": "4NT", "text": "4NT\n\n4NT ist eine alternative Betriebssystem-Shell von JP Software für Windows. Die Software stellt einen Ersatz für den Befehlsprozessor cmd.exe dar, der ab Microsoft Windows NT standardmäßig mitgeliefert wird.\n\n4NT basiert auf der Vorgänger-Shell 4DOS für DOS und Windows-Versionen bis einschließlich Windows Me, ältere Versionen hießen \"4DOS für die Windows NT-Betriebssysteme\".\n\nMit Version 9.00 wurde 4NT unter dem Namen \"Take Command Console\" ein Teil der Shell \"Take Command\", die ebenfalls als Shareware vertrieben wird und zusätzlich, ähnlich der Windows PowerShell, eine grafische Oberfläche bietet.\n\n4NT stellt eine reichhaltige Funktionalität sowohl für die Kommandozeile als auch für die Stapelverarbeitung zur Verfügung. Es kann mit anderen Scriptsprachen zusammenarbeiten, beispielsweise REXX, Ruby und Perl, oder Windows-Scriptsprachen wie VBScript und JScript, um so weitergehenden Zugang zu Betriebssystemfunktionen zu bieten.\n\n4NT bietet im Vergleich zu cmd.exe eine Reihe von Verbesserungen:\n\n\n"}
{"id": "3216836", "url": "https://de.wikipedia.org/wiki?curid=3216836", "title": "MacBook Air", "text": "MacBook Air\n\nDas MacBook Air ist ein Subnotebook des kalifornischen Unternehmens Apple. Es wurde am 15. Januar 2008 von Steve Jobs auf der Macworld San Francisco vorgestellt und als „das dünnste Notebook der Welt“ beworben.\n\nAm 20. Oktober 2010 wurde unter dem Motto „Back to the Mac“ ein komplett überarbeitetes Modell präsentiert, das auch in einer 11,6 Zoll-Variante erhältlich ist.\n\nEine aktualisierte Version wurde am 20. Juli 2011 veröffentlicht. Auch diese Version ist als 11,6 Zoll- und 13,3 Zoll-Variante erhältlich, wobei die größten Änderungen der Einsatz der aktuellen Intel Sandy-Bridge-Prozessoren und die Tastaturbeleuchtung sind.\n\nAm 11. Juni 2012 veröffentlichte Phil Schiller ein aktualisiertes MacBook Air, welches sich vor allem durch die neuen Intel Ivy-Bridge-Prozessoren unterscheidet. Beim günstigsten Modell wird nun als Standard 4 GB RAM verbaut, die maximale Flashspeichergröße beträgt 512 GB. Außerdem wurden die USB-2.0-Ports durch die neuen USB 3.0 getauscht, was von Kritikern lange gefordert wurde. Außerdem besitzt das MacBook Air nun einen MagSafe-2-Anschluss, der dünner und breiter ist. Der zuvor verwendete MagSafe-(1)-Anschluss ist (geometrisch) nicht kompatibel. Apple verkauft einen Adapter, der einen alten MagSafe-(1)-Stecker an die MagSafe-2-Buchse (der neueren Rechner) anpasst.\n\nAm 10. Juni 2013 wurde ein neues MacBook Air mit Intels neuen Haswell-Prozessoren vorgestellt. Dadurch konnte die Batterielaufzeit auf 9 bzw. 12 Stunden erhöht werden.\n\nAm 9. März 2015 wurden aktualisierte Versionen für beide Gerätegrößen (11,6″ und 13,3″) mit Intel Core i5- und i7-Prozessoren der 5. Generation (Broadwell) vorgestellt. Dabei wurde außerdem der Thunderbolt 1-Anschluss durch einen Thunderbolt 2-Anschluss ersetzt.\n\nDas Gerät, dessen Außenhülle aus Aluminium gefertigt ist, läuft zur Vorderseite keilförmig zu. Es ist zwischen 2,8 und 17 Millimetern hoch und wiegt etwa 1,08 bzw. 1,35 Kilogramm. Für dieses Design bekam Apple Anfang Juni 2012 ein \"design patent\" zugesprochen.\n\nDas MacBook Air war nach dem 1997 von Mitsubishi und Hewlett-Packard entwickelten Pedion zum Veröffentlichungstermin das zweitdünnste Notebook der Welt, dicht gefolgt von einer Sonderedition des Sony Vaio X505 aus dem Jahr 2004.\nIm Jahr 2009 wurden mit dem Dell Adamo und dem Sony Vaio X (14 Millimeter Höhe) noch dünnere Notebooks vorgestellt.\n\nDas Display des MacBook Air misst 13,3″ und ist damit ebenso groß wie das der MacBook-Modelle. Das 13,3″-MacBook Air bietet eine native Auflösung von 1440 × 900 Pixeln (16:10) gegenüber dem 11,6″-Display mit einer nativen Auflösung von 1366 × 768 Pixeln (16:9). Beide Displays sind mit LEDs beleuchtet. Die Tastatur hat wieder eine Hintergrundbeleuchtung. Über dem Bildschirm ist die mittlerweile in allen Apple-Notebooks eingebaute iSight-Kamera, die Apple FaceTime-Kamera genannt hat. Das Mikrofon ist an die linke Seite des Unterteils neben den Kopfhöreranschluss gewandert.\n\nAls Neuerung bietet das Trackpad einige der vom iPhone und dem iPod touch bekannten Multi-Touch-Funktionen. So ist es möglich, mit Gesten eines oder mehrerer Finger Steuerungen oder Funktionen direkt und vereinfacht auszuführen, wenn Programme dies unterstützen.\n\nDas neue Gerät hat mehr Anschlüsse als der Vorgänger. Neben dem magnetischen Netzstecker \"(MagSafe 2)\" auf der linken Seite befinden sich ein USB-3.0-Anschluss, der Kopfhöreranschluss und das eingebaute omnidirektionale Mikrofon. Der Thunderbolt-Port ist an der rechten Seite angebracht, daneben ein weiterer USB-3.0-Anschluss. In der 13,3″-Variante hat das Gerät zusätzlich einen SD-Kartenleser. Außerdem funkt das Gerät über WLAN (802.11ac) und Bluetooth . Ein USB- oder Thunderbolt-Ethernetadapter ist separat erhältlich.\n\nEin optisches Laufwerk ist werksseitig nicht vorhanden. Dafür ist das MacBook Air in der Lage, über WLAN auf die CD/DVD-Laufwerke eines anderen Macs oder auch Windows-Rechners zuzugreifen. Das Betriebssystem kann über eine recovery Partition wiederhergestellt werden. Alternativ kann ein externes Laufwerk „Super Drive“ erworben werden, das über den USB-Anschluss mit Strom versorgt wird. Dieses Laufwerk kann mit einem einzelnen USB-Anschluss am MacBook Air betrieben werden, da der USB-Anschluss des MacBook Air für eine höhere Stromabgabe, als es die USB-Spezifikation vorsieht, ausgelegt ist. Da das Laufwerk über keinen zusätzlichen Stromanschluss verfügt, ist für einen sicheren Betrieb an anderen Rechnersystemen der Einsatz eines speziellen Y-USB-Kabel notwendig, der einen zweiten USB-Port des Rechners zur Absicherung der Stromversorgung nutzt. Eine Gehäuseöffnung zum Anschluss eines Kensington-Schlosses, wie bei vielen Notebooks üblich, fehlt.\n\nDas MacBook Air hat in seiner aktuellen Variante Stereo-Lautsprecher verbaut.\n\nDie erste Revision des Gerätes bot in der Standardausführung einen 1,60-GHz-Prozessor, 2 GB auf der Platine fest integrierten Arbeitsspeicher und eine 80-GB-Festplatte, die mit 1,8″ dasselbe Format hat wie Festplatten der iPod-Modelle, jedoch mit 4.200/min schneller dreht als die 3.400/min drehenden Festplatten in den iPods. Auf Wunsch gab es eine Version mit 1,80-GHz-Prozessor und einem 64 GB großen Flashspeicher (Solid-State-Drive) statt einer Festplatte. Der Massenspeicher war über die mittlerweile veraltete Parallel-ATA-Schnittstelle angebunden.\n\nDer Prozessor war eine Besonderheit, da er im sogenannten \"Small Form Factor Package\" von Intel gefertigt wurde und mit einer Fläche von 22 mm × 22 mm ca. 60 Prozent kleiner war als ein normaler Prozessor.\nTechnisch gesehen basierten beide Prozessormodelle auf dem Intel Core 2 Duo mit Merom-Kern, weshalb sie dessen Funktionen inklusive 4 MB Shared-L2-Cache hatten.\n\nIm Oktober 2008 wurde eine verbesserte Neuauflage des MacBook Air „MacBook Air (Ende 2008)“ vorgestellt.\n\nDer Chipsatz (Hauptplatine) wurde nun nicht mehr von Intel, sondern Nvidia geliefert.\nDadurch wurde auch der langsame Intel-Chipsatz-Grafikprozessor durch den bis zu vierfach schnelleren Nvidia-9400M-Chipsatz-Grafikprozessor mit 256 MB DDR3-SDRAM (shared memory) ersetzt, der eine Auflösung von bis zu 2560 × 1600 Pixeln auf einem externen Bildschirm unterstützt.\nDer Takt des weiterhin von Intel gelieferten Prozessors Core 2 Duo blieb mit 1,60 GHz beim Einstiegsmodell unverändert, darüber hinaus war ein mit 1,86 GHz getakteter Prozessor erhältlich.\nDie zuvor beschriebene Sonderbauform eines verkleinerten Intel-Merom-Prozessors wurde durch Standard-Prozessoren mit 6 MB Level-2-Cache aus der neuen „Penryn“-Serie von Intel ersetzt.\n\nDie Systembusgeschwindigkeit wurde von 800 auf 1066 MHz angehoben, der Arbeitsspeicher entsprach nun dem schnelleren DDR3-Standard.\nDer interne Datenträger wurde fortan über die weitverbreitete SATA-Schnittstelle angebunden.\nStandardmäßig wurde eine 120 GB fassende 1,8″-Festplatte mit 4.200/min eingebaut; die Solid-State-Disk-Variante hatte 128 GB statt 64 GB.\n\nDer speziell für das MacBook Air Revision A entwickelte Micro-DVI-Port wurde in Revision B durch den neuen Mini-DisplayPort ersetzt.\n\nErste Tests zeigten einen deutlichen Geschwindigkeitszuwachs der Revision B, trotz der nominell fast unveränderten Prozessorgeschwindigkeiten. Auch gehören die häufig berichteten Probleme der Vergangenheit an, bei denen sich das MacBook Air (Revision A) bei prozessorintensiven Aufgaben überhitzte und einen Prozessorkern abschaltete.\n\nDie Stromversorgung erfolgte durch einen fest eingebauten Lithium-Polymer-Akku, der das Gerät nach Herstellerangaben bis zu 4,5 Stunden betreiben kann. In verschiedenen Tests mit unterschiedlicher Beanspruchung erreichte das MBA Laufzeiten zwischen 2,5 und knapp 5 Stunden.\nFerner verfügte es über WLAN nach dem 802.11n-Standard und Bluetooth 2.1 + EDR.\n\nAm 8. Juni 2009 kündigte Apple mit dem Motto „Dünn wie immer. Schnell wie nie.“ auf der WWDC die Revision C des MacBook Air an. Diese Version war nun wahlweise mit einem 1,86 GHz oder 2,13 GHz Intel-Prozessor lieferbar und somit leistungsstärker als bisher. Zugleich sanken beide MacBook-Air-Modelle im Preis.\n\nAm 20. Oktober 2010 wurde unter dem Motto „Back to the Mac“ eine neue Generation des Macbook Air veröffentlicht. Zusätzlich zum bisherigen Displayformat von 13,3″ (bei höherer Displayauflösung) wurde die Größe 11,6″ eingeführt und zur Datenspeicherung ausschließlich SSD verwendet. Das Volumen wurde geringfügig reduziert. Hinzu kommt ein höher auflösendes Display sowie zwei statt bisher einem USB-2.0-Anschluss. Erstmals konnten im MacBook Air 4 GB RAM statt der bisher maximalen 2 GB verbaut werden. In der 13,3 Zoll-Variante ist zudem ab dieser Version ein SD-Karten-Slot vorhanden. Mit dem ersten Macbook Air hat Apple das Unibody-Design für die Fertigung seiner Notebooks eingeführt und diese Fertigungsart schrittweise auch auf andere Produktreihen ausgeweitet. Im Gegensatz zum normalen MacBook und der Pro-Reihe stellt Apple erstmals auch den Bildschirm im Unibody-Design her. Die noch bei der ersten Vorstellung 2008 integrierte Tastaturbeleuchtung war in den folgenden Modellen bis Mitte 2011 nicht mehr vorhanden.\n\nAm 20. Juli 2011 wurde unter dem Motto „Das neue, schnellere Macbook Air“ die Generation Mitte 2011 gemeinsam mit Mac OS X Lion veröffentlicht.\nWesentliche Änderungen waren schnellere Intel i5- und i7-ULV-Prozessoren (17W TDP), der Thunderbolt-Anschluss, 4 GB RAM bei den 13,3″-Modellen, Tastaturbeleuchtung, die Intel HD 3000 Grafik und die Möglichkeit, eine SSD von 256 GB Speicherplatz in das 11,6″-Modell einbauen zu lassen.\n\nÄhnlich wie ein Jahr zuvor wurde auch 2012 bei der WWDC u. a. ein neues MacBook Air und ein neues Betriebssystem vorgestellt. Äußerlich gibt es kaum Veränderungen zum Vorgängermodell, es wurden lediglich die Schnittstellen an den Seiten des Geräts angepasst und die beiden USB-Anschlüsse als USB 3.0 umgesetzt. Des Weiteren wurde erstmals auf den flacheren MagSafe 2 Stromanschluss gesetzt. Ansonsten sind die wesentlichen Unterschiede zum Vorgängermodell im Innenleben zu suchen. So kommen nun eine 720p-FaceTime-HD-Kamera, der neue Prozessor von Intel \"Ivy Bridge\" und die verbesserte Grafikkarte \"Intel HD Graphics 4000\" zum Einsatz. Auch bietet Apple seinen Kunden optional einen verbesserten Arbeitsspeicher sowie einen größeren Festplattenspeicher, beides allerdings nur gegen entsprechenden Aufpreis. Generell wurden die Preise leicht erhöht.\n\nAm 10. Juni 2013 wurde ein neues MacBook Air mit Intel Haswell-Prozessoren vorgestellt. Dadurch konnte die Batterielaufzeit auf 9 bzw. 12 Stunden erhöht werden. Das MacBook Air unterstützt nun den neuesten WLAN-Standard 802.11ac. Außerdem wurden anstatt eines zwei Mikrofone eingebaut, nebeneinanderliegend und wie bisher auf der linken Seite. In ersten Tests wurde die angegebene Batterielaufzeit nicht nur erreicht, sondern sogar übertroffen.\n\nSeit 29. April 2014 sind im MacBook Air unter Revision B neue Haswell-Prozessoren (Haswell-Refresh) verbaut und die Preisgestaltung hat sich in Deutschland und Österreich geändert. Das 13 Zoll-Modell sank im Preis von 1099€ auf 999€ und das 11″-Modell kostete statt 999€ nur noch 899€.\n\nAm 9. März 2015 wurden aktualisierte Versionen für beide Gerätegrößen (11,6 Zoll und 13,3 Zoll) mit Intel Core i5- und i7-Prozessoren der 5. Generation (Broadwell) vorgestellt. Die Thunderbolt 1-Schnittstelle durch eine Thunderbolt 2-Schnittstelle ersetzt und die Geschwindigkeit der SSD verdoppelt. Die Preise stiegen jedoch aufgrund des Wechselkurses in Deutschland und Österreich wieder um 100 € auf 999 € für das 11″-Modell beziehungsweise auf 1099 € für das 13″-Modell.\n\nAm 27. Oktober 2016 wurde das MacBook Air 11″ eingestellt. Das schon im vorigen Jahr eingeführte MacBook kann aufgrund seiner Größe und seines Aussehens als Nachfolger gezählt werden.\n\nAm 5. Juni 2017 bekam das MacBook Air zur WWDC eine kleine Modellpflege, bei der ein etwas schnellerer Prozessor verbaut wurde, welcher jedoch immer noch Teil der mittlerweile 2 Jahre alten Broadwell-Architektur war.\n\nAls Betriebssystem dient für alle Geräte Mac OS X. Die erste Generation aus Anfang 2008 (MacBook Air 1,1) kann maximal Mac OS X 10.7 Lion ausführen. Generation 2 (MacBook Air 2,1) ab Ende 2008 und alle Generation danach einschließend, kann jedes MacBook Air Mac OS X El Capitan ausführen. Lediglich für Mac OS Sierra und höher ist mindestens ein MacBook Air aus Ende 2010 (MacBook Air 3,1) erforderlich.\n\nOS X bzw. Mac OS lässt sich völlig ohne externe Datenträger über die zeitgleich mit \"Lion\" eingeführte Funktion „Internet-Wiederherstellung“ neu installieren, selbst wenn die SSD komplett formatiert sein sollte.\nBei den vorherigen Geräten wurde das System auf einem Micro-USB-Stick ausgeliefert, um das Betriebssystem ohne DVD-Laufwerk neu aufsetzen zu können.\nNeu ist die Software Remote Disc. Sie ermöglicht es, drahtlos auf optische Laufwerke anderer Macs oder PCs zuzugreifen und von ihnen Programme zu installieren.\nÜber Boot Camp kann Windows auf dem Macbook Air installiert werden.\nDie dazugehörigen Treiber werden davor vom Programm \"Boot Camp\" auf einen USB-Stick oder auf eine CD/DVD gespeichert, damit die Treiber unter Windows installiert werden können.\n\nNach der Kritik an der fehlenden Umweltverträglichkeit hat Apple in das MacBook Air ein quecksilber- und arsenfreies LED-hintergrundbeleuchtetes Display verbaut. Dieses war zuvor bereits im MacBook Pro erhältlich. Weiterhin finden PVC-freie Kabel Verwendung.\n\nDas MacBook Air erfüllt neben „Energy Star“ auch die Anforderungen für „EPEAT Gold“.\n\nLegende: — \n\nAm 30. Oktober 2018 stellte Apple in New York eine komplett überarbeitete Version des MacBook Air vor. Nachdem das vorige Modell 3 Jahre lang mit Technik verkauft wurde, welche noch auf dem Stand von 2015 war, verwendet die neue Generation Zweikernprozessoren der Amber-Lake-Generation von Intel. Dessen durchschnittlicher Energiebedarf ist mit 7 Watt angegeben, was 2 Watt mehr als beim MacBook sind. Im Gegensatz zu diesem verwendet das MacBook Air Retina einen Lüfter. Die größte Überarbeitung erfuhr das Display, welches nun ein IPS-LCD anstelle eines TN-Panels ist. Es besitzt die gleiche Auflösung wie das Display des MacBook Pro mit 13″. Durch die Abdeckung des gesamten sRBG-Farbraums kann es 60 % mehr Farben als das alte Modell zeigen. Die Maximalhelligkeit wurde auf 300 cd/m² erhöht. Somit besitzt es, abgesehen von der Auflösung, die gleichen Eigenschaften wie das Display des MacBook mit 12″. Das Trackpad ist 20 % größer als das des Vorgängers und unterstützt Force Touch mit einer Taptic Engine, wie es schon beim MacBook und MacBook Pro seit 2015 der Fall ist.\n\nWie beim Vorgänger ist das Gehäuse aus Aluminium gefertigt und wird nach vorne hin dünner. An der dicksten Stelle ist es rund 10 % dünner als der Vorgänger. Das Volumen des Gehäuses wurde um 17 % reduziert.\n\nWie schon der Vorgänger verwendet das Retina MacBook Air ein quecksilber- und arsenfreies LED-hintergrundbeleuchtetes Display, PVC-freie Kabel und erfüllt neben „Energy Star“ auch die Anforderungen für „EPEAT Gold“.\n\nZusätzlich ist das Gehäuse, wie beim Mac mini von 2018, zu 100 % aus recyceltem Aluminium gefertigt, welches als Nebenerzeugnis bei der Produktion des iPad Pro der 3. Generation anfällt. In der Qualität und Beschaffenheit soll es sich laut Apple jedoch nicht vom Vorgänger unterscheiden. Im Vergleich zu diesem sollen bei der Produktion 47 % weniger CO₂-Emissionen anfallen.\n\nLegende: — \n\n\"Daten, die kursiv und grau geschrieben sind, können nur auf der Internetseite optional konfiguriert werden. Geräte mit solchen Sonderausstattungen werden oftmals als built-to-order (BTO) bezeichnet, weil sie pro Bestellung extra angefertigt werden, wodurch die Lieferzeit in der Regel länger ist.\"\n\n"}
{"id": "3224923", "url": "https://de.wikipedia.org/wiki?curid=3224923", "title": "Linux4afrika", "text": "Linux4afrika\n\nlinux4afrika ist ein Projekt, das in Deutschland veraltete, aber noch funktionsfähige Computer sammelt, diese umbaut und in ein Terminalserver-Netz einbaut, welches dann an Schulen in Afrika, insbesondere Tansania und Mosambik gespendet wird. Projektträger ist der Freiburger Verein FreiOSS. Als Software kommt edubuntu zum Einsatz, eine Linux-Distribution, die eigens für Schulen zusammengestellt wurde. Die Projekthelfer arbeiten ehrenamtlich und sind meist Informationstechnik–Fachleute.\n\nDie Spenden werden in Lagern gesammelt und für den Einsatz vorbereitet. Stehen genügend gespendete Rechner und Spendengelder für Server und Transport bereit, wird ein Container mit 200 PCs, Monitoren, Tastaturen usw. beladen und mittels Spedition nach Tansania verschifft. Mit dieser Fracht können 10 Schulen in Tansania mit Computern ausgestattet werden. Die Verschickung eines Containers kostet ca. 2500 Euro. In Tansania mangelt es an Internetanbindungen, so dass die Netzwerke vorerst offline arbeiten.\n\nDie ehemalige Kolonie Tansania wurde ausgewählt, weil es dort seit über 40 Jahren keinen Krieg und keine Unruhen mehr gab, die Alphabetisierung bei über 90 Prozent liegt, die Menschen jedoch in tiefer Armut leben. Das durchschnittliche Einkommen liegt umgerechnet bei einem Euro pro Tag.\n\nDas Projekt wurde im Jahr 2006 gegründet.\n\nMitte Juli 2007 konnte der erste Container mit Spenden-PCs gepackt werden, er traf Ende August in Tansania ein. Im Herbst 2007 zog das Projekt linux4afrika die Aufmerksamkeit der Bundesregierung auf sich und wurde in den Entwicklungshilfe–Ausschuss des Bundestages nach Berlin eingeladen. Seit Dezember 2007 gibt es eine Zweigstelle in Leipzig, durch die der Wirkungsraum des Projektes um Mitteldeutschland und Berlin erweitert werden konnte.\n\nAm 20. Februar 2008 wurde linux4afrika von der Jury des Nationalkomitees der UN–Dekade „Bildung für nachhaltige Entwicklung“ als offizielles Dekade–Projekt 2008/2009 ausgezeichnet. Am 26. Februar 2008 startete das Tansania–Projekt offiziell mit der Einweihung des ersten gespendeten Computerraums. Am 27. Februar 2008 traf linux4afrika Mizengo Pinda, den Premierminister von Tansania.\n\nAm 27. Juni 2009 gaben das Projekt linux4afrika, die Linux–Distribution Skolelinux und das Terminalserver–System X2Go ihre partnerschaftliche Zusammenarbeit bekannt. Man erhofft sich, dass Schulen, die Skolelinux einsetzen, aufgrund eines ähnlichen Systems eine Partnerschaft mit einer linux4afrika–Schule eingehen.\n\nFür das Terminalserver-Netz auf LTSP–Basis wird der Server (ein handelsüblicher PC) neu gekauft. Für jeden Computerraum werden ca. 20 gespendete PCs (min. 64 MB RAM, AGP–Grafikkarte) als \"Thin Client\" umgerüstet, indem die Festplatten entfernt sowie die EEPROMs auf den Netzwerkkarten ausgetauscht werden. Die alten PCs booten anschließend vollständig vom Server, sämtliche Berechnungen und Disk–I/O finden auf dem Server statt. Auf diese Weise kann trotz alter Hardware auf den Schüler–PCs eine hohe Arbeitsgeschwindigkeit gewährleistet werden. Solche Terminal–Server–Netze sind auch in deutschen Schulen im Einsatz und genügen problemlos den Anforderungen.\n\n\n"}
{"id": "3229077", "url": "https://de.wikipedia.org/wiki?curid=3229077", "title": "SuperFetch", "text": "SuperFetch\n\nSuperFetch ist eine Speichermanagementtechnik bei den Betriebssystemen Microsoft Windows Vista, Windows Server 2008, Windows 7, 8 und 10 (Bei Windows Server 2008 R2 ist diese Technik nicht mehr vorhanden). Grundlage für SuperFetch ist der Speichervorgriff (engl. \"prefetching\").\n\nHäufig benötigte Inhalte langsamer Speichermedien werden automatisch im Hintergrund in schnelleren Speichermedien (vor allem dem Arbeitsspeicher (RAM), aber auch in Flash-Speichern – durch ReadyBoost und Turbo Memory) bereitgestellt, die dadurch nahezu vollständig ausgefüllt werden. Das ist kein Nachteil, da der Speicher direkt wieder vom Betriebssystem freigegeben wird, wenn ein Programm mehr Arbeitsspeicher benötigt.\n\nDie Entscheidung, welche Inhalte zwischengespeichert werden, basiert auf folgenden Methoden:\n\n\nIn der Windows-Registrierungsdatenbank können verschiedene Modi eingestellt werden – das geschieht über den Registryschlüssel \"HKEY_LOCAL_MACHINE\\SYSTEM\\CurrentControlSet\\Control\\Session Manager\\Memory Management\\PrefetchParameters\" in der Variable \"EnableSuperFetch\". Folgende Werte sind vorgesehen:\n\n\nPositiv an \"SuperFetch\" ist, dass Programme im Arbeitsspeicher schneller gestartet werden können. Das kann sich bemerkbar machen, wenn die Festplatte durch andere Aufgaben, wie z. B. einen Virenscan, ausgelastet ist.\n\nWeil Microsoft \"SuperFetch\" für Windows-Versionen vor Vista nicht anbietet, haben andere Anbieter wie beispielsweise eBoostr diese Lücke gefüllt.\n\nNegativ ist, dass der Systemstart verlangsamt werden kann. Es wird nicht geprüft, ob das Vorladen überhaupt sinnvoll ist. Bei SSDs benötigt man kein Superfetch. Es wird auch nicht unbedingt berücksichtigt, ob eine Datei wirklich häufig genutzt wird. Gelegentliche Nutzung genügt, um Superfetch aktiv werden zu lassen, was z. B. bei Images von virtuellen Maschinen relativ sinnfrei ist. Festplatten werden durch überflüssige Ladevorgänge unnötig gestresst, was die Haltbarkeit verkürzen kann.\n\n"}
{"id": "3229804", "url": "https://de.wikipedia.org/wiki?curid=3229804", "title": "Qucs", "text": "Qucs\n\nQucs (\"Quite Universal Circuit Simulator\") ist ein plattformunabhängiges Open-Source-Programm zur Schaltungssimulation.\nQucs unterstützt analoge und digitale Bauteile und kann mit SPICE-Bauteilen umgehen. Es versteht VHDL und Verilog.\n\nDie grafische Benutzeroberfläche basiert auf Qt 3 und 4.\n\n"}
{"id": "3235615", "url": "https://de.wikipedia.org/wiki?curid=3235615", "title": "WISE-FTP", "text": "WISE-FTP\n\nWISE-FTP ist ein grafisches Clientprogramm für Windows zum Zugriff auf einen FTP-Server. Es wird von der in Darmstadt beheimateten AceBIT GmbH entwickelt.\n\nWISE-FTP ist im deutschsprachigen Raum ein verbreiteter FTP-Client, der unter anderem von den beiden größten deutschen Internet-Providern 1&1 (seit 1999) und Deutsche Telekom (seit 2004) als Bestandteil der Webhosting-Pakete ausgeliefert wird.\n\nWISE-FTP kann mehrere gleichzeitige Server-Verbindungen verwalten. Außerdem können Verbindungen über SSL, TLS und SSH vor unautorisiertem Zugriff geschützt werden.\n\nDie Oberfläche von WISE-FTP ist in deutscher, englischer, spanischer, französischer und arabischer Sprache verfügbar und kann im Programm ohne Neuinstallation umgeschaltet werden.\n\n"}
{"id": "3246180", "url": "https://de.wikipedia.org/wiki?curid=3246180", "title": "Avahi (Software)", "text": "Avahi (Software)\n\nAvahi ist eine freie Implementierung von Zeroconf, einer Technik zur Vernetzung von Geräten in einem lokalen Netzwerk, ohne dass diese manuell konfiguriert werden müssen.\n\nAvahi wurde von Lennart Poettering und \"Trent Lloyd\" entwickelt. Es ist das Ergebnis der 2005 erfolgten Zusammenführung der ursprünglichen mDNS-/DNS-SD-Implementierung von Poettering namens \"FlexMDNS\" und von Lloyds Original-Code namens \"Avahi\". Während der Großteil des heutigen Codes aus dem Projekt FlexMDNS stammte, wurde \"Avahi\" der Name für das gemeinsame Projekt. Die ursprüngliche Entwicklung von FlexMDNS hatte Ende 2004 begonnen, die Arbeit am ursprünglichen Avahi Anfang 2004.\n\nAvahi wurde ursprünglich unter der Schirmherrschaft von freedesktop.org entwickelt, hat sich aber mittlerweile zu einem eigenständigen Projekt entwickelt.\n\nDer Name \"Avahi\" bezieht sich auf die Wollmakis, eine Gattung von Primaten, die in Madagaskar beheimatet sind. Trent Lloyd legte den Namen fest. Der Wollmaki ist auch die Grundlage für das Logo.\n\n\n"}
{"id": "3252258", "url": "https://de.wikipedia.org/wiki?curid=3252258", "title": "Wahltaste", "text": "Wahltaste\n\nDie Wahltaste (engl. \"\") ist eine Sondertaste auf den Tastaturen für Apple-Computer. Für gewöhnlich befinden sich dort zwei Wahltasten (jeweils neben den Befehlstasten); auf den Tastaturen älterer tragbarer Computer gab es wegen Platzmangels nur eine, neben der linken Befehlstaste. Beide Wahltasten haben die gleiche Funktion.\n\nDie Aufschrift auf der Wahltaste ist in Deutschland die Bezeichnung \"alt\" über dem Unicode-Symbol ⌥ für einen Mikroschalter. Dieses Zeichen wird auch in Menüs zur Angabe von Tastaturbefehlen genutzt. Auf englischsprachigen Tastaturen lautet die Bezeichnung \"option\". Auf den Tastaturen anderer Hersteller finden sich in der Regel beide Bezeichnungen. In den Jahren 1980 bis 1984 trug die Wahltaste einen ausgefüllten Apfel, der sie von der Befehlstaste unterscheiden sollte.\n\nAufgrund des aufgedruckten Symbols (⌥) wird die Taste umgangssprachlich auch als \"Weichen-\" oder \"Badewannentaste\" bezeichnet.\n\nAuf der neuesten Version der Apple-Geräte (z. B. Magic Keyboard der 3. Generation) wurde das Symbol (⌥) entfernt und durch die Beschriftung „option“ ersetzt. Die Taste hat nun oben die Beschriftung „alt“ und unten die Beschriftung „option“.\n\nEbenso kann die Taste nur durch das Symbol (⌥) oben rechts und die Beschriftung „option“ – ohne der Aufschrift „alt“ – gekennzeichnet sein.\n\nDie Taste ähnelt auf den ersten Blick sehr der Alt-Taste an Windows- und Unix-Computern; auf USB-Tastaturen senden beide den gleichen Scancode. Unter Mac OS X erfüllt sie jedoch andere Funktionen als die Alt-Taste unter Windows und Unix:\n\nÄhnlich wie bei der Alt-Gr-Taste wird die Wahltaste in Verbindung mit den Buchstaben- und Zahlentasten benutzt, um Sonderzeichen und diakritische Zeichen zu erzeugen. So erzeugt beispielsweise die Kombination + das @-Zeichen oder + das ç.\n\nDie Wahltaste kann in verschiedenen Programmen die Funktion oder das Erscheinungsbild von Menüs und Buttons verändern. Beispiele dafür sind:\nFür gewöhnlich gibt der Anwender die bevorzugte Funktion in den Programmeinstellungen an, die Wahltaste kehrt diese Einstellung um.\n\nIn Textfeldern lässt sich die Wahltaste zur schnellen Navigation mit der Eingabemarke nutzen:\nBis zur Version 10.3.x des Betriebssystems Mac OS konnte die Optionstaste auch mit den Scrollbars genutzt werden. Ein Klick auf einen Pfeil mit gedrückter Wahltaste bewirkt den Sprung um eine Seite anstatt wie gewöhnlich nur einige Zeilen. Ist die Wahltaste gedrückt, wenn an eine Stelle in der Scrollbar geklickt wird, springt das Dokument zu der gewählten Stelle anstatt nur bis zur nächsten Seite. In neueren Versionen kann man dies nur über die Systemeinstellung \"Erscheinungsbild\" konfigurieren.\n\n\nWird das System mit gedrückter Wahltaste gestartet, erscheint ein Dialog, mit dem der Benutzer eine Partition oder ein Laufwerk auswählen kann, von dem gebootet werden soll. Auf neueren Macs mit Intel-Prozessoren lassen sich so auch von Windows-CDs und -Partitionen booten, wenn die Software Boot Camp installiert ist.\n"}
{"id": "3256920", "url": "https://de.wikipedia.org/wiki?curid=3256920", "title": "IMule", "text": "IMule\n\nIMule (invisible Mule; deutsch \"unsichtbares Maultier\") ist ein für mehrere Plattformen verfügbarer P2P-Client, basierend auf aMule, der über das I2P-Netzwerk anonymes Filesharing ermöglicht. Bis zur Version 1.2.2 war hierfür eine separate Routersoftware nötig, um eine Verbindung mit dem I2P-Netzwerk herzustellen. Ab Version 1.2.3 ist iMule aber auch als standalone nutzbar, indem es den I2P-Router integriert hat. Will man die weiteren Dienste, die I2P bietet, jedoch nutzen, so benötigt man einen separaten I2P-Router – eine Software, die sich um die Anonymisierung der Kommunikation verschiedenster Programme kümmert und zudem leicht zu installieren ist.\n\nIm Gegensatz zu aMule, eMule oder ähnlichen eDonkey2000-Clients verbindet sich iMule nicht auch über Server, sondern läuft gänzlich dezentral über das so genannte Kademlia-Protokoll, welches hier aber durch I2P völlig anonymisiert wird.\n\nDem Vorteil des anonymen Datenaustausches über das Kademlia-Protokoll steht der Nachteil der geringeren Geschwindigkeit gegenüber – die Anonymisierung wird durch ein verschlüsseltes Mix-Netzwerk angeboten und dieses verlangsamt die Transfers.\n\n\nFür folgenden Link muss I2P installiert sein und laufen, Zugriff über I2P:\n"}
{"id": "3265407", "url": "https://de.wikipedia.org/wiki?curid=3265407", "title": "Yakuake", "text": "Yakuake\n\nYakuake (Yet Another Kuake) ist ein KDE-Terminalemulator. Als Inspiration für das Design dienten Konsolen in Computerspielen wie Quake, die beim Druck einer Taste vom oberen Bildschirmrand nach unten scrollten und beim wiederholten Tastendruck durch Scrollen nach oben wieder verschwanden.\n\nWird die Yakuake-Konsole ausgeblendet, so läuft das Programm im Hintergrund weiter. Ein erneutes Einblenden kann aus diesem Grund deutlich schneller als ein Start eines neuen Terminals erfolgen. Weiter kann eine Yakuake-Sitzung auf allen Desktops per Tastendruck ein- und ausgeblendet und verwaltet werden. Damit eignet sich die Konsole sowohl für eine schnelle Ausführung kurzer Befehle zur interaktiven Nutzung als auch für den Start lang laufender Kommandozeilen-Aufgaben, deren Ausgabe eher selten eines Blickes bedarf. Dieser Komfort gegenüber herkömmlichen Fenster-gebundenen Terminalemulatoren wie Konsole (KDE) oder xterm und das außergewöhnliche Design brachten Yakuake zu Beginn 2008 unter die 3 beliebtesten KDE-Programme auf \"kde-apps.org\".\n\n\n"}
{"id": "3269707", "url": "https://de.wikipedia.org/wiki?curid=3269707", "title": "Der Vogelschreck", "text": "Der Vogelschreck\n\nDer Vogelschreck (Originaltitel: For the Birds) ist ein computeranimierter Kurzfilm von Pixar aus dem Jahr 2000. Der Film wurde im Kino vor dem Hauptfilm Die Monster AG gezeigt. Der Film kommt völlig ohne gesprochene Worte aus und teilt alle Stimmungen durch die Stimmintonation der Vogelprotagonisten mit.\n\nBei der Oscarverleihung 2002 erhielt der Film die Auszeichnung für den besten animierten Kurzfilm. \n\nViele kleine spatzenähnliche Vögel sitzen auf einem Kabel zwischen zwei Telefonmasten. Als ein großer, langbeiniger Vogel sich zu ihnen gesellen will, veralbern sie ihn wegen seiner Stimme und Gestalt. Dieser jedoch zeigt sich davon unbeeindruckt und setzt sich mitten zwischen sie auf die Telefonleitung. Durch sein großes Gewicht hängt die Leitung jedoch durch, so dass alle Vögel zwangsläufig zu ihm hinrutschen. Da ihnen dieses missfällt, hacken sie auf die Füße des Großen ein, dieser kann sich nicht länger in der normalen Sitzposition halten, fällt und hängt daher schließlich mit seinen Füßen an der Leitung. Zeh für Zeh lässt er unter den Schnabelattacken los und fällt (durch die sehr durchhängende Leitung aus sehr geringer Höhe) mit dem Kopf voran zu Boden. Das gespannte Kabel schnalzt daher nach oben und katapultiert alle Vögel in die Luft. Durch die dadurch entstehende enorme Beschleunigung verlieren sie aufgrund der Trägheit ihre Federn und fallen \"nackt\" wieder zu Boden. Nun lacht der große Vogel die kleinen aus, die aus Scham über ihre Nacktheit ihre Blöße verstecken.\n"}
{"id": "3270569", "url": "https://de.wikipedia.org/wiki?curid=3270569", "title": "SFT-Loader", "text": "SFT-Loader\n\nDer SFT-Loader ist ein Download-Manager für SFT-Dateien, welche verschlüsselte FTP oder HTTP-Links und Anmeldedaten (\"login data\") enthalten, mit denen der Ladevorgang vereinfacht werden soll. Zum Öffnen wird meistens ein Passwort benötigt.\n\nAm 27. Juni 2008 wurde bekanntgegeben, dass der SFT-Loader verkauft wird. Bei einer Auktion wurde der Mindestpreis nicht erreicht. Das Projekt SFT wird verbessert fortgeführt.\n\nSeit dem 16. Mai 2012 ist es möglich SFT-Dateien bis zur Version 2009 zu entschlüsseln, wodurch man die beinhalteten Links im Klartext auslesen kann.\n\nIm November 2012 wurde eine kritische Sicherheitslücke öffentlich, welche es ermöglichte, dem Benutzer Schadcode unterzuschieben.\n\n"}
{"id": "3295529", "url": "https://de.wikipedia.org/wiki?curid=3295529", "title": "P8000", "text": "P8000\n\nDas P8000 ist ein Mikrocomputersystem und wurde 1987 in der Deutschen Demokratischen Republik (DDR) vom VEB Elektro-Apparate-Werke Berlin-Treptow „Friedrich Ebert“ entwickelt. Es bestand aus einem 8-Bit- und einem 16-Bit-Mikrorechnerteil mit dazugehörigem Winchester-Disk-Controller und war als universell einsetzbares Programmier- und Entwicklungssystem für Multi-User-/Multi-Task-Anwendungen gedacht.\n\nNeben der Variante mit 16-Bit-Mikrorechner existierte auch eine abgerüstete Variante nur mit einem 8-Bit-Mikrorechner.\n\nDer 8-Bit-Mikrorechner des P8000 befand sich physisch mit all seinen Funktionsbaugruppen auf einer einzelnen 4-Lagen-Leiterplatte im Format 380 mm×250 mm. Der Rechner mit einer Taktfrequenz von 4 MHz basierte auf dem Mikroprozessor U880 und seinen Peripherieschaltkreisen zuzüglich dem U8272 Floppy-Disk-Controller. Direkter Speicherzugriff war mit Hilfe des Peripherieschaltkreises U858-DMA gewährleistet.\n\nAls Hauptspeicher standen neben dem 64 KB dynamisch arbeitendem RAM noch 8 KB EPROM, sowie 2 KB statischer RAM zum Systemanlauf, Systemtestroutinen sowie dem Systemmonitor zur Verfügung. Diese Zusatzspeicher konnten in 4-KB-Stufen im 64-KB-Adressraum verschoben oder abgeschaltet werden.\n\nDer 8-Bit-Rechner besaß vier serielle Kanäle welche mit tty0 bis tty3 bezeichnet wurden. Die Schnittstellen arbeiteten sowohl mit V.24- wie auch mit IFSS-Signalen. Daneben verfügte der Rechner über eine parallele Schnittstellen welche den Anschluss eines EPROM-Brenners erlaubte. Eine weitere interne 32-Bit-Parallelschnittstelle diente zur Kopplung der 8-Bit- mit der 16-Bit-Mikrorechnerkarte. Der Datenaustausch fand über zwei im Gerät eingebaute 5,25″-Diskettenlaufwerke statt. Extern konnten noch zwei weitere 5,25″- oder aber auch 8″-Diskettenlaufwerke angeschlossen werden.\n\nDer 16-Bit-Mikrorechner des P8000 gliederte sich in die zwei Funktionsbaugruppen 16-Bit-Rechner-Karte und den bis zu vier steckbaren Hauptspeicher-Karten mit Größen von je 256 KB oder 1 MB. Der 16-Bit-Rechner war auf einer 6-Lagen-Leiterplatte im Format 380 mm×250 mm untergebracht. Der Rechner arbeitete mit einer Taktfrequenz von 4 MHz und basierte auf dem 16-Bit-Mikroprozessor U8001. Drei Speicherverwaltungsbausteine U8010 übernahmen in Verbindung mit einer speziellen Steuerlogik die dynamische Speichersegmentzuweisung im Arbeitsspeicher und den Schutz vor unbefugten Zugriffen. Für den Systemmonitor und Eigentestroutinen besaß der Rechner des Weiteren 16 KB EPROM-Speicher sowie 2 KB statischen RAM. Die Peripherie wurde durch Schaltkreise der U880-Familie gebildet. Eine entsprechende Steuerlogik gewährleistete das Zusammenspiel mit dem U8001 Hauptprozessor.\n\nDer 16-Bit-Rechner besaß vier serielle Kanäle welche mit tty4 bis tty7 bezeichnet waren. Die Schnittstellen arbeiteten ebenfalls sowohl mit V.24- wie auch mit IFSS-Signalen. Mittels einer internen 32-Bit-Parallelschnittstelle war der 16-Bit-Rechner mit dem 8-Bit-Rechner verbunden. Über eine weitere parallele Schnittstelle war die Verbindung mit dem externen Winchester-Disk-Beisteller realisiert.\n\nDer Winchester-Disk-Controller (WDC) ist ein intelligenter Festplattencontroller und befand sich in einem extra Gerät dem \"Winchester-Disk-Beisteller\". In diesem Gerät waren neben dem eigentlichen Winchester-Disk-Controller auch bis zu zwei Festplatten untergebracht.\n\nDer WDC war auf Basis des Mikroprozessors U880 realisiert. Als Programmspeicher für die Firmware standen 8 KB EPROM zur Verfügung. Zur Zwischenspeicherung der Daten auf dem Weg vom Hostrechner zur Festplatte dienten 6 KB statischen RAMs. Die Kommunikation des WDC mit dem Hostrechner erfolgte über ein acht Bit breites Parallelinterface in Verbindung mit zusätzlichen Steuerbits. Die Schnittstelle realisierte die blockweise Übertragung der Daten wie auch der Kommando- und Quittierungsinformationen.\n\nAls Festplattenlaufwerke kamen bis zu zwei (gleiche) Laufwerke mit ST506-Schnittstelle zum Einsatz. Waren die Plattenparameter in den ersten Firmwareversionen noch fest in der Firmware enthalten, war es ab der Firmwareversion 4.2 möglich, jede beliebige MFM Festplatte einzusetzen. Neben den Informationen über defekte Sektoren, waren ab Version 4.2 auch die Plattenparameter in dem 1. Sektor der Festplatte gespeichert.\n\nDas P8000 Terminal diente als Ein- und Ausgabegerät des P8000. Es setzte sich aus einem Grünmonitor, einer Tastatur sowie einem Steuerrechner zusammen. Das Terminal unterstützte die Betriebsmodi \"ADM31\" und \"VT100\". Das Terminal bot die Möglichkeit zwischen zwei Zeichensätzen, welche in separaten EPROMs abgelegt waren, umzuschalten. Die Schnittstelle bot die Betriebsarten V.24 oder IFSS. Der Steuerrechner basiert auf einem U884 und dem Grafikcontroller KR580WG75 (Intel 8275). Akustische Signale wurden mit einem Piezophon ausgegeben.\n\nDer 1989 entwickelte P8000 Compact war eine weiterentwickelte Version des P8000. Beim P8000 Compact befand sich der Winchester-Disk-Controller zusammen mit der Festplatte im Gehäuse des Hauptrechners. Dadurch entfiel der extra Beisteller. Das P8000 Compact wurde im Gegensatz zum P8000 als 16-Bit-Version standardmäßig mit einer batteriegepufferten Echtzeituhr ausgeliefert. Auf Wunsch wurde das P8000 Compact auch mit einer dritten CPU, der U80601, geliefert. Der P8000 Compact war der letzte vom EAW entwickelte Computer.\n\n\n\nDer Preis für ein \"P8000\"-System lag zur Markteinführung bei 172125,- Mark. Von Freunden historischer Rechentechnik werden für Geräte und komplette Systeme heute Sammlerpreise bezahlt. Während für die 8-Bit-Variante 2012 gerade 101 Euro erzielt wurden, erzielte eine komplette Entwicklungsumgebung P8000 im selben Jahr 767 Euro und ein P8000 Compact im Jahr 2016 1066 Euro.\n\n\n"}
{"id": "3295763", "url": "https://de.wikipedia.org/wiki?curid=3295763", "title": "Linux Phone Standards Forum", "text": "Linux Phone Standards Forum\n\nDas Linux Phone Standards Forum (LiPS) ist ein im November 2005 gegründetes Konsortium zur Schaffung eines offenen Standards für eine einheitliche Linux-Implementierung für Mobiltelefone als Alternative zu Microsofts Windows CE Plattform. Auch andere proprietäre Herstellersysteme sollen abgelöst werden. Derzeit sind Linux-Plattformen meist nur auf der für jeweilige Version angepassten Hardware einzelner Hersteller lauffähig. Die Standardisierung betrifft daher sowohl Hardware als auch Softwareprodukte. Dabei sollen die Standards der Open Mobile Terminal Platform (OMTP) berücksichtigt werden.\n\nGründungsmitglieder sind unter anderem ARM, France Télécom, Orange, Montavista Software, Open-Plug und PalmSource. Im November 2006 sind ZTE, Telecom Italia und Texas Instruments beigetreten.\n\nIm Dezember 2007 wurde mit dem Linux Phone Standards 1.0 das erste LiPS Release vorgestellt.\n\nDie konkurrierende LiMo Foundation (unter anderem Motorola, NEC, NTT DoCoMo, Panasonic, Samsung und Vodafone) will einen Industriestandard schaffen, der nicht nur für Mobiltelefone, sondern für alle kompakten mobilen Endgeräte geeignet ist. Dieser ist jedoch zu weiten Teilen kein offener Standard. \nIm Juni 2008 schlossen LiPS und die LiMo-Foundation eine Kooperation. LiPS wurde komplett in die LiMo Foundation eingegliedert.\n\n"}
{"id": "3297195", "url": "https://de.wikipedia.org/wiki?curid=3297195", "title": "PulseAudio", "text": "PulseAudio\n\nPulseAudio (früher auch PolypAudio genannt, s. u.) ist eine netzwerktransparente, plattformunabhängige Sound-Middleware, deren API sich an Konzepte des davon abgelösten Enlightened Sound Daemon (ESD) anlehnt.\n\nDie Client-Bibliotheken sind auf jeder netzwerkfähigen Plattform nutzbar (z. B. auch eingebettete oder mobile Geräte), der PulseAudio Daemon als zentraler Soundserver und Hardware-Schnittstelle sowie die dazugehörigen Hilfsprogramme sind auf allen POSIX-kompatiblen Systemen und mit einer veralteten Version auf Windows verfügbar.\n\n\"PulseAudio\" ist freie Software, gemäß den Bestimmungen der GNU Lesser General Public License.\n\n\"PulseAudio\" basiert auf zwei grundlegenden Prinzipien:\n\nDie meisten Programme können direkt mit PulseAudio kommunizieren:\n\nWenige Programme können nicht mit PulseAudio kommunizieren:\n\nPulseAudio ist auch netzwerkfähig:\n\nOhne PulseAudio kann das Programm direkt mit dem Soundkarten-Treiber (\"hier:\" ein ALSA-Treiber) kommunizieren:\n\nAlternativ sollten Programme mit dem ALSA-Soundserver kommunizieren:\n\nDaraus ergeben sich sowohl Vor- als auch Nachteile:\n\nEin zentrales Anliegen von PulseAudio ist es, einerseits die Anwendungen weitestgehend von der tatsächlichen Soundhardware zu trennen (Abstrahierung), ihnen aber andererseits mehr Einfluss auf das Verhalten der Audioströme zu geben, ohne die Komplexität übermäßig zu vergrößern (durch Metadaten).\n\nErreicht wird dies durch die oben genannten Prinzipien, aufgrund welcher alle Prozesse gezwungen sind, ihre Sounddaten an PulseAudio zu übergeben. Dadurch entfällt die Verantwortung der einzelnen Programme für die Sounddaten und wird an zentraler Stelle, nämlich dem PulseAudio Daemon, gebündelt. Dessen Schnittstelle ermöglicht, Einfluss auf die Audiodaten zu nehmen, ohne dass die einzelnen Prozesse in irgendeiner Form darin involviert sind.\n\nErste und offensichtlichste Folge ist, dass ausschließlich Programme, die die PulseAudio-Client-Bibliotheken verwenden, in der Lage sind, Soundein- oder Ausgabeströme zu benutzen. Solange keine Legacy-Anwendungen zum Einsatz kommen, ist dies nicht relevant (fast alle aktuellen Audio- und Mediaplayer sowie die meisten portablen Audio-Bibliotheken (z. B. OpenAL oder SDL) unterstützen PulseAudio direkt).\n\nDamit sich PulseAudio jedoch auch bei Verwendung älterer Programme möglichst nahtlos ins System einfügt, wurden zusammen mit dem eigentlichen Soundserver eine Reihe spezialisierter Anwendungen entwickelt. Diese \"Adapter\" genannten Programme sind einerseits normale PulseAudio-Clients, andererseits bieten sie aber Prozessen auch den Zugriff über andere, auch normalerweise exklusive, Audio-Schnittstellen an, wobei die Daten dann transparent via PulseAudio weiterverarbeitet werden, ohne dass seitens der Legacy-Programme Änderungen nötig sind. Aufgrund der Vielzahl dieser Adapter entstand der ursprüngliche Name \"PolypAudio\".\n\nEin Beispiel ist die Verwendung unter Linux, wo als Hardware-Soundschnittstelle normalerweise ALSA zum Einsatz kommt: Während einige wenige Linux-Treiber für Soundkarten Mixing in Hardware durchaus unterstützen, und außerdem ALSA auch selbst standardmäßig einen simplen Software-Mischer in Form des DMix-Plugins mitbringt und so theoretisch der PulseAudio Daemon parallel zu reinen ALSA-Anwendungen betrieben werden kann, wird für gewöhnlich ein anderer Weg beschritten: Statt des Dmix-Plugins wird der ALSA-PulseAudio-Adapter geladen, der die PulseAudio-Kanäle als ALSA-Soundgeräte den Anwendungen zur Verfügung stellt. Die physischen ALSA-Geräte werden vom PulseAudio Daemon im exklusiven Zugriff gesperrt und der PulseAudio-Adapter als Standard-Audio-Gerät für ALSA definiert. Damit nutzen alle Programme, die ALSA verwenden, automatisch PulseAudio. Ob der Daemon selbst die ALSA-Hardware zur Soundausgabe nutzt oder nicht, spielt keine Rolle.\n\nDamit ist es möglich, selbst auf einem System, das über keinerlei physische Soundhardware verfügt und die Audioausgabe z. B. über einen per WLAN verbundenen Audioverstärker erledigt, normale ALSA-Software ohne Änderungen zu verwenden.\n\nEinschränkungen gibt es, wenn Programme bestimmte Hardwareeigenschaften oder -verhalten erwarten, die vom Adapter nicht emuliert werden können (z. B. festes RAM-Locking oder eine bestimmte Geräte-Nummerierung), sowie bei gemischten 32/64bit Systemen, wenn nicht alle Bibliotheken in beiden Versionen vorliegen.\n\nDie Schnittstelle zum älteren Open Sound System (OSS) kann durch ALSA emuliert werden (aoss), PulseAudio stellt jedoch auch einen eigenen Adapter (padsp) bereit, der die OSS-Gerätedateien (z. B. /dev/dsp) selbst erstellt und verwaltet.\n\nProgramme, die statt PulseAudio noch den Enlightened Sound Daemon (ESD) erwarten, werden direkt unterstützt, da PulseAudio als vollständiger Ersatz für ESD fungiert und dessen Schnittstellen mit übernommen hat.\n\nDie einzelnen Audioströme sind nicht an eine bestimmte Hardware gebunden und können, ohne dass die damit verbundenen Prozesse davon beeinflusst werden, im laufenden Betrieb auf andere Geräte umgeleitet werden. Dies kann sowohl manuell über die von den PulseAudio-Hilfsprogrammen bereitgestellte grafische Oberfläche erfolgen, als auch automatisch. Dafür stellt PulseAudio eine skriptfähige Schnittstelle zur Verfügung, die z. B. beim Anschließen oder Entfernen eines Soundgerätes verwendet wird. Durch benutzerdefinierbare Präferenzen kann so bestimmte Soundhardware (wenn sie verfügbar ist) gegenüber anderer bevorzugt werden.\n\nEin Beispiel ist die Verwendung eines Notebooks, das beim Verbinden mit der Dock-Station die Soundausgabe automatisch von den integrierten Lautsprechern auf den WLAN-Verstärker und die Soundeingabe auf das feste Mikrofon umschaltet, ohne dass es dabei zu einer Unterbrechung der Audioströme kommt. Beim Entfernen vom Dock tritt der umgekehrte Effekt ein.\n\nPräferenzen können mehrstufig sein, z. B. kann ein Bluetooth- oder USB-Headset wiederum eine noch höhere Priorität haben und so zeitweise sowohl die eingebaute als auch die Soundhardware des Docks verdrängen, unabhängig davon ob es zuhause oder unterwegs angeschlossen wird.\n\nNeben den Audiodaten können PulseAudio-Clients auch zusätzliche Metadaten an den Soundserver schicken, die bei der Auswahl des Zielgerätes berücksichtigt werden. So können z. B. die Sounds von Systemmeldungen immer über die eingebauten Lautsprecher, Musik und die Audiospur von Videos über das jeweils bevorzugte Gerät, VoIP-Telefonate aber nur über das Headset geleitet werden.\n\nNeben dem Wechsel von Geräten können auch virtuelle Geräte, z. B. als Zusammenfassung mehrerer physischer oder logischer Soundgeräte definiert werden, die von den Clients normal benutzt werden können. Für ein Screencast lässt sich so z. B. die komplette Ausgabe der PulseAudio-Soundpipeline plus der Eingabe des Mikrofons als neues Eingabegerät (entweder gemischt oder als zusätzliche Spur) schaffen, von dem problemlos einzeln aufgenommen werden kann, ohne späteres Mischen oder Nachbearbeiten zu erfordern. Damit verbunden, lassen sich mehrere Kanäle synchronisieren, ohne dass die Clients selbst die notwendige Wartelogik implementieren müssen.\n\nEin grundsätzliches Problem bei der Audioausgabe unter Linux ist, dass es keine klare Schichtung gibt, sondern verschiedene Systemedienste, bzw. Subsysteme, die den Zugriff auf die Audiohardware, Anpassungen der Abtastrate, Mischen gleichzeitiger Audioströme, Sitzungsmanagement, Zugriffskontrolle, sowie fortgeschrittene Signalverarbeitung in überlappender Weise implementieren. PulseAudio verfolgt dabei den Ansatz, einen vergleichsweise großen Umfang von Diensten in einem Teilsystem zu vereinigen.\n\nDer PulseAudio-Server ist nicht als systemweiter Server konzipiert, der unabhängig von einer Benutzersitzung läuft, vielmehr ist die Hardware ähnlich wie Maus und Bildschirm beim X-Window Display der Benutzersitzung zugeordnet. Für die meisten Desktop-Applikationen ist dies wünschenswert, da ein Zugriff auf die Audio-Eingänge es prinzipiell auch ermöglicht, ein System über das Internet abzuhören, was ein erhebliches Sicherheitsrisiko darstellen kann.\nEine Konfiguration im sogenannten „system mode“ ist prinzipiell möglich, jedoch wird hiervon – sowohl aus Gründen der Sicherheit als auch aufgrund gravierender technischer Nachteile – ausdrücklich abgeraten. Dies steht im Konflikt zu Konzepten, bei denen ein Medienserver wie etwa Music Player Daemon normalerweise als Systemdienst für einen direkten Zugriff auf die Audio-Hardware konzipiert ist, ohne dass notwendigerweise die vollständigen Audio-Daten übertragen werden. Es ist jedoch möglich, auf PulseAudio-Dienste systemweit über die Netzwerk-Schnittstelle zuzugreifen, wobei sich allerdings für die Zugriffsregelung noch kein durchgängiges und einheitliches Konzept etabliert hat, wie es im Bereich der Texteingabe beispielsweise mit den Pseudoterminals besteht.\n\nDurch die Abstraktion können PulseAudio-Clients entfernte und lokale Soundhardware gleichermaßen benutzen, ohne dass dies zusätzlichen Programmieraufwand erfordert. Möglich ist sowohl, dass der lokale PulseAudio Daemon die Daten einem anderen, per Netzwerk erreichbaren Daemon überlässt, als auch, dass der Client direkt einen anderen PulseAudio Server im Netz kontaktiert. Da ein PulseAudio Daemon eine Möglichkeit ist, per Netzwerk auf physische Soundhardware zuzugreifen, muss auf Systemen ohne Soundhardware kein Soundserver laufen. Somit kann in einem Netz mit zentralen Audiogeräten, z. B. einem Heimkino oder in einem Studio, von allen Systemen aus der gleiche, zentrale Soundserver benutzt werden (jedoch s. u. bzgl. Zugriff und Sicherheit).\n\nAlle Audiodaten passieren zwangsweise den PulseAudio Daemon, der damit auch ein geeigneter Ort für die Anwendung von Soundfiltern ist, zumal die meisten heutigen Prozessoren in der Lage sind, gleichartige Berechnungen parallel auf mehreren Datensätzen durchzuführen.\n\nWichtigster und auch in den grafischen Benutzeroberflächen essentiellster Punkt ist die Möglichkeit, die Lautstärke jedes Audiokanals und jedes Audiostroms jeder Anwendung einzeln zu konfigurieren (oder stummzuschalten), auch wenn das entsprechende Programm dafür keine eigene Möglichkeit bietet. Diese Einstellungen können gespeichert werden und bleiben dann für die jeweilige Anwendung bestehen. Daneben können auch Equalizer-Funktionen genutzt werden.\n\nNicht jede Soundhardware kann die gleichen oder überhaupt unterschiedliche Samplingfrequenzen verarbeiten. Manche Anwendungen erzeugen Audioströme mit festen Abtastraten und erwarten auch solche als Eingabe. Der PulseAudio Daemon nimmt die notwendige Konvertierung automatisch vor und stellt dafür unterschiedlich CPU-intensive Algorithmen zur Verfügung. Damit verbunden ist ein vielfach auftretendes Problem, dass ein qualitativ besserer aber auch rechenintensiverer Algorithmus voreingestellt ist (resample-method in /etc/pulse/daemon.conf).\n\nAn physischer Soundhardware unterstützt PulseAudio alles, was das jeweils native Soundsystem des Betriebssystems unterstützt. Unter GNU/Linux ist dies ALSA, OSS unter BSD und DirectSound unter Microsoft Windows. Jedes Soundgerät ist entweder Quelle (Source) oder Senke (Sink) für Audiodaten. Daneben kommen andere, über das Netzwerk verbundene PulseAudio Server sowie Geräte oder Prozesse, die das RTS-Protokoll unterstützen in Frage. Auch PulseAudio-Clients selbst können sowohl Senke als auch Quelle sein. Viele Adapter unterstützen jedoch oft nur die Funktion als Quelle für den adaptierten Prozess. PulseAudio kann auf Bluetooth-Audiogeräte zugreifen, auch wenn das native Soundsystem diese nicht unterstützt (solange Bluetooth allgemein unterstützt wird).\n\nDer PulseAudio Daemon bietet die Möglichkeit, während der Laufzeit durch ladbare, binäre Module erweitert zu werden. Die meisten Adapter und Filter sind auf diese Weise implementiert.\n\nDie Latenz der meisten Operationen ist sehr niedrig und kann von den Clients gemessen und beeinflusst werden. Eine hohe Latenz kann auf embedded und mobilen Geräten zu Energieeinsparungen führen, eine geringe Latenz ist z. B. für VoIP oder Multiplayer-Spiele erforderlich.\n\nInnerhalb des Daemons sowie der lokalen Clients kommt die PulseAudio-Soundarchitektur ohne das zeitaufwändige Umkopieren von Audiodaten aus (zero-copy architecture), dies gilt jedoch nur begrenzt bei der Verwendung von Adaptern.\n\nAufgrund der Abhängigkeit vom Zugang zum PulseAudio Daemon für alle Audiofunktionen ist dieser in der PulseAudio-Client-Bibliothek zentral und automatisch geregelt, welche neben dem reinen Auffinden eines Servers die Möglichkeit der bevorzugten Auswahl aus mehreren verfügbaren bietet. Sofern nicht abgeschaltet, ist ein Server mittels Zeroconf im Netz automatisch auffindbar. Lokal kann dies via D-Bus erfolgen. Adapter, insbesondere der ALSA-Adapter, und auch PulseAudio-Clients können jedoch auch selbst den PulseAudio Daemon starten, wenn dieser so konfiguriert ist und lokal noch nicht läuft. X11-Desktopumgebungen erledigen dies normalerweise automatisch.\n\nAuf unterster Ebene sind für den Zugang zum Server zwei Umgebungsvariablen notwendig: codice_1 und codice_2. Diese werden von der PulseAudio-Client-Bibliothek ausgewertet oder, wenn sie noch nicht existieren, gesetzt. Standardmäßig ist der Daemon X11-sessionbasiert konfiguriert, d. h. die Variablen sind nicht gesetzt, die Einstellungen werden jedoch beim Start des Daemons in die Ressourcen des Root X-Window eingetragen und von den Clients dort ausgelesen und können so z. B. auch über eine SSH-getunnelte Verbindung „mitgenommen“ werden. Ohne die X11-Sitzungsverwaltung können die Zugangsdaten via D-Bus erfragt werden.\n\nFür die Zugriffskontrolle auf den Server übernimmt PulseAudio die Methode von X11 und verwendet dafür ein pseudozufällig generiertes „Cookie“, das in codice_2 erwartet wird, und standardmäßig aus der Datei codice_4 des Benutzers stammt, unter dessen Konto der Daemon läuft. Normalerweise ist PulseAudio so konfiguriert, dass ohne dieses Cookie ein Zugriff auf den Server, auch lokal, nicht möglich ist, selbst wenn der Prozess dem gleichen Benutzer gehört wie der Daemon.\n\n"}
{"id": "3297604", "url": "https://de.wikipedia.org/wiki?curid=3297604", "title": "Ranger (Supercomputer)", "text": "Ranger (Supercomputer)\n\nDer Supercomputer Ranger steht im \"Texas Advanced Computing Center\" (TACC) der \"University of Texas at Austin\". Mit 320 TFlop/s LINPACK-Rechenleistung war er zum Zeitpunkt seiner Inbetriebnahme der schnellste Supercomputer im zivilen Bereich. Er konnte wegen Lieferschwierigkeiten des Prozessorherstellers AMD erst am 4. Februar 2008 seinen Probebetrieb aufnehmen und wurde am 22. Februar offiziell – wenige Stunden nach dem (danach zweit-) schnellsten JUGENE im \"Forschungszentrum Jülich\" – eingeweiht.\n\nIm letzten Ranking der Top500 erreichte er im November 2012 den 50. Platz und wurde danach abgeschaltet.\n\n"}
{"id": "3300912", "url": "https://de.wikipedia.org/wiki?curid=3300912", "title": "TI-InterActive", "text": "TI-InterActive\n\nTI-InterActive ist eine kommerzielle Software, die von Texas Instruments vor einigen Jahren entwickelt wurde. Diese Software integriert eine Textverarbeitung mit einem Computer-Algebra-System (Funktionsumfang wie Voyage 200). Auch kann man eine starke Anlehnung an die Philosophie der TI-Graphikrechner erkennen, was sich insbesondere durch den generellen Funktionsumfang und die Menüstruktur ausdrückt. \n\nEine Stärke von TI-InterActive ist seine Dokumentenstruktur. Alle in einem Dokument sichtbaren Definitionen wirken sich auf nachfolgende Berechnungen aus. Löscht oder ändert man die Definition einer Funktion oder Variablen, so werden sofort alle nachfolgenden Berechnungen, Graphen oder Tabellen neu berechnet. Auf diese Weise hat man immer ein in sich schlüssiges Dokument. In dieser Hinsicht unterscheidet sich TI-InterActive deutlich von anderen CAS-Systemen wie Derive, MuPad, Maple, etc., die diese Konsistenz des Dokuments entweder gar nicht oder nur nach Eingabe eines Neu-Berechnen-Befehls sicherstellen. \nDie Befehlssyntax und Arbeitsweise von TI-Interactive entspricht in etwa derjenigen der CAS-Rechner, TI89 und Voyage 200 von TI. Insofern kann TI-InterActive nicht als Weiterentwicklung von Derive betrachtet werden. \n\nWie Derive wird auch TI-InterActive von Texas Instruments seit 2004 nicht weiter gepflegt.\n\nTexas Instruments scheint derzeit alle Kapazitäten in die Weiterentwicklung von TI-Nspire zu investieren. Dieses Programm, das sowohl auf Handhelds, als auch als Software vorhanden ist, bietet ein höheres Maß an Vernetzung von Graphiken, Tabellen und Berechnungen. Das Programm orientiert sich jedoch an der Darstellung des Taschenrechners, was auf dem Computer wenig ansehnlich erscheint. Zudem bietet TI-Nspire nicht die Möglichkeit, Berechnungen und Graphiken in einen Text zu integrieren. \n\n"}
{"id": "3302512", "url": "https://de.wikipedia.org/wiki?curid=3302512", "title": "WOW64", "text": "WOW64\n\nWOW64 (\"Windows-On-Windows 64-bit\") ist ein Subsystem des Windows-Betriebssystems, das in der Lage ist, 32-Bit-Anwendungen auszuführen. WOW64 ist in allen 64-Bit-Versionen von Windows seit Windows 2000 und Microsoft Windows XP enthalten. WOW64 berücksichtigt sämtliche Unterschiede zwischen 32-Bit-Windows und 64-Bit-Windows, insbesondere strukturelle Änderungen an Windows selbst.\n\nDas WOW64-Subsystem ist eine leichtgewichtige Übersetzungsschicht, die auf allen 64-Bit-Versionen von Windows ähnliche Schnittstellen anbietet. Ihr hauptsächlicher Zweck ist die Schaffung einer 32-Bit-Umgebung, welche sämtliche Schnittstellen zur Verfügung stellt, die 32-Bit-Windows-Anwendungen benötigen, um ohne Anpassungen auf einem 64-Bit-System zu laufen. Aus technischer Sicht ist WOW64 in vier Programmbibliotheken (DLLs) implementiert:\n\nTrotz der äußerlichen Ähnlichkeit auf allen 64-Bit-Versionen von Windows unterscheidet sich die Implementierung von WOW64 aufgrund der Architektur des Ziel-Prozessors. Die 64-Bit-Version von Windows beispielsweise, welche für Intel-Itanium-2-Prozessoren entwickelt wurde (bei Microsoft \"IA-64-Architektur\" genannt), benutzt die codice_2 für die Emulation von x86-Anweisungen mittels des Befehlssatzes des Itanium 2. Diese Emulation ist rechenaufwendiger als die Funktionen der codice_2 auf AMD64-Architekturen, welche lediglich den Prozessor aus dem 64-Bit-Modus in den 32-Bit-Modus schalten, während ein 32-Bit-Thread ausgeführt wird. Auf AMD64-Systemen ist für WOW64 keine Emulation notwendig.\n\nWährend der Programmausführung lädt Wow64.dll die 32-Bit-Version von Ntdll.dll und alle notwendigen 32-bit DLLs, die weitgehend unveränderte 32-bit Versionen sind.\n\nDas WOW64-Subsystem berücksichtigt auch andere wichtige Aspekte für die Ausführung von 32-Bit-Anwendungen. Das betrifft unter anderem die Verwaltung von Zugriffen auf die Windows-Registrierungsdatenbank (siehe HKEY_LOCAL_MACHINE\\Software\\WOW6432Node) sowie auf das Dateisystem, welche in den 64-Bit-Versionen von Windows geringfügige Unterschiede aufweisen.\n\nNormalerweise würde man – wie damals beim Schritt von 16 nach 32 Bit – von einem 64-Bit-Windows erwarten, dass sich die neuen 64-Bit-Systemkomponenten im Verzeichnis %SystemRoot%\\system64 befinden und Namen wie kernel64.dll, user64.dll usw. tragen. Das wäre relativ unproblematisch, da die entsprechende 64-Bit-Software ohnehin neu compiliert werden muss.\n\nZwecks Rückwärts-Kompatibilität nutzt das Betriebssystem jedoch das %SystemRoot%\\system32-Verzeichnis und unveränderte DLL-Namen für seine 64-Bit-Komponenten. Bei der Ausführung von 32-Bit-Anwendungen werden Zugriffe auf Bibliotheken aus diesem Verzeichnis nach %SystemRoot%\\SysWOW64 umgeleitet, wo sich die entsprechenden 32-Bit-Versionen befinden. Ausgenommen von dieser automatischen Umleitung sind die Verzeichnisse \nAnalog dazu werden Zugriffe auf die Windows-Registrierungsdatenbank von HKEY_LOCAL_MACHINE\\Software nach HKEY_LOCAL_MACHINE\\Software\\WOW6432Node umgeleitet. Durch Referenzierung eines einzig zu diesem Zweck geschaffenen, rein virtuellen Verzeichnisses %SystemRoot%\\Sysnative ist es 64-Bit-fähigen Anwendungen jedoch auch unter WOW64 möglich, auf die nativen Bibliotheken zuzugreifen.\n\nNeben dem in aktuellen Windows-Versionen enthaltenen WOW64-Subsystem von Microsoft gibt es auch Varianten für andere Betriebssysteme. Diese Nachbildungen dienen dazu, Windows-32-Bit-Anwendungsprogramme ohne Windows-Betriebssystem auf einer Win64-API zu nutzen. So entwickelt z. B. das Wine-Projekt eine freie WOW64-Nachbildung, die für Linux- und anderen Unix-Systeme verfügbar ist. Aus rechtlichen Gründen sind nicht alle Funktionen des originalen Subsystems vorhanden, was die Kompatibilität mit Anwendungsprogrammen einschränken kann. Da aber die wenigsten Anwendungsprogramme alle Funktionen der Win32-API benötigen, sind viele trotzdem voll nutzbar.\n\n\n"}
{"id": "3302853", "url": "https://de.wikipedia.org/wiki?curid=3302853", "title": "Pororo", "text": "Pororo\n\nPororo der kleine Pinguin (Hangeul: , \"\") ist eine computeranimierte Fernsehserie, entwickelt von den südkoreanischen Animation Studios Iconix Entertainment und hergestellt in einer Koproduktion mit der nordkoreanischen Firma Samchŏlli in Kaesŏng. Die Produktion begann im Jahr 2002, und ein Jahr später startete der koreanische Lern- und Erziehungssender EBS-TV die Ausstrahlung. Zurzeit wird die Serie auch in Frankreich, Hongkong, Indien Taiwan und Skandinavien gesendet.\n\nPororo ist ein kleiner Pinguin, der in der Antarktis lebt. Zusammen mit seinen Freunden erlebt er viele Abenteuer: Poby der Eisbär, Loopy der Biber, Harry der Kolibri, Crong der Dinosaurier, Petty der Pinguin und Eddy der Fuchs.\n\nPororo träumt sehnsüchtig vom Fliegen und mehrere Episoden berichten über seine Missgeschicke. Ständig stellt er Unheil an oder versucht seinen Freunden einen Streich zu spielen. Seine Heimat befindet sich in einer alpinen Schneelandschaft, wo er zusammen mit seinem Freund Crong, dem nichtsprechenden Dinosaurier, in einer Schneehütte lebt.\n\nCrong ist ein Babydinosaurier, der eines Tages durch Zufall von Pororo im noch nicht geschlüpften Dinoei gefunden wird (Folge 1). Der ahnungslose Pororo denkt darüber nach, zu Hause ein leckeres Omelette zu kochen. Nachdem er sich entschlossen hat, das Ei nach Hause zu rollen, ist er plötzlich überrascht, als die Schale zu brechen beginnt und der kleine Dinosaurier schlüpft. Pororo ist die erste Person, die Crong sieht, und er glaubt, er sei ein Elternteil. Folglich sieht man Crong stets mit Pororo im Haus und Crong spielt die Rolle des engsten Kumpels in den vielen Abenteuern. \n\nLoopy ist der einzige weibliche Charakter, bis Petty in der Serie auftaucht. Sie liebt Kochen und Kunst. Pororo und Eddy sind immer um ihre Aufmerksamkeit bemüht. Oft dargestellt als die Mutter der jungen Freunde, löst sie meistens die Probleme, die sich in der Handlung ergeben. Loopy lebt in einem riesigen hohlen Baumstamm. \n\nPobys Zuhause befindet sich in einer Eishöhle. Seine Hauptinteressen sind Fotografie und Eisfischen. Er wird als sanfter Riese dargestellt, der des Öfteren die Gruppe mit klugen Ratschlägen unterstützt. \n\nEddy ist der Intellektuelle der Gruppe. In den meisten Folgen erfindet er neue Maschinen, von Zügen und gigantischen Robotern bis hin zu Raketen. Die Mehrzahl seiner Erfindungen scheitern jedoch und bringen ihn und seine Freunde in brenzlige Situationen. Obwohl Pororo sein bester Freund ist, rivalisieren die beiden stets untereinander. Eddys Zuhause ist ein gigantischer Baumstumpf.\n\n"}
{"id": "3303826", "url": "https://de.wikipedia.org/wiki?curid=3303826", "title": "Computer Kontakt", "text": "Computer Kontakt\n\nDie Zeitschrift Computer Kontakt (CK) war eine Computerzeitschrift mit dem Schwerpunkt „Heimcomputer“. Sie erschien Mitte 1984 unter den Verlegern Werner Rätz und Thomas Eberle im Verlag Rätz-Eberle aus Bretten, Deutschland.\n\nIm Unterschied zu anderen Heften dieser Zeit wie der 64’er wurden die Trends in mehreren Computerreihen wie VC-20, C-64, Atari, Sinclair (-Spectrum) und Schneider CPC vorgestellt. Durch die vielen Beiträge der Autoren Peter Finzel und Thomas Tausend zum Atari wurde die Computer Kontakt meist als Atari-Heft wahrgenommen. Später schärfte sich das Profil auf die Computer Atari, Sinclair und Texas Instruments TI 99/4A.\n\nVon 1984 bis 1988 erschien das Heft zunächst monatlich, nach Schwinden der Nachfrage nach „gemischten“ Magazinen erschien die CK alle zwei Monate. Die Computerzeitschrift wurde im Jahr 1988 mit der Ausgabe 2–3 nach 24 Ausgaben eingestellt. Die Atari-Nutzer wurden mit dem \"ATARImagazin\" aus gleichem Hause weiter betreut.\n\n"}
{"id": "3305175", "url": "https://de.wikipedia.org/wiki?curid=3305175", "title": "InLoox", "text": "InLoox\n\nInLoox ist eine Software des Herstellers InLoox GmbH aus München zur Projektplanung, Projektdokumentation, zum Mind Mapping und Projektcontrolling integriert in Microsoft Outlook und im Web. InLoox wurde 1999 gegründet. Seit 2011 ist InLoox mit einer eigenständigen Gesellschaft (InLoox, Inc.) vertreten.\n\nInLoox macht sich die Kommunikations- und Groupwarefunktionen von Microsoft Outlook zunutze und erweitert dessen Funktionsumfang um den Bereich „Projektmanagement“. Die klassische Gantt-Planung wird ergänzt durch Features von CRM- und Controlling-Software. Vor allem Projekte kleinerer und mittlerer Größenordnung werden mit dem Programm bearbeitet.\n\nDie Software wird derzeit weltweit von rund 50.000 Benutzern eingesetzt. Seit 2007 liegen eine englische, französische und spanische Version vor. Im August 2008 folgte eine italienische und im November 2008 eine russische Version.\n\nAls Besonderheit ist die vollständige Integration in Outlook anzusehen, was die Nutzung vorhandener Daten aus Kalendern, Kontakten und E-Mails ermöglichen soll. Der Hersteller will erkannt haben, dass die Strategie einer Ausrichtung der Geschäftsprozesse nach den Kommunikationsprozessen die Akzeptanz der Mitarbeiter für Projektmanagement-Software beim Einsatz in Unternehmen erhöht.\n\nLaufende Projekte werden in einer gemeinsamen Datenbank gespeichert. Der Hersteller hat sich im Jahr 2007 zu einem Systemwechsel entschlossen. In den Vorgängerversionen (2.x bis 4.x) befinden sich sämtliche Daten und Optionen auf Microsoft Exchange Server oder in einer Outlook-Datendatei. Der Umstieg auf SQL ist vermutlich eine Reaktion auf die von Microsoft seit Jahren angekündigte Abschaffung der öffentlichen Exchange-Ordner.\n\nSeit Version 5 kann je nach Anzahl gleichzeitiger Anwender Oracle, Microsoft SQL Server oder MySQL eingesetzt werden. Laut Angabe des Herstellers existieren Schnittstellen zu Microsoft Project, Microsoft Exchange Server und Microsoft Office SharePoint Server. Seit Version 6 wird zudem Microsoft Project unterstützt.\n\nInLoox bietet mit \"InLoox Web App\" außerdem eine Webanwendung, die aktuell in Version 9 vorliegt. Der Web-Client ermöglicht es Projektteams, sich über das Internet zu verbinden und so von unterwegs aus gemeinsam an Projekten zu arbeiten. Seit Juni 2013 gibt es mit den InLoox Mobile Apps für iPhone und für Android-Geräte auch InLoox-Applikationen für mobile Endgeräte.\n\n2012 startete InLoox die SaaS-Lösung InLoox now! Mit Hilfe der Cloud-Technologie können Anwender InLoox PM Web App ohne Software-Installation und ohne eigenen Server nutzen. InLoox now! ist geräte- und plattformunabhängig und kann über verschiedene gängige Internet-Browser angesteuert werden. Anders als im klassischen Lizenzmodell wird ein Zugang zu InLoox now! für die Dauer der Vertragslaufzeit gemietet und beinhaltet Installation, Wartung und Software-Upgrades.\n\n\n"}
{"id": "3311239", "url": "https://de.wikipedia.org/wiki?curid=3311239", "title": "ISC High Performance", "text": "ISC High Performance\n\nDie ISC High Performance (bis 2015 International Supercomputing Conference bzw. ISC) ist eine jährlich in Deutschland stattfindende internationale wissenschaftliche Konferenz und Ausstellung zu den Themen Supercomputer und Hochleistungsrechnen. Traditionell wird im Rahmen dieser Konferenz die erste der beiden jährlichen Aktualisierungen der Supercomputer-Rangliste TOP500 veröffentlicht.\n\nDie Konferenz wurde 1986 durch Hans-Werner Meuer, damals Direktor des Computerzentrums und Professor für Informatik an der Universität Mannheim, unter der Bezeichnung „Supercomputer Seminar“ mitbegründet.\n\n1993 startete Meuer zusammen mit Erich Strohmaier (früher an der Universität Mannheim, danach am NERSC) und Jack Dongarra (University of Tennessee und ORNL) die TOP500, eine Rangliste der Supercomputer, die anfangs noch „Mannheimer Supercomputer Statistik“ hieß. Die Rangliste wird zweimal jährlich aktualisiert, wobei die erste Aktualisierung bei der ISC in Deutschland und die zweite Aktualisierung bei der IEEE Super Computer Conference in den USA präsentiert wird.\n\nVon 1986 bis 2000 fand die Konferenz in Mannheim statt, von 2001 bis 2005 im \"International Convention Center\" in Heidelberg. Von 2006 bis 2008 wurden die Konferenzen im \"International Congress Center\" in Dresden abgehalten. Die International Supercomputing Conference zog danach ins Congress Center Hamburg, wo sie von 2009 bis 2012 residierte. Ab 2013 wird die ISC im Congress Center Leipzig stattfinden.\n\nHier die Entwicklung der Teilnehmeranzahl gemäß Angaben von den jeweiligen Veranstaltungswebseiten:\n\n"}
{"id": "3311512", "url": "https://de.wikipedia.org/wiki?curid=3311512", "title": "Remote Disc", "text": "Remote Disc\n\nRemote Disc ist eine von Apple entwickelte und auf der Macworld 2008 vorgestellte Software, die es ermöglicht, CDs und DVDs von einem anderen Mac oder Windows-PC an einem MacBook Air zu nutzen. Nötig wird dies durch das Fehlen eines optischen Laufwerks in diesem Notebook. Als Alternative bietet Apple das externe \"MacBook Air SuperDrive\", welches per USB mit dem Rechner verbunden wird. Seit Mac OS X Leopard 10.5.2 ist die Software standardmäßig installiert.\n\nZunächst muss die zugehörige Software auf einem Computer mit optischem Laufwerk, der als Quelle dienen soll, installiert werden. Wichtig ist, dass der Quellcomputer sich im selben Netzwerk wie das MacBook Air befindet. Die Software wird mit dem MacBook Air ausgeliefert und befindet sich auf der System-DVD. Ist die Installation beendet, ist der Quellcomputer dauerhaft für die Nutzung von Remote Disc vorbereitet. Nach dieser Erstkonfiguration kann eine beliebige CD oder DVD eingelegt werden, die am MacBook Air verwendet werden soll. Dieser Datenträger wird im Finder angezeigt und kann nun ganz normal genutzt werden.\n\n\n"}
{"id": "3312002", "url": "https://de.wikipedia.org/wiki?curid=3312002", "title": "Paint chat", "text": "Paint chat\n\nEin Paint chat ist eine Software, die es ihren Benutzern erlaubt miteinander zu chatten und miteinander zu malen, beides in Echtzeit.\n\nOft werden diese Programme als Webanwendungen mit einem Java-, Flash-, oder HTML5-Frontend realisiert. Besonders beliebt sind diese Anwendungen in japanischen Oekaki-Foren.\n\nPaintchatApp ist Freeware und besteht aus einem öffentlichen Java-Server und einem Java-Applet. Es wurde im Jahr 2000 von einem japanischen Programmierer unter dem Pseudonym Shichan entwickelt. Die letzte Version wurde 2003 veröffentlicht.\n\nZiteboard ist ein geräteunabhängiges, webbasiertes Whiteboard mit Zoomfunktion und der Möglichkeit zur Kollaboration in Echtzeit, geschrieben in purem html5 und javascript. Nutzer können zusammen auf der zoombaren Benutzeroberfläche private oder öffentliche Notizen zeichnen und eintippen. Das public visual chat Service ist durch ein permanentes URL erreichbar.\n\nTwiddla ist ein browserbasierendes \"Paint Chat\" Tool mit Gesprächsunterstützung und einem Fokus auf Webkonferenzen. Der Service startete im April 2007.\n\niScribble ist eine Online-Community die \"Paint chat\" Channels anbietet.\n\nRateMyDrawings ist ebenfalls eine Online-Community mit \"Paint chat\" Channels.\n\nQueeky ist ebenfalls eine Online-Community mit \"Paint chat\" Channels im sogenannten \"MultiDraw\". User können eigene \"MultiDraw\" Räume erstellen und andere User einladen diese gemeinsam zu nutzen. Zeichnungen die mit Queeky erstellt wurden, können über eine Replay-Funktion abgespielt werden.\nQueeky ist in deutscher und englischer Sprache verfügbar.\n\nDas Malprogramm openCanvas war besonders für nicht-öffentliche Zeichen-sessions beliebt. Die Freeware-Version aus 2000\n\nSeit 2006 existiert ein verbessertes Serverprogramm \"N-SoCS\", der einige Zusatzfunktionen einführte und das Benutzerlimit von 4 auf 24 anhob.\n\nNintendo's PictoChat (Teil des Nintendo DS Betriebssystems) bietet Paint Chat Funktionalität innerhalb eines lokalen Netzwerks.\n\nDrawpile ist freie quelloffene Software, die kollaboratives digitales Malen ermöglicht. Dabei kann entweder ein Client als Server fungieren (ähnlich wie zuvor \"openCanvas 1.1b\") oder mehrere Clients mit einem Webserver verbunden werden.\n\n"}
{"id": "3324045", "url": "https://de.wikipedia.org/wiki?curid=3324045", "title": "Surgical Segment Navigator", "text": "Surgical Segment Navigator\n\nDer Surgical Segment Navigator ist ein computergestütztes System zur medizinischen Navigation bei der computerassistierten Chirurgie. Er setzt auf der gleichen Plattform auf, wie der Surgical Tool Navigator (STN), der Surgical Microscope Navigator (SMN) und der Mehrkoordinatenmanipulator (MKM) von Carl Zeiss.\n\nDer SSN ist ein computergestütztes System zur Knochensegmentnavigation in der Mund-, Kiefer- und Gesichtschirurgie. Mit diesem Navigationssystem werden knöcherne Fehlstellungen präzise anhand einer präoperativen Simulation und Operationsplanung korrigiert. Das System wurde seit 1997 an der Universität Regensburg mit Unterstützung von Carl Zeiss Oberkochen entwickelt. Es basiert auf einem Infrarotlokalisationssystem, bei dem eine Infrarotkamera mindestens drei Infrarotsender, die mit einem Knochenstück verbunden sind, im Raum vermisst. Der SSN wird in der Dysgnathiechirurgie und zur Rekonstruktion von Fehlstellungen der Augenhöhlenwände oder des Mittelgesichts eingesetzt.\n\nSeit 2001 wurde an der Universität Heidelberg der SSN++ entwickelt, ein Navigationssystem, das die markerlose Patientenregistrierung an einem nativen (= markerlosen) CT- oder MRT-Bilddatensatz ermöglicht. Bei diesem System erfasst ein Computer die Lage des Patienten auf dem Operationstisch mithilfe eines Oberflächenscanners. Der SSN++ gleicht dabei das Oberflächenbild des Patienten auf dem Operationstisch mit dem Oberflächenbild des Weichteilmantels im präoperativen CT- oder MRT-Bilddatensatz ab. Dieses Prinzip entspricht dem Gelände-Kontur-Abgleich von Flugkörpern. Vorteil des Verfahrens ist, dass die Erkennung der Patientenlage vereinfacht und vollautomatisch ablaufen kann; auch ein Zweit-CT mit Registriermarkern (wie in der computergestützten Chirurgie oftmals erforderlich) ist verzichtbar, so dass die Strahlenbelastung für den Patienten herabgesetzt werden kann.\n\n"}
{"id": "3324223", "url": "https://de.wikipedia.org/wiki?curid=3324223", "title": "ARTEX", "text": "ARTEX\n\nARTEX (Artist's Electronic Exchange Network) war ein vom kanadischen Künstler Robert Adrian X in Zusammenarbeit mit Bill Bartlett konzipiertes \"Kommunikationsfeld\" für Künstler. Etwa 35 Künstler weltweit nahmen teil. Basis dieses künstlerischen Netzwerks war die gleichnamige Software ARTEX, die das Wiener Büro von I.P. Sharp als einfaches \"interkontinentales, interaktives, elektronisches Kunst-Austausch-Programm\" 1980/81 zur Verfügung stellte.\n\nDas künstlerische Netzwerk nutzte parallel zum großräumigen Computernetz auch andere Möglichkeiten des Telefonnetzes. Historisch bedeutend war jedoch, dass erstmals ein künstlerisches Netzwerk über ein interkontinentales Computernetz betrieben wurde.\n\nRealisierte Projekte als künstlerisches ARTEX Netzwerk waren unter Anderem:\nAußerdem wurde die Konferenzsoftware mit einem TV System ergänzt und mehrere Faxprojekte durchgeführt. Es gab auch andere künstlerische Netzwerkprojekte, die mit der Software und Ausstattung verwirklicht wurden. Unter Anderem:\n\nMedientheoretisch kann ARTEX als künstlerisches Netzwerk aufgefasst werden, das analoge und digitale Netzkunst sowohl im Sinne von 'Kunst im Netz' als auch 'Kunst mit dem Netz' hervorgebracht hat. Die ARTEX Benutzer betrachteten sich schon als eine Art Online Community. Ihre weltweiten Telekommunikationsprojekte nahmen spätere Entwicklungen vorweg.\n\nBasis der Software war die kommerzielle I.P. Sharp Netzwerk Software ARTEX. Es war ein Texteditor implementiert, mit dem Textnachrichten über das Netz ausgetauscht werden konnten, außerdem ein Online-Konferenzprogramm, mit der damals außergewöhnlichen Fähigkeit, mehreren Teilnehmern gleichzeitig eine Art Chat zu ermöglichen. ARTEX konnte zudem Verbindungen zu internationalen Datenbanken aufnehmen.\n\nDie Hardware bestand weitgehend aus dem Computernetz von I.P. Sharp, das interkontinental über Satellit vernetzt wurde.\n\nDie Vorläufer von ARTEX entstanden aus Zusammenarbeiten verschiedener Künstler mit I.P. Sharp Toronto, z. B. das Artbox E-mail Netzwerk. Die erste Verwendung der Software für ein weltweites künstlerisches Kommunikationsprojekt war 'Interplay' von Bill Bartlett für die \"Computer Culture\" Konferenz in Toronto 1979. Nachfolger von ARTEX war das von Robert Adrian X und Gerfried Stocker betriebene Mailboxsystem Zeronet, das zur \"Steirischen Kulturinitiative\" 1992 entstand und bis 1994 als Teil des Fidonet aktiv war.\n\n\n"}
{"id": "3329519", "url": "https://de.wikipedia.org/wiki?curid=3329519", "title": "TeraGrid", "text": "TeraGrid\n\nDas TeraGrid war ein Zusammenschluss aus elf Partnern, die dauerhaft eine große Rechenleistung für wissenschaftliche Berechnungen zur Verfügung stellte. TeraGrid wurde durch die \"Grid Infrastructure Group\" (GIG) der Universität von Chicago koordiniert. Jeder US-amerikanische Wissenschaftler konnte nach einer Begutachtung Zugriff auf TeraGrid erhalten.\n\nDas TeraGrid-Project wurde durch das Extreme Science and Engineering Digital Environment (XSEDE) abgelöst und erweitert.\n\n"}
{"id": "3330538", "url": "https://de.wikipedia.org/wiki?curid=3330538", "title": "Particle Tracing", "text": "Particle Tracing\n\nMit Particle Tracing werden in der Computergrafik bestimmte Algorithmen zur Bildsynthese bezeichnet, die auf Raytracing basieren. Particle-Tracing-Verfahren arbeiten von den Lichtquellen aus und speichern die Beleuchtung während des Auftreffens der Strahlen oder „Partikel“ auf Objekten.\n\nEin bekanntes Particle-Tracing-Verfahren ist Photon Mapping. Dabei werden „Photonen“ von den Lichtquellen aus in die Szene gesendet und in einer von der Geometrie unabhängigen Datenstruktur gespeichert.\n\nEine weitere Methode ist \"Local Linear Density Estimation.\" Dabei wird die Position der Partikel innerhalb eines Dreiecks gespeichert. Durch diese platzsparende Speicherung kann eine sehr große Anzahl von Partikeln ausgesendet werden. Im Gegensatz zu üblichen Raytracing-Verfahren ist diese Methode unabhängig von der Position des Betrachters und stellt somit eine Alternative zu Radiosity dar.\n\nEin Renderer, der Particle Tracing verwendet, ist MicroStation.\n"}
{"id": "3331718", "url": "https://de.wikipedia.org/wiki?curid=3331718", "title": "TRNSYS", "text": "TRNSYS\n\nTRNSYS (abgekürzt: TRaNsient SYstems Simulation; deutsch etwa: instationäre Systemsimulation) ist ein Werkzeug zur Simulation von Anlagen und Gebäuden. Das Programm wurde 1975 an der Universität von Wisconsin zur Simulation einer Solaranlage entwickelt. Zur Programmierung wurde Fortran eingesetzt. Der modulare Aufbau der Anwendung ermöglicht die Lösung einer Vielzahl von Problemen. Neben der Simulation von Solaranlagen hat sich das Programm vor allem im Bereich von Niedrigenergiehäusern, technischen Anlagen wie Lüftungsgeräten, Wärmepumpen, Kältemaschinen und Heizungen sowie bei Blockheizkraftwerken und Brennstoffzellen etabliert.\n\nEin Hauptanwendungsgebiet ist die thermisch energetische Gebäudesimulation. Nach der Eingabe der Geometrien und Randbedingungen einer oder mehrerer Gebäudezonen lässt sich mit Hilfe eines Wetterdatensatz für das Gebäude eine Aussage über die Temperatur in den Zonen treffen. Dies ist aufschlussreich für die Auslegung von Sonnenschutzeinrichtungen, Fenstern oder Kühlsystemen. Die Solarenergienutzung und der Heizwärmebedarf lassen sich damit optimieren. Ferner ermöglicht die Simulation den Entwurf und die Optimierung von Regelstrategien z. B. die Sonnenschutzsteuerung oder natürliche Nachtlüftung.\nDie Ausgabe der Ergebnisse (Raumtemperaturen und Wärmebedarf) erfolgt grafisch. Diese Ergebnisse sowie Spitzen-Heiz- und Kühllasten sowie Energiebedarfe werden auch im ASCII-Format ausgegeben und können dann (z. B. mittels Tabellenkalkulation) weiterbearbeitet werden.\n\n\n"}
{"id": "3332140", "url": "https://de.wikipedia.org/wiki?curid=3332140", "title": "Digsby", "text": "Digsby\n\nDigsby ist ein Instant-Messaging-Client, der Chat-Dienste, Social-Network-Plattformen und E-Mail-Kommunikation zusammenführt. Die Software bündelt sämtliche Netzwerke unter einer Oberfläche und bietet Zugriff auf ICQ, den Windows Live Messenger, Yahoo Messenger, Google Talk und andere XMPP-Dienste, die Social Networks Facebook und Myspace, den Microbloggingdienst Twitter, sowie auf beliebige E-Mail-Postfächer.\nZuerst war nur eine Betaversion erschienen, für die man einen sogenannten „Einladungscode“ benötigt, um sie zu installieren. Seit dem 20. März 2008 wird dieser Einladungscode nicht mehr benötigt.\n\nDigsby geriet vor allem in Kritik, da es die Rechenzeit seiner Benutzer „verkauft“. Dies wurde auch von einem Digsby-Entwickler bestätigt, allerdings kann man diese Funktion deaktivieren. Bei der Installation der Software werden einige Programme, Toolbars etc. zur Installation angeboten, wobei das Design anscheinend darauf ausgelegt ist, den Anwender zum Akzeptieren zu motivieren. Des Weiteren speichert Digsby sämtliche Account-Daten, also auch die Passwörter, auf einem Server der Betreiberfirma, um so einen einfachen Zugriff auf alle hinterlegten Accounts von jedem Gerät aus zu ermöglichen, ohne dass diese manuell eingerichtet werden müssen.\n\n"}
{"id": "3335058", "url": "https://de.wikipedia.org/wiki?curid=3335058", "title": "Knochensegmentnavigation", "text": "Knochensegmentnavigation\n\nMithilfe der Knochensegmentnavigation werden fehlstehende Knochen in einem computerassistierten chirurgischen Eingriff in die richtige Position geführt und mittels Osteosynthese befestigt. Die Knochensegmentnavigation wurde zuerst in der Mund-, Kiefer- und Gesichtschirurgie realisiert.\nKnochensegmente können angeboren oder aufgrund eines Unfalles fehlstehen. Solche Fehlstellungen können im Mund-, Kiefer- und Gesichtsbereich Einfluss auf die Ästhetik und auf die Funktion der Organe haben; sind die knöchernen Begrenzungen der Augenhöhlenwände betroffen, dann kann es zum Doppelsehen kommen, ist das Kiefergelenk betroffen, dann kann es zu Störungen des Zahnreihenschlusses kommen, ist das gesamte Schädeldach im Sinne einer Kraniosynostose betroffen, kann ein erhöhter Hirndruck resultieren.\n\nDas Durchtrennen des Knochens und Wiederzusammenfügen in korrekter Position bezeichnet man als Umstellungsosteotomie. Damit der Knochen nach der Operation auch wirklich in der korrekten Position steht, wird solch eine Umstellungsosteotomie im Vorfeld der Operation geplant und simuliert.\nDiese Simulation ist erforderlich, um Operationszeit zu sparen. Häufig werden Knochensegmente auch während der Operation gar nicht oder nur zu geringem Teil freigelegt und bleiben so von Muskeln, Fettgewebe und Haut bedeckt – eine Beurteilung, ob ein Knochensegment in der korrekten Position steht, ist intraoperativ so nur schwer oder unmöglich zu treffen – dieser Umstand unterstreicht die Notwendigkeit einer präoperativen Operationsplanung und Simulation an einem freigelegten Knochenmodell.\n\nUmstellungsosteotomien im Rahmen der Dysgnathiechirurgie werden in der Regel anhand von Gipsmodellen der Kiefer in einem Artikulator geplant. Umstellungsosteotomien am nicht-zahntragenden knöchernen Schädel können an Stereolithographiemodellen geplant werden. Diese körperlichen Modelle müssen für die Operationssimulation zersägt und anschließend neu zusammengefügt werden.\nNeu ist seit den 90er Jahren auch die Möglichkeit, Umstellungsosteotomien an einem Computer direkt am präoperativen CT- oder MRT-Bilddatensatz zu simulieren; das erspart die hohen Kosten für die Modellerstellung und die aufwendige Arbeit des Zersägens und Neuzusammensetzens der körperlichen Modelle. Das erste System, das solch eine Operationssimulation erlaubt, ist die Laboratory Unit for Computer Assisted Surgery (LUCAS), die seit 1998 an der Universität Regensburg mit Unterstützung von Carl Zeiss Oberkochen entwickelt wurde.\n\nDer Nutzen einer Operationsplanung hängt davon ab, wie gut es gelingt, die Simulation einer Umstellungsosteotomie am Patienten zu reproduzieren. Überwiegend war für die Übertragung der Operationsplanung auf den Patienten das Augenmaß des Operateurs ausschlaggebend. Darüber hinaus wurden verschiedene Kopfrahmen entwickelt, an denen ein Knochensegmentversatz mit mechanischen Hilfsmitteln eingestellt wurde; solch einen Kopfrahmen musste der Patient dann allerdings bereits während der CT- oder MRT-Bildgebung tragen; erschwerend kam hinzu, dass der Kopfrahmen von der CT- oder MRT-Bildgebung über die Operationsplanung bis hin zum operativen Eingriff unverändert in Position bleiben musste – für den Patienten war das verhältnismäßig unkomfortabel, bei Kindern wegen der fehlenden Mitarbeit sogar ganz unmöglich.\n\nDas erste System, das eine Knochensegmentnavigation erlaubte, war der Surgical Segment Navigator (SSN), der seit 1997 an der Universität Regensburg mit Unterstützung von Carl Zeiss (Oberkochen) entwickelt wurde. Das System arbeitet kopfrahmenlos und besteht aus einer Infrarotkamera und Infrarotsendern, die direkt am Schädelknochen verankert werden. Mindestens drei Infrarotsender werden am Schädeldach befestigt, um Kopfbewegungen insgesamt zu erfassen. Mindestens drei weitere Infrarotsender werden mit dem Knochensegment verbunden, das im Rahmen einer Umstellungsosteotomie versetzt werden soll. Die räumliche Lage der Infrarotsender – und damit auch die des Knochensegments – wird von der Infrarotkamera vermessen, das Prinzip entspricht dem der Satellitennavigation. Die Workstation des Surgical Segment Navigator (SSN) visualisiert die Ausgangs- und Zielposition des zu versetzenden Knochensegments und kennzeichnet zusätzlich die aktuell gemessene Lage des gelösten, frei beweglichen Knochensegments. Das Knochensegment wird entsprechend der Operationssimulation navigiert, indem die Visualisierung der aktuell gemessenen Position mit der Visualisierung für die Zielposition in Deckung gebracht wird.\n\nDie Knochensegmentnavigation wird eingesetzt zur Korrektur von Kieferfehlstellungen im Rahmen der Dysgnathiechirurgie, zur operativen Einstellung des Kiefergelenkköpfchens, zur Rekonstruktion des Mittelgesichts und der Augenhöhlenwände.\n\n\n"}
{"id": "3342182", "url": "https://de.wikipedia.org/wiki?curid=3342182", "title": "Computerwelt", "text": "Computerwelt\n\nComputerwelt ist eine österreichische Fachzeitung für den gesamten Bereich der Informationstechnik und Telekommunikation. Sie erscheint derzeit 14-täglich in einer Auflage von 18.000 Exemplaren und erreichte laut der Österreichischen Media-Analyse 2005 rund 78.000 Leser. Die Zeitung richtet sich an Entscheidungsträger in Unternehmen, für die IT und Telekommunikation eine strategisch entscheidende Rolle spielt. Zusätzlich zur Printausgabe erscheint die COMPUTERWELT auch online und als iPad-Version, ausgewählte Beilagen sind auch als E-Paper frei verfügbar.\n\nDie Zeitung ist nur in ausgewählten größeren Trafiken erhältlich und wird mehrheitlich direkt versendet. Das Format der Zeitung beträgt 228 mm × 300 mm, der Satzspiegel: 208 mm × 272 mm. Beilagen zur Computerwelt sind die Magazine Top 1001 (August), Die IT-Macher (November), und IT-Atlas (Mai). Der Verlag gibt zudem noch das Magazin ComputerPartner sowie das IT-Jahrbuch und das IT-Lexikon heraus. \n\nHerausgeber und Chefredakteur ist Oliver Weiss. Der Redaktion gehören derzeit 7 Personen an, der gesamte Verlag hat 16 Mitarbeiter.\n\nDie Computerwelt wurde im Jahr 1987 von Manfred Weiss als Teil der weltweiten Verlagsgruppe International Data Group (IDG) gegründet und erschien im Verlag \"IDG Communications Verlags GesmbH\", die in der Zieglergasse residierte. Im Jahre 1998 begann der Internetauftritt unter der Adresse cw-online.at. Nachdem der Verlag bis 1999 Überschüsse lieferte, war das Geschäftsjahr 2000 ausgeglichen und 2001 ein Verlust. Es wurde daraufhin ein Restrukturierungsprogramm gestartet, und IDG beschloss, sich aus dem operativen Geschäft in Österreich zurückzuziehen. \n\nIm Dezember 2002 übernahm Manfred Weiß von Ralph Peter Rauchfuss die Herausgeberschaft. Manfred Weiß gründete im Jahre 2003 die \"Info Technologie Verlag GmbH\" in der ums Eck liegenden Halbgasse und übernahm alle Anteile der Computerwelt als Lizenz-Nehmer der IDG. Bis zum Frühjahr wurde die Chefredakteurin Karin Tzschentke von Klaus Lorbeer abgelöst, der später zusammen mit Edmund Lindau agierte. Seit Anfang 2004 ist der Internetauftritt unter computerwelt.at erreichbar. 2005 ging Lorbeer, und im Frühjahr 2011 übernahm Weiss auch die Chefredaktion.\n\n2014 übernahm Oliver Weiss Herausgeberschaft und Chefredaktion, gleichzeitig änderte sich der Verlag von \"Info Technologie Verlag GmbH\" zum \"CW Fachverlag GmbH\", unter Beibehaltung derselben Adresse.\n\n"}
{"id": "3342747", "url": "https://de.wikipedia.org/wiki?curid=3342747", "title": "Operationsplanung", "text": "Operationsplanung\n\nMithilfe einer Operationsplanung wird im Rahmen der computergestützten Chirurgie vor einer Operation ein Zugangsweg festgelegt oder der Versatz eines Knochensegments definiert.\nDie Operationsplanung hat einen hohen Stellenwert in der Neurochirurgie und der Mund-, Kiefer- und Gesichtschirurgie. Das Umsetzen des Operationsplans am Patienten erfolgt dann im Allgemeinen unter Anwendung eines chirurgischen Navigationssystems.\n\nGrundlage für die Operationsplanung bilden üblicherweise CT- oder MRT-Bilddatensätze.\nIn der Mund-, Kiefer- und Gesichtschirurgie können abweichend davon Operationsplanungen im Rahmen der Dysgnathiechirurgie auch anhand von Gipsmodellen der Kiefer in einem Artikulator vorgenommen werden.\n\nFür die Operationsplanung ist ein dreidimensionales Abbild des Patienten erforderlich, innerhalb dessen die Planung vorgenommen wird. Bahnbrechend war in diesem Zusammenhang die dreidimensionale Bildgebung im Rahmen der Computertomographie durch G. Hounsfield in den 1970er Jahren. In den 1980er Jahren gelang dem Radiologen M. Vannier mit seinem Team die erste dreidimensionale Rekonstruktion von CT-Daten auf einem Computer. Anfang der 1990er Jahre konnten dann Operationsplanungen erstmals an Stereolithografiemodellen vorgenommen werden. Ende der 1990er Jahre wurden Operationsplanungen im Rahmen von Umstellungsosteotomien erstmals auch am Computer simuliert und mit einem Navigationssystem am Patienten umgesetzt.\n"}
{"id": "3345886", "url": "https://de.wikipedia.org/wiki?curid=3345886", "title": "Laboratory Unit for Computer Assisted Surgery", "text": "Laboratory Unit for Computer Assisted Surgery\n\nLaboratory Unit for Computer Assisted Surgery (LUCAS) ist ein System, das die Planung einer Umstellungsosteotomie am Computer erlaubt. LUCAS wurde seit 1998 an der Universität Regensburg mit Unterstützung von Carl Zeiss Oberkochen entwickelt. Der Operationsplan kann mit einem Navigationssystem am Patienten reproduziert werden. LUCAS setzt auf der gleichen Plattform auf, wie der Surgical Segment Navigator (SSN), Surgical Tool Navigator (STN), der Surgical Microscope Navigator (SMN) und der Mehrkoordinatenmanipulator (MKM) von Carl Zeiss.\n\nEin CT- oder MRT-Bilddatensatz aus einzelnen zweidimensionalen Schichten wird auf LUCAS geladen. Die Bilddaten werden aufbereitet zur Beseitigung von Bildrauschen, zur Kantenbetonung und Kontrastverstärkung. Anschließend werden die einzelnen zweidimensionalen CT-Schichten zu einem dreidimensionalen Modell am Computer zusammengesetzt. In einem Gittermodell wird das Knochensegment, das umgestellt werden soll, markiert. Das Segment wird auf LUCAS verschoben, bis es korrekt steht; Kriterien für die richtige Stellung eines Knochensegments können ein Symmetrievergleich oder auch die Ausmessung des Orbitavolumens darstellen. Auch eine texturierte Darstellung der Segmentumstellungsosteotomie ist möglich. Die Vektoren für den räumlichen Segmentversatz werden zusammen mit dem Bilddatensatz danach auf den Surgical Segment Navigator übertragen.\n\n"}
{"id": "3352216", "url": "https://de.wikipedia.org/wiki?curid=3352216", "title": "EKA (Supercomputer)", "text": "EKA (Supercomputer)\n\nEKA ist ein indischer Supercomputer. Auf der TOP500-Liste der schnellsten Computer der Welt von November 2013 nahm er Platz 389 ein, ist jedoch im Juni 2014 aus der Liste herausgefallen. EKA war der erste Supercomputer aus einem Schwellenland, der es unter die ersten 10 Platzierungen der TOP500-Liste geschafft hat (im Jahr 2007). Die Bezeichnung EKA ist Sanskrit und bedeutet „Nummer 1“. \n\nAußerdem war EKA bei seiner Indienststellung das weltweit leistungsfähigste industriell eingesetzte System (die anderen damals unter den ersten 20 der Liste befindlichen Computer wurden sämtlich mit staatlichen Mitteln finanziert). EKA wird für neurale und molekulare Anwendungen (z. B. Molekulardesign, Moleküldynamik), Crash-Simulationen und Computeranimationen eingesetzt. Entwickelt wurde EKA an den \"Computational Research Laboratories\" (CRL) in Pune, die eine Tochtergesellschaft der indischen Tata Group sind.\n\nEKA basiert auf einem von HP entworfenen und gelieferten Xeon-Cluster (Platform 3000 BL460c), in dem fast 1.800 Rechenknoten mit 14.240 sogenannten „Clovertown“-Prozessoren (Vierkern-Xeon-Prozessoren), getaktet mit 3 GHz, verwendet werden. Zu ihrer Verbindung ist ein InfiniBand-Netzwerk in der Double Data Rate-Variante (DDR) eingesetzt, und zwar weltweit erstmals unter Einsatz von Lichtwellenleiter-Technologie (Glasfaserkabel). Als Betriebssystem wird Linux verwendet. \n\nDas Layout des Datenzentrums ist annähernd kreisförmig, was eine extrem kompakte Bauweise ermöglichte. Eine solche Rechnerarchitektur wurde in dieser Größenordnung erstmals beim EKA angewandt.\n\nDas System hat eine Spitzenleistung (Rpeak) von bis zu 170 Teraflops (170 Billionen Gleitkommaoperationen pro Sekunde) und eine Dauerleistung von 117,9 Teraflops nach den LINPACK-Benchmarks.\n\n"}
{"id": "3354830", "url": "https://de.wikipedia.org/wiki?curid=3354830", "title": "Menü-Taste", "text": "Menü-Taste\n\nDie Menü-Taste auf einer Computertastatur dient zum Aufruf des sogenannten Kontextmenüs, das eine zur Cursorposition relevante Funktionsauswahl enthält. Die Funktion der Menü-Taste entspricht meist der Tastenkombination + . Im Unterschied dazu liefert der Rechtsklick mit der Maus meist ein mauszeigerrelevantes Kontextmenü.\n\nDie Menü-Taste wurde zusammen mit zwei Windowstasten beim 104/105-Tasten-Layout eingeführt (zuvor 102 Tasten), das im Zuge des Betriebssystems Windows 95 entwickelt wurde.\n\nDas Tastatursymbol ist standardisiert im Amendment 1 (2012) zu ISO/IEC 9995-7:2009 \"„Information technology – Keyboard layouts for text and office systems – Symbols used to represent functions“\" als Symbol 98, sowie in IEC 60417 \"„Graphical Symbols for use on Equipment“\" als Symbol IEC 60417-6089. In Unicode ist das Zeichen zurzeit (September 2012) nicht enthalten, seine Aufnahme ist jedoch beantragt. Für Linux hat die LANANA in der Private Use Area den Codepunkt U+F811 belegt.\n"}
{"id": "3363609", "url": "https://de.wikipedia.org/wiki?curid=3363609", "title": "After Dark", "text": "After Dark\n\nAfter Dark war die weltweit erste kommerzielle und zugleich die erfolgreichste Serie von Bildschirmschonern, die ab 1989 von \"Berkeley Systems, Inc.\" ursprünglich für Apple-Macintosh-Computer entwickelt wurde. Mittlerweile gibt es auch Versionen für Windows-Systeme sowie zahlreiche Nachahmerprodukte. After Dark gehört zu den bekanntesten und einflussreichsten Bildschirmschonern. Kultstatus errang das Modul \"Flying Toasters\" („Fliegende Toaster“), zu dem auch ein Handyspiel existiert. Es erschienen weitere Spiele unter dem Namen \"After Dark Games\"; außerdem wurde das Buch \"Art of Darkness\" von Erfert Fenton von dem Programm inspiriert. Die offene Modultechnik gestattete anderen Herstellern, eigene Bildschirmschoner zu integrieren.\n\nVertrieben wird die Software von Sierra Attractions, in Deutschland von Vivendi Games Deutschland. Die Mac-OS-X-Version wurde an das japanische Unternehmen Infinisys lizenziert.\n\n\n\n\nEinzelne Elemente, insbesondere die fliegenden Toaster, wurden in verschiedene Medien übertragen, so in Fernsehserien wie Die Simpsons, Futurama, Rugrats und Beverly Hills 90210.\n\nIm Zusammenhang mit den fliegenden Toastern wurden zwei Rechtsfälle bekannt: 1993 klagte der Hersteller gegen das kanadische Unternehmen \"Delrina\", das dieses geschützte Modul verwendet hatte (und es später änderte). 1994 wurde Berkeley Systems von der Band Jefferson Airplane vorgeworfen, sie habe dies vom Albumcover \"Thirty Seconds Over Winterland\" übernommen. Diese Klage wurde jedoch abgewiesen, da das Cover nicht geschützt war.\n\n"}
{"id": "3365759", "url": "https://de.wikipedia.org/wiki?curid=3365759", "title": "JavE", "text": "JavE\n\nJavE [] (für engl. \"Java Ascii Versatile Editor\") ist ein grafischer Texteditor für ASCII-Art.\n\nDie Hauptbesonderheit von JavE ist die Mischung aus Text- und Grafikeditor. So können mithilfe der Maus zum Beispiel Linien oder komplexere Formen gezeichnet werden, wobei das Ergebnis in Form von ASCII-Zeichen in einer Textdatei erscheint.\nDa das Programm in Java geschrieben ist, läuft es unter anderem auf Mac OS X, Linux, UNIX und Windows.\n\nNeben einer großen Zahl an Werkzeugen zur Bearbeitung von statischem Text, unter anderem auch das Hantieren mit FIGlet-Fonts, bietet der Editor auch die Möglichkeit, ASCII-Animationen zu erstellen und wiederzugeben.\n\nDie erste Version von JavE entstand im November 2000. Damals handelte es sich noch nicht um eine eigenständige Anwendung, sondern um ein Java-Applet, das direkt in kompatiblen Browsern lief. Durch die Zusammenarbeit über die Newsgroup \"alt.ascii-art\" wurde die Software über zwei Jahre hinweg um Techniken aus dem Fundus von ASCII-Art-Künstlern erweitert und schließlich zu einer eigenständigen Desktop-Anwendung ausgebaut. Seit Juni 2002 ist es in der stabilen Version 5.0 verfügbar.\n\nNach 2002 kam die Weiterentwicklung vorübergehend zum Erliegen. Seit 2005 wird an einer neuen Version 6.0 gearbeitet.\n\nBis einschließlich Version 5 wurde die Software als Freeware veröffentlicht. Mit Version 6.0 soll das Programm erstmals unter der Common Public License als freie Software publiziert werden.\n\n"}
{"id": "3370006", "url": "https://de.wikipedia.org/wiki?curid=3370006", "title": "Foresight Linux", "text": "Foresight Linux\n\nForesight Linux (engl., zu deutsch ‚Blick in die Zukunft‘ oder „Voraussicht“) war eine auf \"rPath Linux\" (damals noch \"Specifix Linux\") basierte Linux-Distribution mit dem Ziel, die jeweils aktuelle — bzw. aus Sicht anderer Distributionen — kommende Version der Oberflächen Gnome, KDE, LXDE und Xfce zu präsentieren. Als Paketverwaltungs-Software kam das von \"rPath Linux\" entwickelte Conary zum Einsatz, das bei einem Update eines Pakets nur geänderte Daten der neuen Paketversion herunterlädt.\n\nDie Anfänge der Distribution sind kaum dokumentiert, Distrowatch führt als älteste Meldung eine Version 0.6 vom März 2005. Foresight Linux erhielt von Ars Technica den \"2008 Ovatio Awards\" und wurde — zusammen mit openSUSE — zur \"Distro of the Year\" ausgezeichnet. \n\nAm 12. Mai 2015 gaben die Entwickler die Einstellung der Distribution bekannt.\n\n"}
{"id": "3379003", "url": "https://de.wikipedia.org/wiki?curid=3379003", "title": "Lucas, der Ameisenschreck", "text": "Lucas, der Ameisenschreck\n\nLucas, der Ameisenschreck (Originaltitel: \"The Ant Bully\") ist ein US-amerikanischer Computeranimationsfilm aus dem Jahr 2006, der auf dem Kinderbuch \"The Ant Bully\" von John Nickle basiert. Regie führte John A. Davis, der auch das Drehbuch schrieb und den Film mitproduzierte.\n\nDer zehnjährige Lucas Nickle zieht in eine neue Nachbarschaft. Hier hat er keine Freunde und wird von einer Bande Jugendlicher schikaniert. Der Anführer der Bande macht sich über Lucas lustig, weil dieser kleiner ist als er und er sich nicht gegen ihn wehren kann. Seinen Frust darüber lässt Lucas dann an noch kleineren Lebewesen aus: Den Ameisen in seinem Garten. Diese und deren Ameisenhügel beschießt er mit seiner Wasserpistole. \n\nAls seine Eltern Fred und Doreen auf Hochzeitsreise gehen, bleibt er mit seiner großen Schwester Tiffany und Großmutter Mommo auf sich allein gestellt. Mit der Schwester versteht er sich nicht besonders und seine Großmutter ist besessen von dem Gedanken, dass Aliens sie entführen könnten. Der Schädlingsbekämpfer Stan Beals nutzt die Abwesenheit der Eltern aus und schwätzt dem jungen Lucas seine Dienste auf: Lucas unterzeichnet einen Vertrag, damit Beals die Ameisen im Garten vernichten soll. Nachdem Lucas abermals vom Anführer der Bande tyrannisiert wird, setzt er mit einem Wasserschlauch deren gesamten Ameisenbau unter Wasser und verursacht damit Zerstörung und Chaos. \n\nDie Ameise \"Zoc\" fertigt daraufhin einen magischen Trank an, der Lucas eines Nachts verabreicht wird. Durch diesen schrumpft Lucas auf die Größe einer Ameise. Den geschrumpften Lucas bringt man in den Ameisenbau. Dort fordern viele Ameisen eine harte Bestrafung für den \"Zerstörer\"; manche wollen ihn auffressen. Doch die weise Königin verurteilt ihn zur Ausbildung und Arbeit im Ameisenbau, damit er die Lebensweise der Ameisen kennenlernt. Die Ameise \"Hova\" meldet sich freiwillig ihn zu unterrichten. Lucas freundet sich mit der Zeit mit den Ameisen an. \n\nAls der von ihm beauftragte Schädlingsbekämpfer auftaucht, hilft er den Ameisen diesen abzuwehren und somit die Kolonie vor dem sicheren Tod zu retten. Die Ameisenkönigin ernennt ihn ehrenhalber zu einer Ameise und gibt ihm den Namen \"Rokai\". \"Zoc\" gibt Lucas das Gegenmittel, wonach der Junge wieder zu seiner normalen Größe zurückkehrt. Lucas freut sich, als seine Eltern zurückkehren. Bei den Ameisen hat er gelernt, dass man nur erfolgreich ist, wenn man zueinander steht und Aufgaben gemeinsam angeht. Er und die Kinder aus der Nachbarschaft wehren sich nun gemeinsam gegen die Übergriffe des Raufbolds und können ihn zusammen in die Flucht schlagen.\n\nDas \"Lexikon des internationalen Films\" schrieb: „Der Computer animierte Trickfilm bietet spannende, zudem pädagogisch lehrreiche Unterhaltung und suggeriert durch die Wahl seines Sujets, dass der heimische Zierrasen ein Dschungel voller Gefahren ist. Für kleinere Kinder könnten einige Szenen zu schreckhaft sein.“\n\nJames Berardinelli schrieb auf \"ReelViews\", der Film sei eine Enttäuschung für jeden, der auf eine Besserung der zuletzt sinkenden Qualität der Animationsfilme gehofft habe. Die aus dem Moralisieren und aus aneinandergereihten Actionszenen bestehende Handlung sei eine „Beleidigung“ für jeden gebildeten Menschen. Berardinelli erwähnte die zahlreichen beteiligten Stars, wunderte sich jedoch, dass der wenig bekannte Zach Tyler die Hauptsprechrolle bekommen habe.\n\nDie Zeitschrift \"Cinema\" schrieb, der einzig für Kinder bestimmte Film sei „kurzweilig, rasant, komisch und sympathisch“. Er beinhalte „beachtliche (wenn auch nicht herausragende) Animationen und eine gehörige Portion Knirps-Action“ sowie „viel schrägen Witz“.\n\n„Anders als die Ameisenhits \"Antz\" und \"Das große Krabbeln\" setzt dieses Spektakel in Videospieloptik ganz auf schrägen Witz und wilde Action. Für Knirpse springt auch eine Lektion in sozialer Kompetenz dabei heraus.“\n\nJulia Roberts wurde im Jahr 2007 für ihre Sprechrolle für den \"Kids' Choice Award\" nominiert. John Debney wurde 2007 für die Filmmusik für den \"Annie Award\" nominiert. Der Film erhielt 2007 den Preis \"Artios\" der \"Casting Society of America\" und wurde für den Tonschnitt für den \"Golden Reel Award\" der US-amerikanischen \"Motion Picture Sound Editors\" nominiert.\n\nDie Produktionskosten betrugen schätzungsweise 50 Millionen US-Dollar. Der Film startete in den US-amerikanischen und in den kanadischen Kinos am 28. Juli 2006, wobei neben der normalen die IMAX-Version veröffentlicht wurde. Der deutsche Kinostart folgte am 12. Oktober 2006. Der Film spielte in den Kinos der USA etwa 28,1 Millionen US-Dollar ein.\n\n"}
{"id": "3382473", "url": "https://de.wikipedia.org/wiki?curid=3382473", "title": "IT-Administrator (Zeitschrift)", "text": "IT-Administrator (Zeitschrift)\n\nIT-Administrator – Das Magazin für professionelle System- und Netzwerkadministration ist eine 2004 gegründete Computerzeitschrift, welche in deutscher Sprache erscheint. Das Magazin wird vom Heinemann Verlag publiziert. Es handelt sich um eine Fachzeitschrift für den System- und Netzwerkadministrator. Neben Produkttests und technischen Hintergrundberichten stehen im Heft vor allem die Beiträge im Vordergrund, die den beruflichen Alltag unterstützen sollen. Dazu zählen Workshops, Tipps und Tricks und Reportagen über den Einsatz unterschiedlicher Produkte.\n\nDas von der Medialinx AG herausgegebene Admin-Magazin wurde mit April 2014 vom Heinemann Verlag übernommen und in die Zeitschrift IT-Administrator integriert.\n\nDie Zeitschrift erscheint seit September 2004 monatlich. Seit 2008 veröffentlicht die IT-Administrator-Redaktion neben den zwölf regulären Monatsausgaben zweimal im Jahr ein Sonderheft. \n"}
{"id": "3387583", "url": "https://de.wikipedia.org/wiki?curid=3387583", "title": "Warnock-Algorithmus", "text": "Warnock-Algorithmus\n\nDer Warnock-Algorithmus ist eine Methode aus der Computergrafik zur Verdeckungsberechnung, also um zu ermitteln, welche Teile von Objekten vom Betrachter aus sichtbar sind. Er wurde 1969 von John Warnock entwickelt und meistens auf polygonale Szenen angewandt.\n\nDer Warnock-Algorithmus teilt die Bildfläche in vier gleiche Quadrate auf. Diese Teilung wird rekursiv fortgesetzt. Bei jedem Schritt des Teilungsprozesses kann ein Polygon auf vier verschiedene Arten mit einem Flächenelement in Beziehung stehen (siehe Bild). Die Teilung wird in folgenden Fällen abgeschlossen, da über die Darstellung eines Flächenelements eine einfache Entscheidung getroffen werden kann:\n\nWenn soweit unterteilt wurde, dass die Flächenelemente nur noch ein einziges Pixel umfassen, und keiner der obigen vier Fälle eingetreten ist, so wird die \"z\"-Koordinate aller Polygone am Mittelpunkt der Fläche berechnet. Das Polygon mit der \"z\"-Koordinate, die am nächsten zum Betrachter liegt, bestimmt dann die Farbe des Pixels. Um Antialiasing zu erreichen, können die Flächen noch weiter unterteilt werden, sodass sich die Farbe eines Pixels aus dem Mittelwert der dazugehörigen Flächenelemente ergibt.\n\n"}
{"id": "3387850", "url": "https://de.wikipedia.org/wiki?curid=3387850", "title": "Weiler-Atherton-Algorithmus", "text": "Weiler-Atherton-Algorithmus\n\nDer Weiler-Atherton-Algorithmus ist ein Verfahren aus der Computergrafik zur Verdeckungsberechnung von Polygonen.\n\nDer erste Schritt des Weiler-Atherton-Algorithmus besteht darin, alle Polygone näherungsweise nach ihren \"z\"-Koordinaten zu sortieren. Das Polygon \"A\", das laut dieser groben Sortierung am nächsten liegt, wird nun dazu verwendet, alle Polygone gegen \"A\" zu clippen und entlang dessen Kontur aufzuteilen. So entstehen zwei Listen: eine „Innenliste“, die alle Polygonteile enthält, die sich nach Projektion innerhalb vom clippenden Polygon \"A\" befinden (im Beispielbild rechts \"B\"\"A\"), sowie eine „Außenliste“ mit allen außerhalb liegenden Teilen (im Beispielbild \"B\"\"A\").\n\nAlle Polygone der Innenliste, die sich hinter \"A\" befinden, werden gelöscht, da sie nicht sichtbar sind. Falls hingegen eines der Polygone der Innenliste näher am Betrachter als \"A\" liegt, so liegt das daran, dass die anfängliche Sortierung hier versagt hat. Für jedes dieser Polygone werden die Polygonteile der Innenliste darauf getestet, ob sie näher liegen, und eventuell geclippt. Dies läuft rekursiv ab. Am Ende des Prozesses wird die Innenliste entsprechend aktualisiert. Anschließend werden die Polygone der Außenliste abgearbeitet.\n\nZum Clippen werden stets die anfänglichen und nicht die aufgeteilten Polygone verwendet, da dies wegen der meist einfacheren Form der Originalpolygone weniger Aufwand erfordert. Daher muss für jedes aufgeteilte Polygon auch das Originalpolygon angegeben werden.\n\nUm auch Polygone verarbeiten zu können, die sich gegenseitig überlappen, verwendet der Algorithmus einen Stapelspeicher. Dieser enthält alle clippenden Polygone, deren Verarbeitung wegen eines rekursiven Aufrufs unterbrochen wurde. Wenn ein Polygon gefunden wurde, das sich vor dem aktuellen clippenden Polygon befindet, wird es zunächst im Stapelspeicher gesucht. Falls es dort schon eingetragen wurde, ist keine Rekursion nötig, da alle Polygonteile innerhalb und hinter diesem Polygon bereits entfernt wurden.\n\n"}
{"id": "3388197", "url": "https://de.wikipedia.org/wiki?curid=3388197", "title": "Haloed-Line-Algorithmus", "text": "Haloed-Line-Algorithmus\n\nDer Haloed-Line-Algorithmus ist ein Verfahren der Computergrafik, um Drahtgittermodelle oder allgemeine dreidimensionale Linien darzustellen. Die gezeichneten Linien erhalten dabei eine Kontur („Halo“), die dahinterliegende Linien verdeckt. Dadurch wird der Eindruck von Räumlichkeit verstärkt. Wenn die Breite des Halos groß genug gewählt wird, entsteht der Effekt, dass wie bei einer vollständigen Verdeckungsberechnung nur die sichtbaren Flächen angezeigt werden.\n\nDer Haloed-Line-Algorithmus besteht aus einer Vorbereitungs- und einer Anzeigeroutine. Bei der Vorbereitung wird das Bild in ein Gitter eingeteilt, dessen Feinheit von der durchschnittlichen Linienlänge abhängt. Zusätzlich werden für jede Linie die Koeffizienten der entsprechenden Geradengleichung formula_1 gespeichert. Für jede Linie formula_2 werden die Gitterzellen ermittelt, durch die sie läuft. In einer Liste werden zu jeder Zelle die dazugehörigen Linien als formula_3 vermerkt und nach Zelle sortiert.\n\nFür jede Zelle werden hindurchlaufende Linien formula_2 und formula_5 paarweise geprüft, ob sie einander schneiden. Ist dies der Fall, so wird der Schnittpunkt formula_6 beider Linien ermittelt. Außerdem wird bestimmt, welche der Linien am Schnittpunkt die kleinere \"z\"-Koordinate besitzt, also dem Betrachter näher liegt. Liegt formula_2 näher, so wird der Winkel formula_8 zwischen formula_2 und formula_5 berechnet. Die Ergebnisse werden als formula_11 in einer Tabelle formula_12 gespeichert. Sobald alle Gitterzellen abgearbeitet wurden, wird die \nTabelle formula_12 nach formula_2 sortiert. Linien, die nicht in der Tabelle eingetragen sind, schneiden keine anderen Linien und sind somit stets sichtbar; sie werden ebenfalls in die Tabelle eingetragen.\n\nUm die Linien anzuzeigen, wird die Tabelle formula_12 Eintrag für Eintrag durchgegangen. Für jeden Eintrag werden mittels formula_16 und formula_8 die Punkte formula_18 und formula_19 berechnet, an denen das Halo auf der Linie um den Schnittpunkt herum aufhört und wieder anfängt. Die Paare formula_20 und formula_21 werden in einer Tabelle formula_22 gespeichert. Zusätzlich werden die Paare formula_23 und formula_24 gespeichert, wobei formula_25 und formula_26 die Endpunkte der Linie sind.\n\nDie so entstandene Tabelle formula_22 wird nun nach formula_2 sortiert und der Reihe nach durchgegangen, wobei die jeweiligen Werte +1 oder −1 summiert werden. Wenn die Summe 1 beträgt, wird angefangen, die Linie zu zeichnen, wenn sie einen Wert ≤0 erreicht, wird die Zeichnung der Linie wieder gestoppt.\n\nDiese Prozedur ist beendet, wenn alle Einträge der Tabelle formula_12 abgearbeitet wurden.\n\n"}
{"id": "3389269", "url": "https://de.wikipedia.org/wiki?curid=3389269", "title": "Ark (Packprogramm)", "text": "Ark (Packprogramm)\n\nArk, ehemals \"KZip\", ist ein freies Packprogramm des K Desktop Environments (Teil des Paketes \"kdeutils\") zum grafischen Verwalten von Archivdateien einer Vielzahl von Formaten. Es stellt ein grafisches Frontend zu verschiedenen Datenkompressions- und Packprogramme für die Kommandozeile (Backend) dar. Das Programm ist freie Software unter der GNU General Public License (GPL).\nDas Programm erschien ursprünglich als KZip. Am 3. November 1998 erschien Ark 0.5 als erste Version unter dem neuen Namen. Damals noch integriert mit dem KDE file manager, KFM, was sich am 23. Oktober 2000 mit dem Erscheinen von Version 2 von KDE änderte, in welcher Konqueror als Dateimanager eingeführt wurde.\n\nArk kann den Inhalt von Archiven anzeigen, Dateien daraus entpacken, löschen oder welche hinzufügen sowie auch neue Archive erstellen. Die Funktionen stehen über das Hauptfenster über dessen Schaltflächen und Menüs sowie per Drag and Drop zur Verfügung oder können auch aus anderen KDE-Programmen wie Konqueror oder Dolphin heraus genutzt werden, in die es als KPart eingebunden werden kann. So können zum Beispiel aus Konqueror über das Kontextmenü Dateien gepackt und entpackt werden.\n\nÜber die entsprechenden Kommandozeilenprogramme kann es mit Archivdateien der Formate 7z, ar, RAR, zip, gzip, bzip, bzip2, LHA, zoo und ar umgehen. Die entsprechenden Packprogramme müssen separat installiert werden.\n\n\n"}
{"id": "3390763", "url": "https://de.wikipedia.org/wiki?curid=3390763", "title": "Roberts-Algorithmus", "text": "Roberts-Algorithmus\n\nDer Roberts-Algorithmus ist ein Verfahren aus der Computergrafik zur Verdeckungsberechnung von Polyedern. Er wurde 1963 veröffentlicht und ist damit der älteste Algorithmus zur Verdeckungsberechnung.\n\nDer Roberts-Algorithmus verarbeitet Polygonkanten und setzt voraus, dass diese zu konvexen Polyedern (Polygonnetzen) gehören. Konkave Körper müssen erst in mehrere konvexe aufgeteilt werden.\n\nDer Roberts-Algorithmus wendet zunächst Backface Culling an, um zu nicht sichtbaren Polygonen gehörende Kanten zu entfernen. Anschließend wird jede verbleibende Kante nach und nach gegen jedes Polyeder, das sie verdecken könnte, getestet. Durch einfache Vergleiche der Koordinaten lassen sich viele Kanten trivial eliminieren.\n\nBeim Vergleich einer Kante mit der durch einen Polyeder aufgespannten Fläche könnten drei Fälle auftreten:\n\nDer Sichtbarkeitstest des Roberts-Algorithmus verwendet lineare Optimierung, um die Werte der Geradengleichung zu ermitteln, für die der Projektionsstrahl durch ein Polyeder verläuft und somit der entsprechende Punkt verdeckt ist.\n\nDa jede Kante gegen jedes Polyeder getestet wird, hat der Roberts-Algorithmus theoretisch quadratische Laufzeit. Dies und das größere Interesse an bildpräzisen Verfahren zur Verdeckungsberechnung führte dazu, dass der Roberts-Algorithmus wenig beachtet wurde. Er lässt sich jedoch durch eine vorausgehende Sortierung nach den \"z\"-Koordinaten und einfache Bounding-Volume-Tests so verbessern, dass er eine nahezu lineare Laufzeit aufweist.\n\n"}
{"id": "3392852", "url": "https://de.wikipedia.org/wiki?curid=3392852", "title": "Horton hört ein Hu!", "text": "Horton hört ein Hu!\n\nHorton hört ein Hu! (Originaltitel: \"Horton hears a Who!\") aus dem Jahr 2008 ist der vierte Computeranimationsfilm der Blue Sky Studios in Spielfilmlänge. Regie führten Jimmy Hayward und Steve Martino. Der Film basiert auf dem gleichnamigen Kinderbuch von Dr. Seuss aus dem Jahr 1954, welches in einer halbstündigen Zeichentrickfassung (in Deutschland unter dem Titel \"„Das sprechende Staubkorn“\", der Originaltitel war gleich) bereits 1970 im Fernsehen ausgestrahlt wurde. Nach \"Der Grinch\" aus dem Jahr 2000 und \"Ein Kater macht Theater\" von 2003 ist dies die dritte Adaption eines Buches von Dr. Seuss als Spielfilm. Dies ist jedoch die erste Umsetzung eines Werkes von Dr. Seuss mittels Computeranimation.\n\nIm Dschungel des Landes \"Nümpels\" (im Original: \"Nool\") nimmt der fürsorgliche und verträumte Elefant Horton ein Bad in einem Teich. Zufällig schwebt ein Staubkorn an ihm vorbei, und er hört ein leises Rufen. Im Glauben, es könnten mikroskopisch kleine Leute auf dem Staubkorn leben, fängt er das Staubkorn mit der Blüte einer Kleepflanze auf, die er im Rüssel hält.\n\nHorton findet heraus, dass das Staubkorn die Stadt \"Hu-Heim\" (\"Whoville\") beheimatet, geführt von ihrem Bürgermeister Ned McDodd. Ned hat eine Ehefrau, 96 Töchter und einen halbwüchsigen Sohn namens Jo-Jo. Die Tradition sieht vor, dass der älteste Sohn des Bürgermeisters sein Amt übernehmen soll. Ned schenkt Jo-Jo daher besondere Aufmerksamkeit, während dieser eher lustlos auf seine vorgezeichnete Zukunft blickt.\n\nHorton und der Bürgermeister sind sich einig, dass Hu-Heim eine sichere und stabile Umgebung benötigt. So beschließt Horton, das Staubkorn an die Spitze des Berges \"Nümpelsberg\" (\"Mount Nool\") zu bringen, dem sichersten Ort im Dschungel. Als Horton aber den anderen Bewohnern des Dschungels von Hu-Heim erzählt, erntet er nichts als Spott und Gelächter für seine vermeintliche Spinnerei. Die Wortführerin der Dschungelmeute, eine überhebliche Kängurumutter, versucht gar, Horton das Staubkorn abzunehmen, damit „solche lächerlichen Ideen nicht in die Köpfe der Kinder gelangen“. Das gar nicht mehr so kleine Kängurujunge findet ihr Verhalten jedoch eher peinlich.\n\nHorton will sich von seinem Vorhaben nicht abbringen lassen und wird daher von den anderen in einen Holzkäfig gesperrt. Das Staubkörnchen wollen sie in einem Kessel voll kochendem \"Bieselnuss-Öl\" (\"Beezlenut-Oil\") verbrennen. Jetzt schlägt in Hu-Heim Jo-Jos große Stunde: Mit seinem Ideenreichtum und seiner Einsatzbereitschaft schaffen er und die anderen Hu-Heimer es, mit viel Krach und Musik auch die restlichen Dschungelbewohner von ihrer Existenz zu überzeugen. Das kleine Känguru löst sich von seiner Mutter und rettet Blume samt Staubkorn, und die Dschungelmeute sieht ein, dass Hu-Heim auf dem Staubkorn geschützt werden muss. Sogar das herrische Känguru hat ein Einsehen und der Film endet mit einer Freudenfeier.\n\nDie deutsche Synchronisation entstand nach einem Dialogbuch von Oliver Rohrbeck unter seiner Dialogregie im Auftrag der Berliner Synchron AG.\nZum Ende des Films singen die Protagonisten das Lied \"Can’t Fight This Feeling\". In der deutschen Synchronisation wird anstelle des Originaltexts eine deutsche Version gesungen.\n\nDie Produktionskosten wurden auf 85 Millionen US-Dollar geschätzt. Der Film spielte in den Kinos weltweit rund 297 Millionen US-Dollar ein, davon 154 Millionen US-Dollar in den USA und 14 Millionen US-Dollar in Deutschland.\n\n\n"}
{"id": "3404158", "url": "https://de.wikipedia.org/wiki?curid=3404158", "title": "Cobold", "text": "Cobold\n\nCOBOLD war der Name eines Kleincomputers, der im März 1983 von der westdeutschen Zeitschrift Elrad vorgestellt wurde. Er wurde von einer Artikelserie als Bauanleitung (Ausgaben 3, 4 und 5 1983) begleitet und war zudem der Grundstein für eine Reihe weiterer, zumeist spezialisierter Kleincomputer, wie dem CEPAC 65 (CMOS-Version, nur eine Platine) und dem SET-65 (Adapter mit EPROM-Brenner und Tastatur/Display für CEPAC-65), die sich vor allem für Steuerungs- und Entwicklungszwecke eigneten. Die Artikel und das Handbuch wurden von Christian Persson geschrieben.\nDie 1980er Jahre waren eine Zeit des Wildwuchses der Homecomputer. Es gab teils große Preis- und Leistungsunterschiede und fast keine Standards. Daher gab es einige solcher Anleitungen, mit denen sich der Hobbyist einen Computer günstig selbst bauen und erweitern konnte und mit diesem die Grundlagen der Programmierung lernte. Denn ein Standard-PC, zudem noch ein Markengerät, war für Privatleute nahezu unerschwinglich. \n\nZum COBOLD gab es, im Gegensatz zu vielen anderen, ein Handbuch, das zugleich Dokumentation und Lehrbuch war. \n\nIn der kleinsten Ausbaustufe war er bereits in der Lage, Programme auf einem Kassettenrecorder zu speichern, über ein serielles Interface und ein Terminal einen Bildschirm-Editor anzubieten, sowie über freie Portleitungen externe Hardware anzusteuern. Dies beinhaltete 2 KByte Ram und 16 I/O-Leitungen.\n\nDie notwendige Busplatine und die Menge von max. 48 I/O-Ports machten den COBOLD damals zu einem flexibel einsetzbaren Kleincomputer für universellen Einsatz. Danach erschienen in der ELRAD einige Artikel (genannt COBOLD-Bits) in der Rubrik Computing Today, aus der später die Zeitschrift c't hervorging.\n\nDer Computer bestand aus insgesamt drei Platinen im Europakarten-Format (160*100 mm):\n\nEin Gehäuse war nicht vorgesehen.\n\nDie Prozessorplatine war eigentlich bereits der komplette Computer. Sie hatte Platz für einen Prozessor vom Typ 6502 in der NMOS- oder CMOS-Variante, der dann auch im Commodore VC20 und Atari 800 XL Verwendung fand und als 6510 im bekannten C-64. Der Systemtakt betrug 1 MHz, konnte aber bis 4 MHz (mit passenden Bauteilen) erreichen. Weiter waren in der Grundausbaustufe ein statisches RAM (SRAM) von 2 KByte Größe und ein ROM von 4 KByte Größe mit dem Betriebssystem vorhanden. Dazu gehörte einer von bis zu drei Ein-/Ausgabe-Bausteinen vom Typ 6532. Dieser RIOT genannte Baustein hatte 128 Byte RAM, 16 I/O-Ports und mehrere Timer. Als Adressdekoder diente (im Gegensatz zu seinen Zeitgenossen) hier ein PROM, mit dem die Chip-Select-Signale erzeugt wurden. Ein Taktgenerator (s. o.) durfte natürlich nicht fehlen, und über Steckfelder war eine Anpassung an verschiedene RAM/ROM-Typen möglich. Die Platine hatte eine 44-polige Stiftleiste für den Prozessorbus und eine 64-polige Steckleiste (nach DIN 41612 Typ C) zur Verbindung mit der Busplatine, über die alle Ports herausgeführt sind. \n\nDie Busplatine enthielt bereits Lochungen für 5 Porterweiterungen mit 64-poligen Steckleisten. Allerdings musste nur eine bestückt werden für die Prozessorplatine. Weiter waren an einem Rand ein Spannungsregler, ein Kassettenrecorder-Interface zur Datenspeicherung und ein serieller Anschluss für ein Terminal vorhanden.\n\nDie Tastatur-Display-Karte enthielt zwei Blöcke an Eingabetasten, zwei Blöcke LED-Sieben-Segment-Displays sowie zwei Kippschalter.\n\nDer Displayteil war unterteilt in eine vierstellige Adressanzeige und eine zweistellige Datenanzeige. Rechts daneben befand sich ein Kippschalter zur Abschaltung der Anzeigen und einer für den Einzelschrittmodus. Darunter befanden sich 16 Datentasten für die Eingabe von hexadezimalen Werten und rechts ein Block mit Steuertasten.\n\n"}
{"id": "3404525", "url": "https://de.wikipedia.org/wiki?curid=3404525", "title": "FreeOTFE", "text": "FreeOTFE\n\nFreeOTFE ist eine 2004 erschienene Open-Source-Software zur Verschlüsselung von Festplatten unter Microsoft Windows (PCs) und Windows Mobile (PDAs).\n\nDie (ehemalige) Website des Projekts, FreeOTFE.org, ist seit 2013 nicht mehr zugänglich und wurde auf einen anderen Besitzer umgemeldet.\n\nFreeOTFE ist eine Umsetzung von OTFE und ist mit unter Linux erstellten verschlüsselten Volumes (z. B. LUKS, cryptoloop dm-crypt) kompatibel. Diese Volumes könnten unter Windows gelesen und beschrieben werden. Es können versteckte Volumes angelegt und geöffnet werden. Das ermöglicht eine glaubhafte Abstreitbarkeit (engl. \"\").\n\nDas Programm bietet einen „Portable mode“. Es muss vor der Benutzung nicht installiert werden, jedoch werden unter Windows Administrator-Rechte zur Einbindung der Treiber benötigt. Die Programmvariante \"FreeOTFE Explorer\" benötigt allerdings keine Administrator-Rechte.\n\n\n\nIn der Vergangenheit versagte die Lizenz von FreeOTFE dem Autor von Modifikationen jegliche Rechte an seinen Änderungen, da diese gemeinfrei veröffentlicht werden mussten, was dem Punkt 3 der Open Source Definition widersprach. Anfang 2009 fand jedoch ein Lizenzwechsel statt. Die neue Lizenz, welche unter dem Namen „“ veröffentlicht wurde, ähnelt inhaltlich der Reciprocal Public License, welche zwar als Open Source anerkannt ist, jedoch nicht von der Free Software Foundation.\n"}
{"id": "3407449", "url": "https://de.wikipedia.org/wiki?curid=3407449", "title": "Kerkythea", "text": "Kerkythea\n\nKerkythea ist eine freeware Standalone-Renderengine, die Raytracing und globale Beleuchtung unterstützt. Sie arbeitet mit physikalisch korrekten Materialien und Lichtern.\nDas Ziel von Kerkythea ist es, den Prozess des Qualitäts-Renderings zu vereinfachen, indem die nötigen Tools für automatisierte Szenen-Konfiguration wie Render-Einstellungs-Editor, Material-Editor und Echtzeit-3D-Bearbeitung unter einem Interface vereint werden.\n\nDie Entwicklung der Kerkythea-Anwendung begann im September 2004, wenn auch damals noch eher wenig Zeit darin investiert wurde. Hierbei wurde auf dem Kern und den Programmbibliotheken von Phos, einer älteren Renderengine, aufgebaut, sodass ein großer Teil des Codes bereits getestet und gut strukturiert war.\n\nEtwas mehr als ein halbes Jahr später, im April 2005, war die erste Version von Kerkythea fertiggestellt. Damals gab nur es eine Windows-Version, im Oktober wurde die erste Linux-Version veröffentlicht.\n\nHauptentwickler der Software ist Ioannis Pantazopoulos.\n\nEs existieren fünf offizielle Exportmodule für Kerkythea:\n\nAußerdem lassen sich alle \"Obj-\" und \"3ds\"-Dateien in Kerkythea importieren.\n\n"}
{"id": "3407946", "url": "https://de.wikipedia.org/wiki?curid=3407946", "title": "Linux Documentation Project", "text": "Linux Documentation Project\n\nThe Linux Documentation Project (TLDP) ist ein vollständig ehrenamtlich betriebenes Projekt, das eine große Sammlung Linux-bezogener Dokumentation unterhält und online veröffentlicht. Es wurde als Dokumentations-Austauschplattform zwischen Linux-Hackern und den Benutzern ihrer Programme ins Leben gerufen. Die Dokumente richten sich zumeist an erfahrene Benutzer (z. B. professionelle Systemadministratoren), aber es findet sich auch Lehrmaterial für Einsteiger darunter.\n\nIn den Ursprüngen von 1992 war das LDP eine FTP-Seite, aber MetaLab übertrug es 1993 in das World Wide Web. Es könnte sich dabei um die erste linuxbezogene Webseite überhaupt handeln.\n\nHeute stellt das LDP 475 Dokumente bereit, die von noch mehr Autoren verfasst werden. Ein gutes Dutzend der Dokumente umfassen Buchlänge und die meisten davon werden tatsächlich von hauptsächlich technischen Verlagen (z. B. O’Reilly) als Druckwerke bereitgestellt.\n\nDas LDP veröffentlicht schwerpunktmäßig Howto-Dokumente, die dem Benutzer die diversen Lernziele Schritt für Schritt nahebringen. Die Lernziele sind gelegentlich sehr spezifisch (z. B. wie man ein bestimmtes Modem konfiguriert), in anderen Fällen sehr umfassend (z. B. wie man das Netzwerk eines Internet-Dienstanbieters administriert).\n\nBesonders umfassende Themen werden von Handbüchern abgedeckt, buchlange Dokumente bezüglich allgemeiner Themen, z. B. Sicherheit oder Netzwerktechnik.\n\nDas LDP veröffentlicht außerdem FAQ-Listen, Manpages und andere Dokumente, sowie zwei Webzines: Die Linux-Gazette und den Linux-Fokus.\n\nEin großer Teil der LDP-Sammlung ist unter der GNU-Lizenz für freie Dokumentation (GFDL) lizenziert. Viele andere Lizenzen werden ebenfalls genutzt, solange diese eine freie Verbreitung gestatten. Die gegenwärtige Projekt-Marschroute empfiehlt die GFDL oder die Open Publication License ohne die Exercising-Möglichkeiten A oder B. \n\nDer \"Linux Network Administrators’ Guide\" ist ein Buch dieser Reihe.\n\n"}
{"id": "3409392", "url": "https://de.wikipedia.org/wiki?curid=3409392", "title": "Yacas", "text": "Yacas\n\nYacas ist ein vielseitig nutzbares Computeralgebrasystem. Der Name ist ein Akronym für \"Yet Another Computer Algebra System\". Yacas ist freie Software und als solche unter der GPL veröffentlicht.\n\nYacas liefert ein System zur symbolischen Berechnung von Ausdrücken und bringt eine eigene Programmiersprache mit, in der neue Algorithmen entwickelt werden können, aber auch bereits vorhandene genutzt werden können. Als Ein- und Ausgabe kann ASCII oder OpenMath gewählt werden, aber es gibt auch Befehle zum Exportieren nach LaTeX. Zur grafischen Darstellung von Funktionen, sowohl in 2D wie auch in 3D, wird Gnuplot verwendet.\n\nSowohl Yacas als auch Gnuplot sind für viele verschiedene Betriebssysteme erhältlich. Sie sind auf der Homepage zusammen mit ausführlicher Dokumentation verfügbar, wo man das System auch online ausprobieren kann.\n\nDie Software könne zwar nicht mit den kommerziellen Computeralgebrasystemen mithalten, würde aber einen guten Einblick in die Funktionsweise dieser Systeme gewähren, so Harald Bögeholz im „Themen-Special: Wissenschaftliche Software“ des Computermagazins c't.\n\n"}
{"id": "3409659", "url": "https://de.wikipedia.org/wiki?curid=3409659", "title": "Multiplizierer (Digitaltechnik)", "text": "Multiplizierer (Digitaltechnik)\n\nEin Multiplizierer ist in der Digitaltechnik eine elektrische Schaltung, die aus zwei oder mehr digitalen Zahlen mit der mathematischen Operation der Multiplikation das Produkt ermittelt. Der Multiplizierer ist bei Prozessoren Teil der arithmetisch-logischen Einheit (ALU) und kommt dort als Multiplikationsakkumulator (MAC) vor, kann aber in programmierbaren digitalen Schaltungen wie FPGAs auch als eine eigenständige Funktionseinheit realisiert werden.\n\nNeben der Addition, welche in digitalen Schaltungen mit geringerem schaltungstechnischem Aufwand in Form von Addierwerken realisiert ist, ist eine schnelle, hardwarebasierende Multiplikation insbesondere im Bereich der digitalen Signalverarbeitung wesentlich. Anwendungsgebiete des Multiplizierers liegen daher bei der Signalverarbeitung wie der Bildverarbeitung oder im Bereich digitaler Filter. Er findet aber auch Anwendung in der digitalen Regelungstechnik. Einer der ersten Einsatzbereiche waren digitale Signalprozessoren (DSP). \n\nMultiplizierer werden aufgrund der verarbeiteten Zahlenformate unterschieden:\nDer Aufwand ist im Wesentlichen durch die Anzahl der zu multiplierenden Bits (steigt quadratisch) bestimmt. Gleitkomma-Multiplizieren benötigen noch etwas zusätzliche Logik:\nBei Verwendung von parallelen Multiplizieren in modernen CPUs und Grafikkarten liegt der Hauptaufwand (double: ca. 97 Prozent, float: ca. 92 Prozent) im Multipliziernetzwerk.\n\nDie binäre Multiplikation verläuft analog wie im dezimalen System, und kann in digitalen Schaltungen als eine Abfolge von Additionen und Schieboperationen realisiert werden. In nebenstehender Schaltung ist ein vorzeichenloser, paralleler Multiplizierer (MAC) für zwei je vier Bit breite Zahlen \"X\" und \"Y\" und dem Summanden \"K\" mit Volladdierern dargestellt. Die acht Ausgabebits \"P\" werden in der kombinatorischen Logik mit folgender Gleichung gebildet:\n\nDer Vorgang der Multiplikation gestaltet sich dabei nach folgendem Schema, die vier Eingangsbits \"K\" sind der Einfachheit wegen auf 0 gesetzt:\n\nDieser einfache Multiplizierer lässt sich aus einzelnen Volladdierern und die Schiebeoperation durch direkte Verschaltung realisieren. Die binären Stellen des Produktes \"P\" sind gleich der Summe der Stellen der beiden Faktoren \"X\" und \"Y\". Ein in der Position fixer Kommapunkt wird generell nicht schaltungstechnisch abgebildet, sondern die Position des Kommapunktes im Produkt ergibt sich aus der Summe der Stellen nach dem Komma der beiden Eingangsfaktoren. Im obigen Beispiel ist bei beiden Faktoren die Stellenanzahl hinter dem Komma null, wodurch auch im Produkt der Kommapunkt rechts der letzten Stelle zu liegen kommt.\n\nBei vorzeichenbehafteten Zahlen, welche in digitalen Schaltungen meistens als Zweierkomplement dargestellt sind, ist eine entsprechende schaltungstechnische Erweiterung des Multiplizierers nötig. Die vorzeichenrichtige Multiplikation von zwei je vierstelligen binären Zahlen gestaltet sich nach folgendem Schema, wobei zu beachten ist, dass die letzte Zeile aufgrund des negativen Faktors subtrahiert werden muss:\n\nSchaltungstechnisch kann die Subtraktion in der letzten Zeile durch erweiterte Volladdierer realisiert werden, welche neben der Addition auch die Subtraktion beherrschen.\n\nDer Parallelmultiplizierer, bei dem die Rechengeschwindigkeit nur von der maximalen Gatterlaufzeit abhängt, ist allerdings für größere Bitbreiten aufwendig zu realisieren. Ein anderes Verfahren ist der serielle Multiplizierer, bei welchem zu Lasten des Durchsatzes mit geringerem Hardwareaufwand pro Takt ein Bit (eine Stelle) des Ergebnisses berechnet wird. Des Weiteren existieren Verfahren, welche auf Tabellen mit bereits vorab berechneten Werten (engl. \"Look-Up-Tables\") basieren. Es existieren auch effiziente Algorithmen, mit denen sich schnelle Multiplizierer mit moderatem schaltungstechnischen Aufwand realisieren lassen. Beispiele hierfür sind der Wallace-Tree-Multiplizierer oder der Booth-Algorithmus.\n\nEine beliebige Gleitkommazahl \"x\" wird aus dem Vorzeichenbit \"s\" (±1), der Mantisse \"m\" und einem Exponenten \"e\", mit einer willkürlich gewählten und fixen Basis \"b\", gebildet:\n\nBei der in der Computertechnik üblichen Norm IEEE 754 zur Darstellung von Gleitkommazahlen wird mit einer Basis \"b\" = 2 und je nach benötigter Auflösung verschieden breiten Mantissen und Exponenten gearbeitet. \n\nDie Multiplikation zweier Gleitkommazahlen lässt sich immer auf eine Multiplikation der beiden Mantissen und eine Addition der beiden Exponenten mit Festkommaarithmetik zurückführen. Die Addition und Multiplikation kann aus Geschwindigkeitsgründen parallel mit getrennten Schaltungsteilen erfolgen. \nDie einzelnen Schritte\nDie folgenden Schritte können parallel ausgeführt werden:\nNun folgt:\nFallunterscheidung:\nZusätzlich existieren bei Gleitkommamultiplizierern noch Steuerlogiken, welche das korrekte Vorzeichen des Ergebnisses bilden und bestimmte Sonderfälle des IEEE-754-Gleitkommaformates, wie „Keine Zahl“ (\"NaN\", engl. für \"Not-A-Number\"), behandeln.\n\nAlle Logikbauelemente besitzen eine Signallaufzeit. Mit zunehmender Bitbreite des Multiplizierers nimmt die Anzahl der Logikbauelemente, die parallel und/oder in Reihe geschaltet sind, zu. Mit zunehmender Anzahl von Logikbauelementen steigt die gesamte Signallaufzeit des Multiplizierers.\n\n"}
{"id": "3411919", "url": "https://de.wikipedia.org/wiki?curid=3411919", "title": "KDE Software Compilation 4", "text": "KDE Software Compilation 4\n\nKDE Software Compilation 4 (kurz \"KDE SC 4\") ist die vierte Generation der frei verfügbaren Softwaresammlung von \"KDE\", welche vor allem aus den \"KDE Plasma Workspaces\" für Netbooks und PCs sowie einigen Einzelanwendungen besteht und neben Deutsch in vielen anderen Sprachen verfügbar ist.\n\nDie \"KDE Software Compilation\" ist vorrangig für Rechner gedacht, auf denen ein unixähnliches Betriebssystem läuft. Ab der Version 4.1 kann sie auch direkt unter Windows oder macOS genutzt werden.\n\nIm Gegensatz zu den vorherigen Generationen erlaubt das flexible Design des technischen Fundaments, insbesondere Plasma, die Anpassung an unterschiedlichste Geräte-Typen und -Formfaktoren. Das KDE-Projekt nennt die daraus resultierenden unterschiedlichen Oberflächen \"Workspaces\". Derzeit werden offiziell Oberflächen für Desktops, Netbooks, Tablets und Media Center angeboten. Weitere, z. B. für Smartphones, sind in Arbeit.\n\nDie \"Software Compilation\" ist ein sehr großes Programmpaket, das viele einzelne Anwendungen enthält. Essenzielle Anwendungen sind:\n\nNicht Teil der \"Software Compilation\" sind Calligra Suite und Extragear-Anwendungen wie z. B. Amarok und K3b.\n\nAm 18. August 2006 veröffentlichte das KDE-Team die erste Vorabversion von KDE 4.0 mit dem Namen \"Krash\" unter der Versionsnummer 3.80.1, der zwei weitere Schnappschüsse folgten. Diese drei Versionen richteten sich in erster Linie an Entwickler, die ihre Anwendungen auf KDE 4 portieren oder neue Anwendungen für KDE 4 schreiben wollen. Für Anwender sichtbare Änderungen gegenüber KDE 3.5 gab es in diesen Versionen noch kaum.\n\nAm 11. Mai 2007 wurde KDE 4 in einer ersten Alpha-Version veröffentlicht. Erstmals zeigte KDE 4 auch optisch deutliche Unterschiede zu KDE 3, da beispielsweise der \"Oxygen\"-Iconsatz eingebunden wurde. Die erste von vier Beta-Versionen erschien am 2. August 2007. Am 20. November 2007 erfolgte die Freigabe des ersten von zwei Release Candidates (RC). Am 11. Januar 2008 wurde schließlich das finale KDE 4.0 veröffentlicht.\n\nKDE 4.0 basiert auf Qt 4.3. Da es auch weitere grundlegende Änderungen mit sich bringt, ist es binär inkompatibel zu KDE 3.x.\n\nDie an der Oberfläche sichtbaren Änderungen wurden hauptsächlich im Rahmen der Projekte \"Oxygen\" und \"Plasma\" entwickelt. Ersteres kümmert sich insbesondere um die Schaffung des Artworks von KDE 4, also um ein neues Icon- sowie Widget-Thema und ähnliches. Plasma ersetzt Programme wie \"kicker\" (Taskleiste) und \"kdesktop\" (Desktop) und gestaltet dadurch die Oberfläche einheitlicher.\n\nWeiterhin wurde die Multimedia-API \"Phonon\" entwickelt, welche den alten Soundserver aRts ablöst. Phonon stellt vergleichbar mit DirectShow unter Windows und \"CoreAudio\" unter Mac OS X eine einheitliche API zur Entwicklung von Audio- und Video-Anwendungen bereit. Es selbst ist jedoch kein Soundserver wie aRts, sondern nur ein Wrapper, der die Benutzung von verschiedenen Back-Ends ermöglicht.\n\nDas Projekt \"Solid\" dient dazu, den Benutzern den Umgang mit der Computer-Hardware zu erleichtern, insbesondere mit solcher, die während des Betriebs ein- und ausgesteckt werden kann. Eine weitere Intention von Solid ist es, KDE plattformunabhängiger zu gestalten: Um auch native Versionen von KDE für andere Betriebssysteme, wie beispielsweise Windows, zu veröffentlichen, ist eine Abstraktionsschicht nötig, die es Anwendungen ermöglicht, die Hardware unter allen Betriebssystemen gleich anzusteuern. Ohne diese Schicht müsste jede Anwendung für verschiedene Betriebssysteme angepasst werden, was ein sehr großer Aufwand wäre und deshalb nur einmal – für alle anderen Applikationen benutzbar – in Solid implementiert wurde.\n\nAußerdem verwendet KDE 4 zur Kommunikation zwischen den Anwendungen nicht mehr DCOP (Desktop Communication Protocol), sondern das vom freedesktop.org-Projekt entwickelte D-Bus. Weitere Neuerungen sind der Dateibrowser Dolphin und der universale Dokumentenbetrachter Okular, der die bisherigen Einzelprogramme KPDF, KGhostview, KView, KFax und KDVI ersetzt.\n\nEbenfalls wurde die Rechtschreibprüfung KSpell2 durch die Neuentwicklung Sonnet ersetzt. Sonnet verfügt neben der Rechtschreibprüfung auch über eine Grammatikprüfung, Übersetzungsfunktionen und die Erkennung der Sprache eines geschriebenen Textes. Das Team um Sonnet arbeitete dabei eng mit dem AbiWord-Team zusammen und nutzt unter anderem deren Schnittstelle zum Zugriff auf verschiedene Rechtschreibprüfungs-Systeme.\n\nDer Fenstermanager KWin von KDE 4 stellt nun auch sogenannte \"Compositing-Funktionen\" auf der Basis von OpenGL oder XRender zur Verfügung. Effekte wie Fensterschatten und Fenstertransparenzen sind in vorhergehenden Versionen in erster Linie durch externe Projekte wie Compiz realisiert worden.\n\nKDE 4.1 wurde am 29. Juli 2008 veröffentlicht. Seit KDE 4.1 steht auch die integrierte PIM-Suite \"Kontact\" und ein neues Videoabspielprogramm, Dragon Player, zur Verfügung.\n\nKDE 4.2 wurde am 27. Januar 2009 veröffentlicht. Diese Veröffentlichung war die erste der aktuellen KDE-Generation, die für normale Nutzer ausgelegt war.\n\nSpeziell die Plasma-Oberfläche erfuhr in dieser Version weitreichende Änderungen: Der Desktop konnte auf eine traditionelle Ansicht umgestellt werden, Seitenleisten konnten versteckt werden usw.\n\nEine ganze Reihe weiterer neuer Funktionen wurden mitgeliefert, u. a.: PowerDevil (stellt Funktionen zur Energieeinsparung bereit), andere neue Hilfsprogramme, neue Spiele und Feinschliff an den bekannten Anwendungen wie z. B. Dolphin.\n\nKDE 4.3 wurde am 4. August 2009 veröffentlicht. Feinschliff der vorhandenen Komponenten stand bei dieser Version im Vordergrund und weniger das Einführen neuer Funktionen.\nDie wenigen neuen Funktionen sind u. a. die Integration von PolicyKit, NetworkManager und Geolocation, sowie eine Plasma-Komponente zur Einbindung eines sozialen Netzwerks.\n\nKurz vor Veröffentlichung der ersten Beta-Version wurde bekannt gegeben, dass sich das KDE-Projekt auf eine neue Markenstrategie geeinigt hat. Fortan ist „KDE“ nur noch Name für das Projekt selbst und das Programm-Paket wird „Software Compilation“ (\"SC\") genannt.\n\nDie Software Compilation 4.4 wurde am 9. Februar 2010 veröffentlicht. Einige der neuen Funktionen sind:\n\nDie Software Compilation 4.5 wurde am 10. August 2010 veröffentlicht. Fokus dieser SC-Generation liegt auf dem Feinschliff vorhandener Features, wenngleich auch einige Neue hinzukamen.\n\nUrsprünglich geplant war, die Akonadi-Portierung von KMail und den anderen Kontact-Anwendungen abzuschließen und mit der SC 4.5 zu veröffentlichen. Dieser Plan musste aufgrund von Verzögerungen jedoch verworfen werden, was dazu führte, dass keine 4.5-Versionen dieser Anwendungen erschienen. Die 4.4-Versionen erhalten bis zur Veröffentlichung von SC 4.6 Fehlerkorrekturen.\n\nVersion 4.6.0 der Programmsammlung wurde am 26. Januar 2011 veröffentlicht. Die Entwicklung von Features wurde für den 4.6-Zyklus am 12. November 2010 abgeschlossen. Besonderes Augenmerk liegt in dieser Version auf der Verbesserung der Geschwindigkeit, so werden u. a. einige bisher nur über den Prozessor berechnete Effekte (Programmstartanzeige, Fenstergeometrieanzeige) ebenfalls grafikbeschleunigt mittels OpenGL gerendert.\n\n\n\nKDE SC 4.7.0 wurde am 27. Juli 2011 veröffentlicht.\nNeuigkeiten von 4.7 sind unter anderem die Unterstützung von OpenGL ES im Fenstermanager KWin, um diesen auch auf mobilen Plattformen und unter Wayland einsetzen zu können. Außerdem wurden die Plasma Activities überarbeitet und erweitert, der Dateimanager Dolphin erhielt ein aufgeräumteres Benutzerinterface und Marble-Unterstützung für Sprachausgabe.\n\nKDE SC 4.8.0 wurde am 25. Januar 2012 veröffentlicht.\nUnter anderem wurde das Power-Management verbessert, das nun vorgefertigte Profile bereitstellt. KWin und Dolphin wurden in Sachen Geschwindigkeit und Stabilität optimiert.\n\nKDE SC 4.9.0 wurde am 1. August 2012 veröffentlicht.\nNeben zahlreichen Bugfixes wurden u. a. die Aktivitäten verbessert, weitere Plasmoids wurden nach QML portiert und in Okular erstellte Kommentare können nun im PDF-Dokument gespeichert werden.\n\nKDE SC 4.10.0 wurde am 6. Februar 2013 veröffentlicht.\nNeben zahlreichen Bugfixes wurden u. a. der Datei-Indexer neu geschrieben und ersetzt den Indexer Strigi. Ebenfalls neu geschrieben wurde der Druckmanager.\n\nKDE SC 4.11.0 wurde am 14. August 2013 veröffentlicht.\nNeben zahlreichen Bugfixes wurden u. a. die experimentelle Wayland-Unterstützung hinzugefügt und die Kontact-Anwendungen um neue Funktionen erweitert. Neu hinzugekommen ist ebenfalls die Unterstützung für KScreen, welches der Einstellung der Monitorkonfiguration dient.\n\nDie Workspaces sollen zwei Jahre Support erhalten, um den Umstieg auf Qt 5 vorzubereiten.\n\nKDE SC 4.12.0 wurde am 18. Dezember 2013 veröffentlicht. Diverse KDE-Anwendungen erhielten eine Fehlerbereinigung. Die PIM-Suite erhielt eine Unterstützung von Sieve-Filtern um E-Mails bereits auf dem Server vorzufiltern, Okular unterstützt EPUB-Videos, Kate erweitert die Autovervollständigung. Die Workspaces hingegen wurden, wie zur Version 4.11 angekündigt, nicht geändert.\n\nDie Version 4.13 wurde am 16. April 2014 fertiggestellt. Die größte Änderung in dieser Version ist die Umstellung der semantischen Suche von Nepomuk auf das ressourcenschonendere Baloo.\n\nDie Version 4.14 wurde am 20. August 2014 freigegeben und stellt voraussichtlich den letzten Versions-Sprung der Version 4 dar.\n\nAufgrund der existierenden zeitlichen Fristen für neue Funktionen können bestimmte Entwicklungen bereits begonnen worden sein, werden aber nicht mit der folgenden \"Software Compilation\" erscheinen, sondern auf nachfolgende Versionen verschoben. Teilweise existieren aber auch sehr konkrete Planungen für Funktionen, die über mehrere Veröffentlichungszyklen Schritt für Schritt umgesetzt werden, weshalb mitunter lange im Voraus gewisse Entwicklungen bekannt sind. Zudem wird über eine Verschlankung diverser Oberflächen diskutiert. Das Menü in einen Knopf zu packen, soll möglicherweise künftig systemweit und einheitlich für jede Anwendung zugreifbar über den Fenstermanager KWin möglich sein.\n\n\nNoch vor der Veröffentlichung von KDE SC 4.9, das am 1. August 2012 veröffentlicht wurde, liefen bereits die Planungen für die nächste Generation von KDE Platform, genannt KDE Frameworks 5 (KF5) an.\n\nDie Klassenbibliothek Qt 5, welche die Basis von KF5 bildet, wurde am 19. Dezember 2012 veröffentlicht. Der Umbruch soll nicht so radikal verlaufen wie der von KDE 3 im Zuge des Umstiegs von Qt 3 zu Qt 4, sondern mehr Evolution statt Revolution sein. Trotzdem sind tiefgreifende Erneuerungen anvisiert:\nZiel ist es, modularer zu werden. Da es bereits verschiedene Programme gibt, die auf mehreren Plattformen wie Windows, Mac und Linux laufen, will man es den Entwicklern für einzelne Programme erleichtern, nur an einzelnen Modulen zu arbeiten. Das Framework wurde am 7. Juli 2014 freigegeben.\n\nDas Fundament der \"Software Compilation\" bildet die \"KDE Platform\". Sie umfasst eine ganze Reihe an Frameworks, so z. B.:\n\nWeitere Plattform-Komponenten sind:\n\nDen \"Plasma Workspaces\" zugrunde liegt auch ein neues GUI-Toolkit, das zwar für viele Techniken auf Qt zugreift, allerdings eigene Bedienelemente (Knöpfe, Scrollleisten usw.) zur Verfügung stellt. Diese Bedienelemente können auch von separaten Programmen eingesetzt werden, wie zum Beispiel bei Amarok ab Version 2.0 der Fall. Auch KDevelop setzt Teile der Plasma-Technik ein.\n\n\"Phonon\" (früher auch \"KDEMM\" genannt) ist die Multimedia-Schnittstelle von KDE. Darüber hinaus verwendet die Firma \"Qt Software\" die Schnittstelle für Multimedia-Funktionen in der Qt-Bibliothek.\n\nSolid ist die einheitliche Schnittstelle und Framework zum Einbinden von Hardware.\n\nSolid wurde eingeführt, um den Umgang mit Hardware zu erleichtern, insbesondere mit im Betrieb wechselnden Geräten (Hotplug), wo es die bisherigen Behelfslösungen für die Verwaltung ablösen soll. Des Weiteren macht diese Abstraktionsschicht Anwendungen, die sie nutzen, extrem flexibel und portabel und vereinfacht ihre Entwicklung; so kann die Hardware auch unter verschiedenen Betriebssystemen einheitlich angesteuert werden, was wesentlich zur Plattformunabhängigkeit von KDE SC beiträgt.\n\nÄhnlich der Multimedia-Schnittstelle Phonon verwaltet sie die Hardware nicht direkt selber, sondern macht bestehende Lösungen durch eine einheitliche Schnittstelle zugänglich.\nDabei können alle zugrundeliegenden Teile ausgetauscht werden, ohne eine Anwendung zu stören.\n\nSolid teilt sich in verschiedene Hardware-Bereiche, wie Bluetooth oder Energieverwaltung, die unabhängig voneinander arbeiten und zu denen nach Bedarf weitere hinzugefügt werden können.\n\nDie momentane Lösung benutzt die Hardwareabstraktionsschicht des freedesktop.org-Projektes, den NetworkManager und BlueZ (Linux' offiziellen Bluetooth-Protokollstapel). Es wird auch an einem Back-End für die Windows-Portierung von KDE SC gearbeitet, die auf der Windows Management Instrumentation (WMI) aufbaut.\n\nÜber \"libkdehw\" stellt Solid Informationen über wechselnde Verbindungen und Geräte und zur Energieverwaltung bereit.\n\n"}
{"id": "3412038", "url": "https://de.wikipedia.org/wiki?curid=3412038", "title": "Empathy", "text": "Empathy\n\nEmpathy ( für \"Empathie\", \"Einfühlungsvermögen\") ist ein freier Instant-Messaging-Client. Daneben ist es ein GUI-Toolkit mit einer Reihe wiederverwendbarer Benutzeroberflächenelemente für die Realisierung von Instant-Messaging-Funktionalitäten in Programmen. Empathy wird als Teil des Telepathy-Projektes entwickelt. Er ist der offizielle Instant-Messaging-Client des Gnome-Projekts.\n\nEmpathy bietet als Toolkit Elemente für das Bearbeiten von Benutzerkonten, Kontaktlisten, Privat- und Gruppenchats, das Betrachten von Gesprächsprotokollen und einige andere Funktionen. Es baut auf GTK+ auf und integriert sich in Nokias Kontenverwaltungssystem \"Mission Control\" und Systemdiensten wie dem NetworkManager.\n\nAls Teil der Referenzimplementierung des von freedesktop.org spezifizierten \"Telepathy\" stellt es den Client dar, der im Kern aus der Programmbibliothek \"libempathy\" besteht. Er nutzt die Benutzeroberfläche von \"Gossip\" und andere Teile der Telepathy-Referenzimplementierung, die Verbindungsmanager, um mit verschiedenen Instant-Messaging-Netzwerken zu kommunizieren. Mit Empathy ist keine Form fortschrittlicher Ende-zu-Ende-Verschlüsselung möglich, wie sie viele andere Messenger mittels Off-the-Record Messaging (OTR) ermöglichen. Dagegen ist er neben Pidgin, Gajim und Jitsi einer der wenigen Clients, der Video- und Audio-Chat über das XMPP-Protokoll unterstützt.\n\nEmpathy löste in Ubuntu 9.10 den Messenger-Client Pidgin als Standard-IM-Programm ab (Pidgin wird aber weiterhin für Ubuntu paketiert und kann über die Paketquellen nachinstalliert werden).\n\n\n"}
{"id": "3415401", "url": "https://de.wikipedia.org/wiki?curid=3415401", "title": "Popular Electronics", "text": "Popular Electronics\n\nPopular Electronics war eine US-amerikanische Computerzeitschrift aus dem Ziff-Davis Verlag. Die Zeitschrift erschien erstmals im Oktober 1954 und war für Hobbyisten gedacht, die sich mit dem Thema Elektronik beschäftigen wollten. Schnell wurde es „World's Largest-Selling Electronics Magazine“. Es wurden 240,151 Exemplare im April 1957 und 400,000 Exemplare im Jahre 1963 monatlich verkauft. Der Ziff-Davis-Verlag veröffentlichte die Zeitschrift bis in den April 1985. Gernsback Publications übernahm den Titel im Jahre 1988 und nannte die Zeitschrift in „Hands-On Electronics“ um. Diese Version von Popular Electronics wurde veröffentlicht bis zum Dezember 1999.\n\nDie wohl bekannteste Ausgabe der Zeitschrift erschien im Januar 1975, als der Altair 8800 Computer auf dem Cover abgebildet war und die Heimcomputer-Revolution einläutete. Paul Allen zeigte die Ausgabe Bill Gates, die daraufhin den BASIC Interpreter für den Altair Computer schrieben und die Firma Microsoft gründeten.\n\n"}
{"id": "3416827", "url": "https://de.wikipedia.org/wiki?curid=3416827", "title": "Zenon (Software)", "text": "Zenon (Software)\n\nZenon (Eigenschreibweise: zenon) ist ein Software-System des Systemhauses COPA-DATA für die industrielle Automatisierung und die Energiebranche. Benannt wurde die Software nach dem Philosophen Zenon von Elea.\n\nzenon ist ein offenes und unabhängiges Software-System, das weltweit von Unternehmen aller Größen zur Prozessvisualisierung, sowie als Maschinenbediensystem (HMI) oder als Prozessleitsystem (SCADA) eingesetzt wird.\n\nZur zenon Produktfamilie gehören:\n\n\nHauptmerkmale\n\n\nDer Zenon Editor ist die Applikation, mit welcher die Visualisierung projektiert werden kann. Hier wird z. B. auch der Treiber zur SPS projektiert. Dadurch können Werte der SPS zur Steuerung der Visualisierung verwendet werden.\n\nDer Zenon Editor ist über Module aufgebaut, wobei diese jeweils einen bestimmten Verwendungszweck erfüllen. Je nach Art des Projektes ergibt sich, welche Module zur Verwendung kommen.\n\nDer zenon Editor besticht durch die Tatsache, dass große Projekte mit ihm einfach und modular projektiert werden können. Wenn der Grundaufbau entsprechend erstellt wurde, lassen sich projektweite Änderungen einfach und problemlos realisieren. Dies ist einer der Gründe warum der zenon Editor am Markt heraussticht.\n\nDie Zenon Runtime ist die Applikation, die am endgültigen System läuft. Im zenon Editor wird das Projekt compiliert, wodurch das Runtime Projekt erstellt wird. Dieses Projekt kann nun auf dem Endsystem gestartet werden, auf welchem die Visualisierung startet. Die Runtime ist also die Applikation in der die Werte der Steuerung angezeigt werden bzw. das System gesteuert wird.\n\nMit dem zenon Editor hat man außerdem die Möglichkeit, das Aussehen nahezu aller Benutzeroberflächen anzupassen. Dadurch ist es kein Problem, eine eigene Farbauswahl, Schriften etc. zu verwenden. Um das ganze modularer zu gestalten, existieren hierfür Farbpaletten welche man definieren und bei Bedarf umschalten kann. Außerdem existieren Stile, mit welchen für alle Elemente die jeweiligen Farben, Schiften etc. festgelegt werden können.\n\nDer zenon Editor ist auch dafür ausgelegt, möglichst wenige, ähnliche Elemente zu erstellen. Durch Substitution muss beispielsweise nicht pro Motor eine Übersichtsseite erstellt werden, sondern eine Übersichtsseite wird mittels verschiedener Parameter beim Öffnen jeweils mit verschiedenen Werten gefüttert. Dadurch wird Speicherplatz gespart und die Übersicht über ein Großprojekt bleibt erhalten. Um den Überblick nicht zu verlieren werden teilweise Vorschaumechanismen angeboten, um das Ergebnis vorab kontrollieren zu können.\n\nAußerdem können durch einen groß ausgebauten Filtermechanismus gezielt verschiedenste Daten angezeigt werden, was das Arbeiten an der endgültigen Anlage erleichtert. Sollte das nicht ausreichen, existiert der zenon Analyzer, welcher detailreiche Datenanalysen liefert.\n\nStraton ist der Teil der Suite, mit welchem eine SPS programmiert werden kann ohne eine Fremdsoftware erwerben zu müssen. Auch hier stehen die Treiber für verschiedenste SPS-Hersteller zur Verfügung. Folgende Sprachen sind verfügbar: ST, IL, FBD, SFC, LD.\n\nDies ist die integrierte Soft SPS.\n\nBeim zenon Analyzer handelt es sich um eine extra erwerbbare Applikation, bei der sich alles um das Analysieren, Verbreiten und Präsentieren von Daten dreht.\n\nMit der Smart Checklist steht ein Erweiterungsmodul zur Verfügung das es ermöglicht eine Benutzerführung zu integrieren. Mit dem Modul ist es möglich den Bediener Schritt für Schritt durch einen Arbeitsprozess zu führen. Die Eingaben des Bedieners könne erfasst und als Protokollierung der durchgeführten Arbeitsschritte weiter verwendet werden.\n\nZenon wird in 8 Sprachen ausgeliefert:\n\n\nLaut Infografik zum zenon8 release hatte zenon damals 135.000 Installationen und einen Vertrieb bzw. Support in 47 Ländern.\n\nzenon wird in folgenden Branchen eingesetzt:\n\n\n\n\n"}
{"id": "3417180", "url": "https://de.wikipedia.org/wiki?curid=3417180", "title": "Hessischer Hochleistungsrechner", "text": "Hessischer Hochleistungsrechner\n\nDer Hessische Hochleistungsrechner (HHLR) war ein Teil des Hochleistungsrechnerverbundes des Landes Hessen. Er war von 2002 bis 2012 in Betrieb. Nachfolgesystem ist der Lichtenberg-Hochleistungsrechner.\n\nDie leistungsstärksten Rechner der Welt werden von den USA, China und Japan eingesetzt. Auf europäischer Ebene wurde die \"Partnership for Advanced Computing in Europe - Initiative \"(PRACE) gegründet, an der Deutschland als einer von fünf Hauptpartnern unter derzeit insgesamt 17 Ländern beteiligt ist. Diese Initiative verfolgt das Ziel, eine europaweite Infrastruktur für Hochleistungsrechnen zu schaffen. Auf dieser Ebene sollen Hochleistungsrechner der höchsten Leistungsklasse angesiedelt sein.\n\nAuf nationaler Ebene besteht mit dem Gauß-Centre for Supercomputing (GCS) ein Zusammenschluss der drei Bundesrechenzentren in Jülich, Garching und Stuttgart. Das GCS tritt vertritt die deutschen Interessen in der PRACE-Initiativen.\n\nDie Gauß-Allianz (GA) ist der Zusammenschluss aller Supercomputerzentren auf Landesebene. Die hessischen Interessen in der Gauß-Allianz werden durch die TU Darmstadt als stimmberechtigtes Mitglied und die Goethe-Universität Frankfurt als assoziiertes Mitglied vertreten. Die TU Darmstadt betrieb den HHLR.\n\nDer Rechner wurde 2002 installiert. Er konnte von Wissenschaftlern aller hessischen Hochschulen sowie dem Helmholtzzentrum für Schwerionenforschung GSI genutzt werden. Er wird durch die beteiligten Hochschulen und das Land Hessen finanziert.\n\nEr wurde vom Hochschulrechenzentrum (HRZ) der TU Darmstadt im Auftrag des HHLR-Beirats sowie der Kompetenzgruppe wissenschaftliches Hochleistungsrechnen im Forschungszentrum Computational Engineering (CE) betrieben.\n\nDer Rechner war Teil des hessischen Hochleistungsrechnerverbundes. Gemeinsam mit dem vom Center for Scientific Computing CSC der Universität Frankfurt aufgebauten Linux-PC-Cluster für weniger eng gekoppelte Aufgabenstellungen (geringer Kommunikationsbedarf) ist er einer der beiden Pfeiler des Hessischen Hochleistungsrechner-Konzeptes.\n\nNachfolge-System ist seit 2012 der Lichtenberg-Hochleistungsrechner, dessen Leistung die des HHLR bereits bei Einführung in der ersten Ausbaustufe um das 30-fache übertraf.\n\nDas System bestand aus 19 SMP-Knoten mit insgesamt 580 Prozessoren. Die 18 Rechenknoten verfügten jeweils über 32 Power6-CPUs und mindestens 128 GB Arbeitsspeicher. Als Shared Memory Rechner (SMP) waren die Maschinen besonders gut für parallele Probleme mit sehr hohem Kommunikationsbedarf geeignet. Um auch Programme, die mehr als einen SMP-Knoten benötigen, effektiv verarbeiten zu können, waren die Rechner untereinander mit einem schnellen internen Netzwerk verbunden (8 Lanes DDR InfiniBand).\n\nDie Hauptaufgabe des Systems bestand darin, Ressourcen für Problemstellungen zur Verfügung zu stellen, deren Anforderungen die lokalen Möglichkeiten eines Instituts oder einer einzelnen Hochschule übersteigen.\n\nDer HHLR war dabei speziell auf Anwendungen mit hoher Parallelität und hohem Kommunikationsbedarf ausgerichtet. Aber auch Anwendungen mit besonders hohem Hauptspeicherbedarf hatten hier ihren Platz.\n\n"}
{"id": "3417709", "url": "https://de.wikipedia.org/wiki?curid=3417709", "title": "Big Buck Bunny", "text": "Big Buck Bunny\n\nBig Buck Bunny ist ein computergenerierter Kurzfilm, der hauptsächlich unter Verwendung von freier Software hergestellt wurde, insbesondere der 3D-Grafiksoftware Blender.\n\nSowohl der fertige Film als auch die Produktionsmaterialien wie Animationsdaten, Charaktere und Texturen wurden am 30. Mai 2008 unter der freien Creative-Commons-Namensnennungs-Lizenz veröffentlicht. Bei der Ankündigung des Films wurde er als „funny and furry“ (witzig und pelzig) beschrieben, was vor allem durch Slapstick-Komik erreicht wird.\n\nDer Hauptcharakter ist ein ungewöhnlich großes und fülliges Kaninchen („Big Buck Bunny“), das sich zu Beginn des Films an Blumen und Schmetterlingen erfreut. Als jedoch das Flughörnchen Frank, das Eichhörnchen Rinky und das Chinchilla Gamera auftauchen, zwei Schmetterlinge töten und das Kaninchen mit Früchten und Nüssen bewerfen, beschließt es, seine Sanftmütigkeit abzulegen und an den Nagetieren Rache zu nehmen. Dazu baut es verschiedene Fallen auf.\n\nAm 10. Juni 2007 kündigte Ton Roosendaal einen Nachfolger des ersten Projekts der Blender Foundation von 2006 – \"Elephants Dream\" – an.\nIm August 2007 fand sich ein Produktionsteam mit acht Mitgliedern aus sieben Ländern im \"Blender Institute\" in Amsterdam zusammen. Dieses Studio wurde von der Blender Foundation eingerichtet, um das Erstellen von freien Filmen und Spielen zu erleichtern.\n\nUnter dem Codenamen \"Peach\" begann die Arbeit im Oktober 2007. Neben dem eigentlichen Film ergaben sich auch Verbesserungen der verwendeten Software Blender, wie Systeme zur Darstellung von Fell, Bäumen und volumetrischem Licht sowie einige Fehlerbehebungen. Nach der Premiere des Films am 10. April 2008 in Amsterdam und dem Versand von vorbestellten DVDs ab dem 20. Mai wurde er am 30. Mai 2008 im Internet veröffentlicht.\n\nDer Film wurde durch die Blender Foundation, Spenden der Blender-Community, den Vorverkauf der DVD des Films und kommerzielle Sponsoren finanziert. Auf Basis des Films wurde im Blender Institute ein Computerspiel mit dem Namen \"Yo Frankie\" bzw. Codenamen \"Apricot\" erstellt.\n\nEin weiterer Kurzfilm der Blender Foundation mit Namen Sintel wurde ebenfalls von Ton Roosendaal produziert und am 1. Oktober 2010 veröffentlicht.\n\n\n\n"}
{"id": "3436131", "url": "https://de.wikipedia.org/wiki?curid=3436131", "title": "MacWWW", "text": "MacWWW\n\nMacWWW, auch Samba genannt, ist ein früher Webbrowser von 1992 – der erste, der auf einem Macintosh-Computer lief, und der erste für Nicht-Unix-Systeme. Im Gegensatz zu moderneren Webbrowsern öffnete Samba jeden Link generell in einem neuen Fenster. Programmiert wurde die Anwendung von Robert Cailliau unter Mitarbeit von Nicola Pellow am CERN.\n\n"}
{"id": "3438018", "url": "https://de.wikipedia.org/wiki?curid=3438018", "title": "G-Eclipse", "text": "G-Eclipse\n\ng-Eclipse ist ein Open-Source-Framework und ein integriertes Werkzeug, welches Anwender, Administratoren und Entwickler bei der Arbeit mit Computing Grids unterstützt. Es basiert auf der offenen Eclipse-Plattform und lässt sich mit weiteren Funktionalitäten ergänzen.\n\nAufbauend auf der Plug-in-Architektur der Eclipse-Plattform erweitert g-Eclipse deren Funktionalität und grafische Benutzeroberfläche um spezielle Funktionen, mit denen auf existierende Grid-Infrastrukturen zugegriffen werden kann. Außerdem unterstützt g-Eclipse Entwickler bei der Erstellung und der Installation von Anwendungen, sowie Administratoren bei der Überwachung und Einrichtung von Grid Ressourcen.\n\nDie Funktionalität von g-Eclipse lässt sich über Extension Points in eigenen Plug-ins erweitern.\n\ng-Eclipse ist ein Teilprojekt des Technologieprojekts der Eclipse Foundation und wird seit 2006 aktiv von einem Konsortium bestehend aus dem Forschungszentrum Karlsruhe, dem Poznań Supercomputing and Networking Center, der Johannes Kepler Universität Linz, der Universität Zypern, der Innoopract GmbH (Deutschland), der Universität Reading und IT Innovation Centre weiterentwickelt.\nDas g-Eclipse-Konsortium wird durch das 6. Forschungsrahmenprogramm gefördert.\n\ng-Eclipse erweitert Eclipse um drei Perspektiven:\n\nIntern arbeitet g-Eclipse mit einem Modell, das unabhängig von der verwendeten Grid-Middleware ist.\nZurzeit existieren für g-Eclipse spezifische Grid-Middleware-Implementierungen für gLite (Verwaltung von Virtuellen Organisationen, Anwendungen und Daten, Überwachung der Infrastruktur und Anwendungsprogramme, Visualisierung von Daten und Erstellung von Workflows), für GRIA (Verwaltung von Anwendungen und Daten) und für die Cloud Computing Implementation der Amazon Web Services \"Elastic Compute Cloud\" (EC2) und Simple Storage Service (S3). \n"}
{"id": "3447139", "url": "https://de.wikipedia.org/wiki?curid=3447139", "title": "Arduino (Plattform)", "text": "Arduino (Plattform)\n\nArduino ist eine aus Soft- und Hardware bestehende Physical-Computing-Plattform. Beide Komponenten sind im Sinne von Open Source quelloffen. Die Hardware besteht aus einem einfachen E/A-Board mit einem Mikrocontroller und analogen und digitalen Ein- und Ausgängen. Die Entwicklungsumgebung basiert auf Processing und soll auch technisch weniger Versierten den Zugang zur Programmierung und zu Mikrocontrollern erleichtern. Die Programmierung selbst erfolgt in einer C bzw. C++-ähnlichen Programmiersprache, wobei technische Details wie Header-Dateien vor den Anwendern weitgehend verborgen werden und umfangreiche Bibliotheken und Beispiele die Programmierung vereinfachen. Arduino kann verwendet werden, um eigenständige interaktive Objekte zu steuern oder um mit Softwareanwendungen auf Computern zu interagieren (z. B. Adobe Flash, Processing, Max/MSP, Pure Data, SuperCollider, diversen Skriptsprachen, Terminal, vvvv etc.). Arduino wird beispielsweise auch an Kunsthochschulen genutzt, um interaktive Installationen aufzubauen.\n\nDas Arduino-Projekt wurde im Rahmen des Prix Ars Electronica 2006 mit einer Anerkennung in der Kategorie Digital Communities ausgezeichnet.\n\nDas erste Board wurde 2005 von Massimo Banzi und David Cuartielles entwickelt. Der Name „Arduino“ wurde von einer Bar in Ivrea übernommen, in der sich einige der Projektgründer gewöhnlich trafen. (Die Bar selbst wurde nach Arduin von Ivrea benannt, der von 1002 bis 1014 auch König von Italien war.) David Mellis entwickelte die auf C/C++ basierende Diktion dazu. Das Schema wurde im Netz veröffentlicht und unter eine Creative-Commons-Lizenz gestellt. Die erste Auflage betrug 200 Stück, davon gingen 50 an eine Schule. Bis 2008 wurden etwa 50.000 Boards verkauft.\n\nVom Januar 2015 bis Ende August 2016 befanden sich die Gründergruppe der Arduinoplattform (Arduino LLC) und die Produzenten der offiziellen Arduinoboards (Arduino S.r.l.) in einem Rechtsstreit um die Inhaberschaft des Markenrechtes von Arduino. Demnach habe die Arduino S.r.l. die Marke Arduino für die Nizza-Klassen 9 und 42 registriert, während die Arduino LLC die Marke zeitgleich in den USA nur für die Nizza-Klasse 9 registrierte. Nun war unklar, ob diese parallelen Markenrechtseintragungen legitim waren bzw. wem nun die Marke Arduino gehörte.\n\nIm März 2015 gab der Arduino-Gründer Massimo Banzi bekannt, dass der Hersteller der Arduinoboards Arduino S.r.l. seit einem Jahr keine Lizenzgebühren mehr an die Arduino LLC zahle, mit denen jedoch zuvor die relativ hohen Preise der Arduinoboards begründet wurden.\n\nZeitweise existierten zwei Webpräsenzen von Arduino: arduino.org, die von der Arduino S.r.l. betreut wird, sowie arduino.cc, die von der Arduino LLC aufgebaut wurde. Auf diesen Websites wurden seit dem Rechtsstreit verschiedene Varianten der Arduino IDE mit unterschiedlichen Versionsangaben zum Download angeboten. Diese waren nicht vollständig mit der Hardware des jeweils anderen Unternehmens kompatibel. Mit Version 1.8 wurden beide Varianten wieder zusammengefügt. Seitdem gibt es wieder eine offizielle Version, die die Boards beider Unternehmen unterstützt.\n\nBei einer Veranstaltung des US-amerikanischen Magazins \"Make\" im kalifornischen San Mateo am 16. Mai 2015 stellte Massimo Banzi einen neuen Markennamen für das Arduino-Projekt vor, der \"Genuino\" lautet. Dieser neue Markenname soll in solchen Situationen genutzt werden, wo die Markenrechte der eigentlichen Marke \"Arduino\" ungeklärt sind. Außerdem kündigte er an, zukünftig Boards von mehreren Herstellern herstellen zu lassen, anstatt von einem einzigen, wie es bisher der Fall war. Banzi nannte die Firma Adafruit Industries als ersten offiziellen Hersteller. Die Gründerin von Adafruit Industries Limor Fried bestätigte die Zusammenarbeit. Damit macht sich die Arduino LLC unabhängig vom eigentlichen Hersteller der Arduinoboards Arduino S.r.l.\n\nIm Juni 2015 gab Banzi in Shenzhen die Zusammenarbeit mit dem Hersteller \"Seeedstudio\" bekannt, der die Microcontroller-Boards mit der neu erschaffenen Marke Genuino herstellen soll. Mit den von Seeedstudio hergestellten Boards soll der asiatische Markt, insbesondere der chinesische Markt, bedient werden.\n\nAuf der Maker Faire Rom im Oktober 2015 wurden zum ersten Mal Genuino Boards aus europäischer Produktion verkauft.\nDie Boards wurden von \"Watterott electronic\" aus Deutschland und \"AXEL Elettronica\" aus Italien gefertigt.\n\nDie Marke Genuino soll nun für jene Microcontroller-Boards verwendet werden, die außerhalb der Vereinigten Staaten verkauft werden. Außer der neuen Marke sollen die verkauften Boards sich nicht von den ehemaligen Arduino-Boards unterscheiden. Anscheinend ist die Rechtslage um die Markenrechte von Arduino nur in den Vereinigten Staaten eindeutig.\n\nBei der World Maker Faire 2016 in New York haben sich die Arduino S.r.l. und die Arduino LLC darauf geeinigt, die Streitigkeiten beizulegen, dafür sollen alle Produkte einzig über eine neu gegründete kommerzielle Arduino Holding vertrieben werden. Außerdem soll die gemeinnützige Arduino Foundation gegründet werden, die als not-for-profit Organisation die weitere Entwicklung der Open-Source Software Arduino Desktop IDE vorantreibt. Deswegen sind Arduino S.r.l. und Arduino LLC Ende 2016 in der Arduino AG aufgegangen. Allerdings wurde selbige nach Betrugsvorwürfen gegen den Geschäftsführer und Hauptanteilseigner Federico Musto von der von Gründern des Arduino-Projekts gegründeten BCMI aufgekauft.\n\nDie Hardware eines typischen Arduinoboards basiert auf einem Atmel-AVR-Mikrocontroller aus der megaAVR-Serie wie dem ATmega328. Abweichungen davon gibt es unter anderem bei den Arduinoboards Arduino Due (ARM Cortex-M3 32-Bit-Prozessor vom Typ Atmel SAM3X8E), Yún, Tre, Gemma und Zero, wo andere Mikrocontroller von Atmel eingesetzt werden. Eine Besonderheit stellen zudem die Arduinoboards Yún und Tre dar, die zusätzlich zum Mikrocontroller einen stärkeren Mikroprozessor besitzen. Alle Boards werden entweder über USB (5 V) oder eine externe Spannungsquelle (7–12 V) versorgt und verfügen über einen 16-MHz-Schwingquarz. Es gibt auch Varianten mit 3,3 V-Versorgungsspannung und Varianten mit abweichendem Takt. Über Erweiterungen können auch andere Mikrocontroller, etwa der ESP8266 über die Arduino-IDE programmiert werden.\n\nKonzeptionell werden alle Boards über eine serielle Schnittstelle programmiert, wenn Reset aktiviert ist. Der Mikrocontroller ist mit einem Bootloader vorprogrammiert, wodurch die Programmierung direkt über die serielle Schnittstelle ohne externes Programmiergerät erfolgen kann. Bei älteren Boards wurde dafür die RS-232-Schnittstelle genutzt. Bei aktuellen Boards geschieht die Umsetzung von USB nach seriell über einen eigens entwickelten USB-Seriell-Konverter, basierend auf dem ATmega8u2. Zuvor wurde das mit dem populären Baustein FT232RL von FTDI realisiert. Die Version Arduino Leonardo verwendet als Prozessor den ATmega32u4, der die USB-Unterstützung nativ bereitstellt und sich damit auch als Tastatur oder Maus gegenüber einem PC ausgeben kann.\n\nAlle Arduinoboards, bis auf den Arduino Esplora, stellen digitale Input- und Output-Pins (kurz: I/O-Pins) des Mikrocontrollers zur Nutzung für elektronische Schaltungen zur Verfügung. Üblich ist auch, dass eine bestimmte Anzahl dieser Pins PWM-Signale ausgeben können. Zusätzlich stehen dem Benutzer eine bestimmte Anzahl an analogen Eingängen zur Verfügung. Für die Erweiterung werden vorbestückte oder teilweise unbestückte Platinen – sogenannte „Shields“ – angeboten, die auf das Arduino-Board aufsteckbar sind. Es können aber auch z. B. Steckplatinen für den Aufbau von Schaltungen verwendet werden.\nArduino bringt eine eigene integrierte Entwicklungsumgebung (IDE) mit, die auf Wiring IDE basiert. Dabei handelt es sich um eine Java-Anwendung, die für die gängigen Plattformen Windows, Linux und macOS kostenlos verfügbar ist. Sie basiert auf der IDE von Processing, einer auf die Einsatzbereiche Grafik, Simulation und Animation spezialisierten Entwicklungsumgebung. Die Arduino-IDE bringt einen Code-Editor mit und bindet gcc als Compiler ein. Zusätzlich werden die avr-gcc-Library und weitere Arduino-Librarys eingebunden, die die Programmierung in C und C++ stark vereinfachen.\n\nFür ein funktionstüchtiges Programm genügt es, zwei Funktionen zu definieren:\n\nHier ein Beispiel für ein Programm (in der Arduino-Diktion: \"Sketch\"), das eine an das Arduino-Board angeschlossene LED blinken lässt:\nint ledPin = 13; // Die LED ist an Pin 13 angeschlossen, was in der Integer-Variable ledPin gespeichert ist.\n\nvoid setup() {\n\nvoid loop() {\n\nMit S4A (Scratch for Arduino) und mblock (basierend auf scratch) gibt es Scratch-Modifikationen, die freie visuelle Programmiersprache mit Programmierumgebung für den Arduino-Mikrocontroller zur Verfügung stellt.\n\nWeiterhin bietet Arduino mit \"Arduino Create\" eine webbasierte Lösung an, um im Browser zu programmieren. Geschriebene Sketche werden online in einer Cloud abgelegt. Die Kommunikation zwischen Browser und Arduinoboard wird über Plugins für das jeweilige Betriebssystem ermöglicht. Arduinoboards können über USB und Netzwerkverbindung angesprochen werden. Die Nutzung von Arduino Create erfordert eine kostenlose Registrierung bei diesem Dienst. Betrieben wird die Plattform durch Amazon Web Services.\n\n\n\n\n"}
{"id": "3447894", "url": "https://de.wikipedia.org/wiki?curid=3447894", "title": "Windows-1258", "text": "Windows-1258\n\nWindows-1258 ist eine 8-Bit-Zeichenkodierung des Windows-Betriebssystems. Sie deckt die Orthographie der vietnamesischen Sprache ab.\n\nDa diese Orthographie insgesamt 134 Zeichen außer den ASCII-Zeichen verwendet, ist eine einfache Erweiterung des ASCII-Zeichensatzes auf 8-Bit nicht möglich. Windows-1258 löst dieses Problem durch die Verwendung kombinierender Zeichen, die sich mit den Basiskonsonanten verbinden und so die nötigen Kombinationen erzeugen. Der Zeichensatz selber basiert größtenteils auf Windows-1252, enthält jedoch außer den Diakritiken einige weitere vietnamesische Buchstaben und Zeichen.\n\nDie folgende Tabelle zeigt das Repertoire von Windows-1258. Diakritische Zeichen sind rot markiert, weitere Unterschiede zu Windows-1252 blau. Unbelegte Positionen sind grün markiert.\n\n\nWindows 1258\n"}
{"id": "3448835", "url": "https://de.wikipedia.org/wiki?curid=3448835", "title": "Codepage 932", "text": "Codepage 932\n\nDie Codepage 932 ist eine Zeichenkodierung des Windows-Betriebssystems, die mit Windows 3.1 eingeführt wurde. Sie ist bei der IANA als \"Windows-31J\" registriert.\n\nDie Codepage ist eine Erweiterung des Shift-JIS-Zeichensatzes und kodiert unter den führenden Bits 87, ED, EE, FA, FB und FC zusätzliche eingekreiste und eingeklammerte Zeichen, römische Ziffern, mathematische Zeichen und zusätzliche Kanji.\n\nAls 1982 der Shift-JIS-Zeichensatz erstellt wurde, hat Microsoft diesen Zeichensatz für seine Produkte eingeführt. Eine modifizierte Version dieses Zeichensatzes wurde 1983 für den IBM 5550 verwendet. Zur gleichen Zeit wurde eine andere Version für die NEC-Computer der PC-9800-Serie eingeführt. Als Microsoft die japanische Version von Windows 3.1 entwickelte, entschied es sich, die zusätzlichen Zeichen sowohl der NEC- als auch des IBM-Computers in ihren Zeichensatz zu integrieren und in Windows zu verwenden.\n\n"}
{"id": "3452740", "url": "https://de.wikipedia.org/wiki?curid=3452740", "title": "Bit Power", "text": "Bit Power\n\nBit Power war eine Zeitschrift, die sich dem Belangen von Heimcomputern widmete. Sie wurde monatlich herausgegeben.\n\nGegründet wurde Bit Power von Maik Heinzig, der auch als Chefredakteur fungierte. Herausgeber war der mhs-Verlag Dresden (sh. Quellen). Es ist nicht bekannt, wie viele Ausgaben in der Zeit von 1988 bis 1990 erschienen, da diese nicht über den Postzeitungsvertrieb gehandelt wurden. Ab März 1990 wurde die Zeitschrift jedoch regelmäßig zu Monatsbeginn bis zu ihrer Einstellung im März 1991 herausgegeben.\nDie ersten Ausgaben, die noch während des Bestehens der DDR hergestellt und vertrieben wurden, wurden noch komplett mit einer Schreibmaschine geschrieben und im Buchdruckverfahren hergestellt. Ab Mitte 1990 wurde dann der Offset-Druck genutzt.\n\nUngewöhnlich war, dass die Zeitschrift nicht nur über DDR-eigene Entwicklungen, sondern auch über Atari, Commodore und Schneider Computer berichtete. Sie veröffentlichte unter anderem Bauanleitungen, Artikel über Softwareentwicklungen, Listings, sowie Soft- und Hardwaretests. Die Bit Power wurde unter Register Nr. 1775 vertrieben.\n\n"}
{"id": "3452929", "url": "https://de.wikipedia.org/wiki?curid=3452929", "title": "Information Retrieval Facility", "text": "Information Retrieval Facility\n\nDas Information Retrieval Facility (kurz IRF) war eine Forschungsplattform und diente der Zusammenarbeit von Experten im Bereich der Information Retrieval (IR). Es wurde 2006 gegründet und hatte seinen Sitz in Wien. Das IRF war das weltweit erste E-Science-System, das ausschließlich der semantischen Verarbeitung von Text gewidmet war. Zu den Mitarbeitern zählten Experten, Forscher und Studenten in den Bereichen Information Retrieval und Informationsmanagement. Das IRF hat 2012 seine Aktivitäten eingestellt.\n\nDas Information Retrieval Facility diente der Forschung und Schaffung von wissenschaftlichen Zielen. Dazu gehörten unter anderem die Modellierung von Information-Retrieval-Systemen für globale Patentdokumentsammlungen. Mit diesen Systemen, die sich mit dem Begriff Informationsrückgewinnung übersetzen lassen, können komplexe Suchen durchgeführt werden. Diese umfasst neben Textdateien auch Informationen aus Bildern.\n\nEin weiterer Zweig des Unternehmens war die die Erforschung und Entwicklung einer technischen Infrastruktur, die interaktive Experimente mit formalen und mathematischen Retrieval-Konzepten für sehr große Dokumentsammlungen ermöglicht. Eng verbunden ist damit die Untersuchung der Usability von multimodalen User-Interfaces groß angelegter Information-Retrieval-Systeme, um eine angemessene Bedienung durch verschiedene Benutzergruppen zu ermöglichen. Des Weiteren wurde die Integration von Usern und deren Bedürfnissen in den Prozess der Modellierung von Information-Retrieval-Systemen eingebunden, so dass eine genaue Leistungsbewertung sichergestellt war. \n\nPatentdaten wurden in unterschiedlichen Ansichten dargestellt, so dass in der entsprechenden Abhängigkeit der Fokus gewährleistet ist. \n\nDas IRF hat sich auch für eine Definition standardisierter Methoden für die Bewertung der Information-Retrieval-Prozesse in den Patentschriftkollektionen eingesetzt. Sie wollten die Fähigkeit, Text- und Nicht-Text-Anteile eines Patents in einer kohärenten Weise in den Griff bekommen und Suchmaschinen entwickeln, die es ermöglichen, strukturierte und semi-strukturierte Dokumente in sehr großen Patent-Sammlungen zu finden. Im Rahmen der Erprobung sollten Bewertungen vorgenommen werden. Dabei sollten zeitliche Dimensionen von Patentdokumenten in Retrieval-Strategien integriert werden.\n\nEin weiteres Ziel der IRF war die Verbesserung der Effizienz und Präzision von Patent-Retrieval basierend auf Ontologien und verschiedenen Sprach-Techniken und die Schaffung von verbesserten IR-Methoden, mit denen die Nutzung unstrukturierter Abfragen innerhalb eines Patentdokuments möglich wird. Formale (mathematische) Identifikation und Spezifikation von Business-relevanten Informationen sollen helfen, Intellectual Property (Geistiges Eigentum) zu erkennen. Zudem wurde die Erforschung von Skalierungsmechanismen im Information-Retrieval Bereich unter Berücksichtigung der Merkmale von Patentdaten und die Ermittlung und das Experimentieren mit Computing-Architekturen für sehr hohes Kapazität-Informations-Management vorangetrieben.\n\nDie Schaffung einer offenen E-Science-Plattformm, die auf eine einheitliche und einfache Weise die Erstellung und Durchführung von IR-Experimenten auf einer gemeinsamen Forschungsinfrastruktur ermöglicht, stand weiterhin auf der Agenda des Unternehmens. Hinzu kam die Entdeckung und Erforschung von Anwendungszwecken und Business-Anwendungen, die sich aus Informationen der Intellectual Property ergeben. Aktiviert wurden dazu formale Informationsrückgewinnungen (Information Retrieval), Sprachen und semantische Verarbeitungen in den Bereich der angewandten Wissenschaften, die die Informationen in den globalen, industriellen Kontext bringen. Die Entwicklung und Integration von verschiedenen Informations-Zugriffsmethoden und die Forschung über effektive Methoden für die interaktive Information-Retrieval war ein weiteres Tätigkeitsfeld.\n\nAktuelle Technologien zur Extraktion von Konzepten aus unstrukturierten Dokumenten sind mit intensiver Rechenleistung verbunden. Um das interaktive Experimentieren mit großen Text-Korpora zu ermöglichen, besaß das IRF eine High-Performance-Computing (HPC)-Umgebung für performantes Text Mining. Diese war mit einem Multi-Node-System ausgestattet, das 80 Kernen besteht, dass bis auf 1024 Kerne aufgestockt werden konnte. Dieses war mit einer Höchstgeschwindigkeits-Interconnect Technologie verbunden. Hinzu kamen einzelne Systeme mit großen Speichermöglichkeiten von 320 GB, die bis 4 TB ausgebaut werden konnten. Die Systeme waren dank 4 FPGA-Cores, die bis zu 256 Cores ausbaubar waren, zudem komplett schaltbar. \nDie Zielsetzung des IRF war die Schaffung einer Plattform für Patent-Experten, die auf modernen Information-Retrieval-Technologien basiert. Es wurde erwartet, dass die Information Retrieval (IR)-Technologien in den Mittelpunkt der Informationstechnologie treten werden.\n\nDie Gesamtheit aller Patent-Dokumente stellt einen gewaltigen Textkorpus dar. Patente haben sich zu einem entscheidenden Thema insbesondere für globale Unternehmen und Universitäten entwickelt. Die industriellen Anwender von Patentdaten gehören zu den anspruchsvollsten und wichtigsten Informationsprofis überhaupt. Diese Zielgruppen werden am meisten von einer Technologie profitieren, die ihnen bei der Erforschung großer Datenmengen hilft.\n\n\n"}
{"id": "3453595", "url": "https://de.wikipedia.org/wiki?curid=3453595", "title": "Codepage 874", "text": "Codepage 874\n\nDie Codepage 874 ist eine 8-Bit-Zeichenkodierung des Windows-Betriebssystems. Sie kodiert die thailändische Schrift.\n\nSie basiert auf und ist aufwärtskompatibel von TIS-620 sowie ISO 8859-11, enthält aber außerdem die typografischen Zeichen von Windows-1252 sowie das Eurozeichen.\n\nDie folgende Tabelle zeigt das Repertoire der Codepage 874. Kombinierende Zeichen sind hellblau markiert, nicht belegte Stellen grün.\n\n"}
{"id": "3455960", "url": "https://de.wikipedia.org/wiki?curid=3455960", "title": "GBK", "text": "GBK\n\nGBK (kurz für ; von GB Standard sowie ) ist ein chinesischer Zeichensatz. Er erweitert GB2312 um traditionelle Schriftzeichen sowie um Schriftzeichen, die nach der Einführung von GB2312 1981 vereinfacht wurden.\n\n1993 wurde Unicode 1.1 veröffentlicht, das 20.902 chinesische Schriftzeichen enthält. Die chinesische Regierung hat daraufhin GB13000.1-93 veröffentlicht, welcher identisch mit Unicode 1.1 ist. Um die Lücke zwischen diesem Standard und dem älteren GB2312 (1980) zu überbrücken, wurde auch GBK eingeführt, das GB2312 um die Zeichen aus GB13000.1-93 erweitert. Weil GBK jedoch nie zur offiziellen Norm wurde, erhielt es auch keine reguläre GB-Nummer. 1995 wurde GBK um 95 weitere Schriftzeichen erweitert.\n\nIn Windows 95 wurde GBK als \"Codepage 936\" in unveränderter Form übernommen. Dadurch stieg die Verbreitung von GBK enorm, und GBK wurde zum De-facto-Standard. Später wurde das Eurozeichen zur Codepage 936 hinzugefügt, was die Codepage inkompatibel zu GBK machte.\n\nIn den meisten Windows-Varianten wird GBK jedoch irreführend als GB2312 bezeichnet. Erst ab Windows XP wurde zusätzlich auch die ursprüngliche Norm GB2312 unter Windows angeboten, und zwar unter der Codepage-Nummer 20936 mit der Bezeichnung \"GB2312-80\".\n\nSeit 2000 ist GBK offiziell von GB 18030 abgelöst.\n\nGBK ist eine variable 16-Bit-Kodierung, d. h. ein Zeichen kann entweder ein oder zwei Byte groß sein. Die Zeichen im Bereich 0x00-0x7F sind identisch zu ASCII und bestehen aus nur einem Byte. Die Zeichen im Bereich 0x81-0xFE hingegen bestehen aus zwei Bytes.\n\nEin in GBK kodierter Text kann nur vorwärts durchsucht werden, da bei einem beliebigen Zeichen nicht unterschieden werden kann, ob es Anfangsbyte oder Endebyte einer Zweibyte-Kodierung ist. Zur Unterscheidung muss der Text von Anfang an untersucht werden. Diese nachteilige Eigenschaft hat GBK mit GB2312 und GB18030 und den anderen asiatischen Kodierungen SHIFT-JIS (japanisch), BIG-5 (traditionelles Chinesisch) und EUC-KR (koreanisch) gemeinsam.\n\nBei GB2312 kann auch ein durch Rückwärtssuche gefundenes ASCII-Zeichen (Bytewert kleiner als 128) als Ausgangspunkt für eine Vorwärtsanalyse verwendet werden, da diese Werte nicht in Zwei-Byte-Zeichen enthalten sind; bei GBK reduziert sich diese Möglichkeit auf ASCII-Zeichen im Bereich 0 bis 63, da auch Bytewerte im Bereich 64 bis 127 als End-Byte eines Zwei-Byte-Zeichens verwendet werden.\n\nDieses Problem vermeidet die Unicode-Transformation UTF-8. Obwohl hier auch bis zu vier Byte pro Zeichen benötigt werden, kann doch von jedem Byte eindeutig gesagt werden, ob es ein Ein-Byte-Zeichen, ein Anfangs-Byte eines Mehr-Byte-Zeichens oder ein Folge- oder End-Byte eines Mehr-Byte-Zeichens ist.\n\nDer Zwei-Byte-Bereich ist in acht \"levels\" eingeteilt:\n\n"}
{"id": "3456547", "url": "https://de.wikipedia.org/wiki?curid=3456547", "title": "APT-RPM", "text": "APT-RPM\n\nAPT-RPM ist eine Portierung des Advanced Packaging Tool (APT), welches so modifiziert wurde, dass es mit dem RPM Package Manager (RPM) arbeitet. Ursprünglich wurde es nach RPM von Alfredo Kojima portiert, später aber von Gustavo Niemeyer verbessert und weiterentwickelt. Beide arbeiteten zur selben Zeit bei Conectiva Linux.\n\nIm März 2005 gab Gustavo Niemeyer bekannt, dass er am Programm nicht mehr mitentwickeln wird, und sich stattdessen mehr auf den geplanten Nachfolger von APT-RPM, den Smart Package Manager konzentrieren wird.\n\nEin Jahr später wurde die Entwicklung von Panu Matilainen aufgenommen, der Multilib-Funktionalität und Unterstützung für die üblichen Repository Metadaten einbringen will.\n\nVerwendung findet APT-RPM beispielsweise in der Linux-Distribution PCLinuxOS.\n"}
{"id": "3457047", "url": "https://de.wikipedia.org/wiki?curid=3457047", "title": "Rane Serato Scratch Live", "text": "Rane Serato Scratch Live\n\nRane Serato Scratch Live (SL oder SSL) war ein DJ-Programm, welches als sogenannter Vinyl-Emulator und Bestandteil eines Digital Vinyl Systems, der von den Unternehmen Rane Corporation (Hardware) und Serato Audio Research (Software) entwickelt wurde. Dabei wird ein Steuersignal in Form eines analogen Audiosignals von einer Schallplatte oder CD an die Software geleitet. Diese setzt den Code zur Steuerung verschiedener Audio- und Videosignale um. Eine Veränderung des Signals durch eine Vorwärts- oder Rückwärtsbewegung der Schallplatte oder der CD (z. B. durch Scratchen) wirkt sich dabei 1:1 auf die Ausgabe des Audiosignales aus. Zusätzlich beinhaltet die Software Funktionen wie Loop, Cue Points etc. Das über das Steuersignal erstellte Audiosignal wird vom Computer über den USB-Port an die Rane Audiointerfaces gegeben und von diesen als analoges Signal ausgegeben. Die Hardware ist dabei Soundkarte und Dongle für die Software zugleich. Das Produkt ist in 2013 abgekündigt und durch Serato DJ Pro ersetzt worden.\n\nScratch Live kann man, auch bevor man die nötige Hardware besitzt, von der offiziellen Homepage herunterladen. So hat man die Möglichkeit, seine Musik zu organisieren und die Software zu konfigurieren. Die Benutzeroberfläche weist dabei jedoch nur einen Player auf, um Musik direkt in der Software vorhören zu können und z. B. CUE-Punkte oder Loops zu setzen. Zusätzlich können die eingepflegten Songs gescannt werden, um mögliche Defekte und Inkompatibilitäten zu erkennen. Erst wenn man die Hardware verbindet, erscheinen zwei virtuelle Decks zum Arbeiten. Eine komplizierte Registrierung, Freischaltung oder Bindung an einen Account ist nicht nötig, da die Hardware den Dongle (und Kopierschutz) darstellt.\nScratch Live besitzt einen Geschwindigkeitszähler (BPM), Echtzeit-Geschwindigkeitsanpassung, bis zu 5 Cue Points und 9 Loops pro Lied und die Einbindung von iTunes-Playlisten. Weitere Funktionen wie ID3-Tag Suche, Sortierung nach verschiedenen Kriterien und eine Coveranzeige sind in dem eigenen Browser auch vorhanden. Jede Funktion von Scratch Live kann auch mit einem MIDI-Gerät gesteuert werden und so die Bedienung erleichtern.\n\nMit der offiziellen Erweiterung Video-SL (VSL) ist zusätzlich die Ausgabe von Videodateien möglich. Das Plug-in nutzt QuickTime und kann daher weitgehend alle von QuickTime unterstützten Formate nutzen. Zudem ist von Rane ein Mixer namens TTM 57SL erschienen, der den legendären TTM 56 mit Scratch LIVE verbindet. Der Mixer hat die nötige Hardware direkt integriert und ermöglicht das einfache Steuern der Software mit verschiedenen Potentiometern und Knöpfen.\nIm September 2012 veröffentlichte Serato eine neue Version der Erweiterung mit dem Namen Serato Video (SV). Dabei wurde zum einen die Performance der Erweiterung verbessert, sowie der Umfang an Effekten und Bedienmöglichkeiten vergrößert. Nutzer von Video-SL hatten die Möglichkeit, ihre Lizenz kostenlos zu konvertieren, um damit auf die neuen Funktionen von Serato Video zuzugreifen.\n\nÜber ein USB-Audio-Interface wird dem Computer das Steuersignal vom Plattenspieler oder CD- Spieler zugeführt. Auch werden die Signale des Computers hier zurück in ein Audiosignal gewandelt, um diese an das Mischpult weiterzugeben. Mit Serato Scratch Live kompatible Audio-Interfaces sind die von Rane hergestellten SL1, SL2, SL3 und SL4, die man in jedes vorhandene Setup einbauen kann. Des Weiteren sind auch die Tonmischer von Rane (Sixty Eight, Sixty Two, Sixty One und TTM 57SL), in welche ein Audio-Interface schon eingebaut ist, mit Serato Scratch Live kompatibel. Für diese wird kein weiteres Audio-Interface benötigt.\n\nMit dem \"Thru Modus\" des Audio-Interface besteht die Möglichkeit, das analoge Audiosignal der Platten oder CDs direkt an den Tonmischer weiterzugeben. Die Inhalte vorhandener Tonträger können so weiterhin genutzt werden.\n\nDie Schallplatten und CDs beinhalten einen speziellen Code, die Serato Noisemap. Es handelt sich hierbei nicht wie bei vergleichbaren Produkten um einen Timecode, sondern um einen auf LFSR basierenden Pseudozufalls Code maximaler Länge, welcher zur Positionsbestimmung verwendet wird. Dazu wird das Audiosignal, ähnlich wie beim Fax oder der Datenkassette, akustisch abgegriffen und als Datenstrom interpretiert.\n\n\n"}
{"id": "3461329", "url": "https://de.wikipedia.org/wiki?curid=3461329", "title": "Foomatic", "text": "Foomatic\n\nFoomatic ist ein konfigurierbares Drucksystem, das die Nutzung von Druckern unter Linux vereinfacht. Dabei wird über PPD-Dateien eine Konfiguration erstellt, die es dem Spooler ermöglicht, die Daten optimal an den Drucker zu übertragen. Das Paket funktioniert dabei mit den meisten Druckern, die Ghostscript als Druckprozessor einsetzen.\n\nZurzeit kümmert sich die Arbeitsgruppe von OpenPrinting der Linux Foundation um die Weiterentwicklung.\n\nDas Paket beinhaltet:\n\nDie folgenden Treiber wurden speziell für die Kompatibilität mit \"Foomatic\" entwickelt:\n\n\n"}
{"id": "3461344", "url": "https://de.wikipedia.org/wiki?curid=3461344", "title": "Windows-1255", "text": "Windows-1255\n\nWindows-1255 ist eine 8-Bit-Zeichenkodierung des Windows-Betriebssystems und kodiert die hebräische Schrift. Es ist zwar inkompatibel zu ISO 8859-8, deckt aber in Gegensatz zu diesem außer Hebräisch auch Jiddisch ab und enthält auch alle Vokalzeichen.\n\nDie folgende Tabelle stellt das Repertoire von Windows-1255 dar. Rote Felder stellen Vokalzeichen dar.\n\n"}
{"id": "3461792", "url": "https://de.wikipedia.org/wiki?curid=3461792", "title": "Stanford Bunny", "text": "Stanford Bunny\n\nDer Stanford Bunny (auf Deutsch: \"Stanford-Hase\") ist eines der meistverwendeten Testmodelle der 3D-Computergrafik. Es wurde 1994 von Greg Turk und Marc Levoy an der Stanford University entwickelt.\n\nDas 3D-Oberflächenmodell besteht aus 69.451 Dreiecken, die durch einen 3D-Scanner von einer Porzellanfigur bestimmt wurden. Das Modell kann für verschiedene Grafik-Algorithmen wie zum Beispiel Polygonvereinfachung, Komprimierung und Oberflächenglättung verwendet werden. Obwohl er als Standardtestobjekt etabliert ist, hat der Stanford Bunny einige Einschränkungen. Dazu gehören zum Beispiel seine Einfachheit (geringe Polygonanzahl) nach heutigen Standards, dass er mannigfaltig verbunden ist und es wegen eingeschränkter Scanmöglichkeit fehlende Informationen in den Daten gibt.\n\nDas Modell war ursprünglich im .ply-Datei-Format („ply“ für Polygon) mit vier verschiedenen Auflösungen erhältlich.\n\nNeben dem Polygonmodell bietet das Stanford 3D Scanning Repository auch CT-Scans des Hasen. \n\n\n"}
{"id": "3463209", "url": "https://de.wikipedia.org/wiki?curid=3463209", "title": "ST Magazin", "text": "ST Magazin\n\nDas Atari-Computermagazin ST Magazin 68000’er ist aus dem Computermagazin 68000er entstanden und unter diesem Namen im Markt & Technik Verlag monatlich ab Ausgabe 04/1988 bis 08/1993 erschienen. In dieser Computerzeitschrift wurde schwerpunktmäßig der Atari ST Computer behandelt. Anschließend wurde dieses Magazin in die Computerzeitschrift \"ST-Computer\" vom Heim-Verlag integriert. Es sind auch einige Sonderhefte erschienen, die am Anfang (1988/89) in Zusammenarbeit mit der Computerzeitschrift \"Happy Computer\" herausgegeben wurde.\n\n"}
{"id": "3463249", "url": "https://de.wikipedia.org/wiki?curid=3463249", "title": "68000er", "text": "68000er\n\n68000er ist ein ehemaliges aus einer Sonderheftreihe (4 Ausgabe, Erscheinungsweise quartalsweise) der \"Happy Computer\" im Jahr 1986 entstandenes Computermagazin. Benannt war das Magazin nach den Motorola-68000-Prozessoren, es behandelte ursprünglich sowohl die Atari ST-, die Amiga- und die Apple Macintosh-Computer. Ab Mitte 1987 wurde das Magazin eingestellt und thematisch auf drei neue Zeitschriften aufgeteilt – \"ST Magazin\", \"Amiga-Magazin und Macintosh-Magazin\".\n\nIm Jahr 1986 erschienen vier 68000'er-Sonderhefte der \"Happy Computer\"; ab der Ausgabe 01/1987 erschien das 68000'er-Magazin monatlich und eigenständig im Markt & Technik Verlag mit einem markanten silberfarbenen Titelcover. Ab Ausgabe 04/1988 wurde die Zeitschrift für einen kurzen Zeitraum in \"ST Magazin 68000'er\" und letztendlich in ST Magazin umbenannt. Damit wurde dem Umstand Rechnung getragen, dass durch das seit Mitte 1987 erschienene Amiga-Magazin der Amiga sowie Macintosh-Magazin aus und damit der Apple Macintosh dieser Computerzeitschrift herausgenommen und nur noch über den Atari ST berichtet wurde. Chefredakteur der Zeitschrift war \"Horst Brandl\".\n\n"}
{"id": "3463916", "url": "https://de.wikipedia.org/wiki?curid=3463916", "title": "Deep Thought (Schachcomputer)", "text": "Deep Thought (Schachcomputer)\n\nDeep Thought () war ein Schachcomputer, der von Feng-hsiung Hsu an der Carnegie Mellon University entwickelt wurde. Er wurde benannt nach dem fiktionalen Computer \"Deep Thought\" aus Douglas Adams’ Romanreihe \"Per Anhalter durch die Galaxis\". Die erste Version entstand im Mai 1988, eine Vorläuferversion trug den Namen \"ChipTest\". Die Weiterentwicklung des Programms durch IBM erfolgte unter dem Namen Deep Blue. \n\nDeep Thought gewann die nordamerikanische Computer-Schachmeisterschaft 1988 und die Computer-Schachweltmeisterschaft 1989. Schlagzeilen machte Deep Thought 1988 durch einen geteilten 1. Platz beim Open von Long Beach, Kalifornien, bei dem das Programm eine Partie gegen Großmeister Bent Larsen gewinnen konnte, der zu dieser Zeit auf Platz 96 der Weltrangliste stand. Es lief auf einer Sun 4 Workstation und konnte etwa 720.000 Stellungen pro Sekunde berechnen. Im Jahr 1989 verlor Deep Thought zwei Partien gegen Garri Kasparow sowie eine Fernschachpartie gegen Michael Valvo, konnte aber eine Turnierpartie gegen Robert Byrne gewinnen und besiegte den Internationalen Meister David Levy in einem Wettkampf deutlich mit 4:0.\nDie Benennung von multiprozessorfähigen Schachprogrammen folgte später dem Beispiel Deep Thoughts (so etwa Deep Blue, Deep Fritz und Deep Junior). \n"}
{"id": "3463995", "url": "https://de.wikipedia.org/wiki?curid=3463995", "title": "Milan (Computer)", "text": "Milan (Computer)\n\nMILAN ist der Name eines TOS-kompatiblen Computersystems, das als Nachfolger der Atari-ST-Computer geplant wurde. Es wurde von der MILAN-Computersystems GbR entwickelt. Das System wurde 1998 auf dem Markt eingeführt und wird heute nicht mehr hergestellt. Das Basismodell trägt die Bezeichnung MILAN 040, die auf die Zentraleinheit Motorola 68040 verweist. Der MILAN 040 konnte später mit einem Motorola 68060 nachgerüstet werden. Besonderheit des Systems ist, dass sich das TOS-Betriebssystem in einem Flash-ROM befindet. Hierdurch wird ein schnelles Hochfahren des Computers ermöglicht. Durch die Verwendung von Standard-PC-Komponenten sollten die Fertigungskosten gesenkt und die spätere Erweiterbarkeit verbessert werden.\n\nNachfolger des MILAN 040 sollte der MILAN 060 bzw. MILAN II werden. Ausgestattet mit einem Motorola 68060 als Hauptprozessor und USB-Schnittstellen sollte der MILAN II über Kaufhäuser und Fachhändler vertrieben werden. Ein Prototyp war bereits fertiggestellt. Die Serienproduktion blieb jedoch aus. Die Geschäftsführung der MILAN-Computersystems GbR begründete dies im Jahr 2000 damit, dass wichtige Bauteile der Hauptplatine nicht mehr verfügbar gewesen seien.\n\nDer MILAN 040 basiert auf einer Hauptplatine im AT-Format, sie hat drei ISA-Steckplätze, vier PCI-Steckplätze, vier Bänke für EDO-RAM, zwei IDE-Schnittstellen, eine Floppy-Schnittstelle, einen Anschluss für eine PS/2-Maus, Tastaturanschluss, eine parallele Schnittstelle und zwei serielle Schnittstellen.\n\nDas TOS-Betriebssystem trägt die Versionsnummer 4.08 und ist eine Weiterentwicklung des TOS des Atari Falcon 030.\n\nNeben dem TOS wurde das MILAN-MultiOS mitgeliefert. Dieses TOS-kompatible Betriebssystem wird zusätzlich auf Festplatte installiert und bietet dem Anwender präemptives Multitasking. MILAN-MultiOS ist eine spezielle Version von N.AES. Dieses wiederum basiert auf einem MiNT-Kernel. \n\nSpäter war auch das TOS-kompatible MagiC-Betriebssystem als weitere Alternative für den MILAN erhältlich.\n\n"}
{"id": "3466542", "url": "https://de.wikipedia.org/wiki?curid=3466542", "title": "Szene (Computergrafik)", "text": "Szene (Computergrafik)\n\nIn der 3D-Computergrafik ist eine Szene eine dreidimensionale Beschreibung von Objekten, Lichtquellen, Materialeigenschaften sowie der Position und Blickrichtung eines virtuellen Betrachters. Die Modellierung von Objekten innerhalb einer Szene ist Gegenstand der geometrischen Modellierung, die Berechnung eines Bildes aus einer Szene geschieht mittels Bildsynthese.\n\nIn einer Szene werden Objekte oft hierarchisch gruppiert und in einem Szenengraph zusammengefasst.\n\n3D-Grafikprogramme speichern Primitiven oft hierarchisch oder listenförmig in einem bestimmten Dateiformat ab. Besondere Formen sind \"Szenenbeschreibungssprachen\" (Scene Description Language, SDL), die sich nicht auf die bloße Auflistung von Primitiven beschränken, sondern eine 3D-Szene mit einer Art Programmiersprache beschreiben. Beispiele für SDLs sind die Virtual Reality Modeling Language (VRML) oder die von POV-Ray verwendete, C-ähnliche Sprache.\n"}
{"id": "3469037", "url": "https://de.wikipedia.org/wiki?curid=3469037", "title": "Cryptography API: Next Generation", "text": "Cryptography API: Next Generation\n\nCryptography API: Next Generation (CNG) ist Microsofts Kryptographieplattform ab Windows Vista, und ersetzt damit 2007 die CryptoAPI der vorherigen Versionen von Windows. Alle deren Funktionen werden weiterhin unterstützt. Mithilfe der CNG können Kryptographieanwendungen in hochsicherheitsrelevanten Umgebungen realisiert werden.\n\nDie API vereint mehrere Aufgabenpakete:\n\n\nAufgaben des CNG werden zum großen Teil im Microsoft Software Key Storage Provider (KSP) auditiert.\n\n"}
{"id": "3469472", "url": "https://de.wikipedia.org/wiki?curid=3469472", "title": "Lifted", "text": "Lifted\n\nLifted ist ein US-amerikanischer Animations-/Kurzfilm aus dem Jahr 2006, der im Kino als Vorfilm vor \"Ratatouille\" gezeigt wurde. Regie führte der siebenfache Oscar-Gewinner Gary Rydstrom. \"Lifted\" war bei der Oscarverleihung 2007 in der Kategorie \"Bester animierter Kurzfilm\" nominiert, erhielt aber keinen Preis.\n\nAm 29. Juni 2007 lief \"Lifted\" zusammen mit seinem Hauptfilm in den US-amerikanischen Kinos an. Der Kinostart in Deutschland, Österreich und der Schweiz war der 3. Oktober 2007. Uraufgeführt wurde der Film bereits am 14. Oktober 2006 auf dem Chicago International Film Festival.\n\nVor den Augen eines strengen Prüfers muss ein noch unerfahrenes Teenager-Alien die tausenden identischen Schalter der Raumschiffkonsole richtig bedienen. Die Testaufgabe besteht darin, einen ahnungslosen, schlafenden Farmer auf dem Planeten Erde mit einem Traktorstrahl zu erfassen und zielgenau aus dem Schlafzimmerfenster in das Raumschiff zu verfrachten. In der erdrückenden Stresssituation begeht das unsichere Alien einen Fehler nach dem anderen: Der Mensch stößt im Rahmen der Test-Entführung unzählige Male gegen die Wand, und schließlich kracht das ganze Raumschiff auf dessen Haus.\n\nIm Film werden mehrere bekannte Soundeffekte eingesetzt, so z. B. der Wilhelmsschrei, ein Systemklang aus Mac OS X oder ein Effekt aus den Star-Wars-Filmen.\n\n"}
{"id": "3470128", "url": "https://de.wikipedia.org/wiki?curid=3470128", "title": "Hyper-V", "text": "Hyper-V\n\nHyper-V ist eine Hypervisor-basierte Virtualisierungstechnik von Microsoft für Computer mit x64-fähigem x86-Prozessor. Erhältlich ist Hyper-V sowohl als fester Bestandteil der Serverbetriebssysteme (z. B. Windows Server 2016 Standard und Datacenter) als auch in Form der Standalone-Ausführung (z. B. Microsoft Hyper-V Server 2016). Darüber hinaus enthalten die Pro- und Enterprise-Editionen der Clientbetriebssysteme Windows 8 und Windows 10 die Hyper-V-Technologie. In all diesen Produkten ist Hyper-V entweder standardmäßig aktiv oder bei Bedarf als Komponente installierbar. Die Standalone-Ausführung \"Microsoft Hyper-V Server 2016\" ist kostenlos erhältlich und entspricht einer Windows Server 2016 Installation in der Core Edition ohne grafische Benutzeroberfläche und mit aktivierter Hyper-V Rolle.\n\n\nEinem Gastsystem können bis zu 64 Prozessoren und 1 Terabyte RAM zugewiesen werden.\n\nDer Hypervisor wird in zwei Varianten ausgeliefert: Als Serverrolle bzw. Betriebssystem-Feature (z. B. in Windows Server 2016 oder Windows 10) und als eigenständiges Produkt (z. B. Microsoft Hyper-V Server 2016). Letztgenannte Version ist kostenfrei, beinhaltet aber keine ggf. benötigten Lizenzen für das Gastsystem. Außerdem ist diese Version ausschließlich im Core-Betrieb verwendbar, was durch die Verwendung von PowerShell vereinfacht wird (PsHyper-V). Dies sind die einzigen Beschränkungen gegenüber der kostenpflichtigen Variante.\n\nHyper-V ist in vielen Szenarien effektiv einsetzbar – sowohl zur Virtualisierung ganzer Rechenzentren als auch von kleineren Umgebungen. Es kann darüber hinaus die komplette Netzwerkkonfiguration ohne Werkzeuge von Drittanbietern (engl.: Third-Party-Tools) (z. B. NIC-Teaming und VLAN-Konfiguration) durchgeführt werden, dazu bietet Hyper-V u. a. auch die Virtualisierung ganzer Switches (vSwitch). Für einen erweiterten Funktionsumfang sorgt System Center Virtual Machine Manager (SCVMM), welcher für die Verwaltung von VMs und Hosts u. a. basierend auf Hyper-V eingesetzt werden kann.\n\nMit der in Windows 8 und Windows 10 verfügbaren Version kann Hyper-V allerdings auch für Client-Virtualisierung gebraucht werden. Allerdings sollte beachtet werden, dass nach dem Aktivieren von Hyper-V das Root-Betriebssystem selbst in einer privilegierten virtuellen Maschine läuft und es z. B. bei latenzkritischen Echtzeitanwendungen Probleme geben kann. Deshalb ist es bei Hyper-V auch falsch, beim Root-OS von einem „Host-OS“ und bei den Kindsystemen von einem „Guest-OS“ zu sprechen, da beide (Root-OS und Kindsysteme) auf einer Ebene ausgeführt werden.\n\nMicrosoft unterstützt neben einigen Windows-Versionen auch offiziell einige Linuxdistributionen und FreeBSD.\n\n\nAb Windows 8 werden Windows XP Home und ältere Versionen nicht mehr im virtuellen Modus unterstützt. Eine Liste sämtlicher unterstützter Betriebssysteme gibt es bei Microsoft TechNet. Seit der Linux-Kernelversion 2.6.32 sind die Hyper-V Integration Components fester Bestandteil und können auch in anderen Linuxdistributionen einfach aktiviert werden. Ab CentOS 6.4 sind die Treiber für Hyper-V direkt in den Paketen der Distribution enthalten. Seit 2012 wird FreeBSD offiziell von Hyper-V unterstützt. Ab der Version 6.6 von RHEL und CentOS wird auch die Ausführung in einer Virtuellen Maschine der 2. Generation unterstützt.\n\n"}
{"id": "3470300", "url": "https://de.wikipedia.org/wiki?curid=3470300", "title": "TreeSize", "text": "TreeSize\n\nTreeSize ist eine Software-Produktfamilie, die dem Management des Festplattenplatzes dient (\"disc usage utilities\") und beim Aufräumen und Finden von Platzverschwendern und überflüssigen Dateien hilft. TreeSize läuft unter Windows Vista und 7 inklusive der 64-Bit-Versionen sowie unter Windows 8, 10, Server 2012 sowie Server 2016. Es gibt eine Free- und zwei Shareware-Editionen dieser Software.\n\nMit dem Windows-Explorer lassen sich die Ordnergrößen nicht strukturiert (nebeneinander oder in einem Tree Map) anzeigen. Die wichtigste Funktionalität von TreeSize, die allen Editionen gemein ist, besteht in der Ermittlung von Ordnergrößen in einem Verzeichniszweig oder Laufwerk, verbunden mit unterschiedlichen vergleichenden Darstellungs- und Reportfunktionen für die Ergebnisse.\n\nDie Professional- und Personal-Edition suchen ferner auch nach besonders großen, unbenutzten oder überflüssigen Dateien, wie zum Beispiel Temporärdateien oder den Dateien im Cache von Web-Browsern wie Internet Explorer, Mozilla Firefox oder Opera. Darüber hinaus können doppelte Dateien mittels MD5- oder SHA256-Prüfsumme gefunden und ebenfalls gelöscht oder durch Hardlinks ersetzt werden. TreeSize greift beim Scan von lokalen NTFS-Laufwerken auf die Master File Table zu und liest die dort gespeicherten Metadaten aus.\n\nTreeSize ermöglicht auch die Beobachtung der Entwicklung des Speicherplatzverbrauchs auf Festplatten. Dies geschieht wahlweise mit der Hilfe exportierter XML-Dateien oder durch den Vergleich sogenannter Schattenkopien, die durch die Windows-Systemwiederherstellung angelegt oder manuell erstellt werden können.\n\nDie erste Version von TreeSize wurde 1996 von Joachim Marder entwickelt, um die Einschränkungen von Tools wie dem Unix-Kommando \"du\" (\"disk usage\") bzw. dem Rechtsklick im damaligen Windows Explorer zu überwinden. Das Programm war von Anfang an geplant als GUI-basierte Version von \"du\" für Windows. Seitdem wird es stetig durch die von ihm gegründeten Firma JAM Software weiterentwickelt und hat mittlerweile die Versionsnummer 6.3 (Stand: September 2016) erreicht.\n\nTreeSize Free ist die uneingeschränkt kostenlose Variante des Programms. Sie verfügt über die Grundfunktionalität zum genauen Ermitteln von Ordnergrößen und integriert sich in den Windows-Explorer, wobei es dieselbe Code-Basis nutzt wie die umfangreicheren Shareware-Versionen.\n\nAnders als viele andere Speicherplatz-Manager scannt TreeSize Free ab Version 3.4 auch Mobilgeräte, die sich per Media Transfer Protocol mit dem Rechner verbinden. Android-Geräte können auch via WebDAV gescannt werden.\n\nDie auch als Shareware zur Verfügung stehende Personal-Edition umfasst sämtliche Funktionen von TreeSize Professional (ebenfalls Shareware), läuft allerdings nicht auf Windows-Serversystemen, unterstützt keine Windows-Domänen, kann nicht über Kommandozeilen-Parameter gesteuert werden und verfügt über weniger Export-Funktionen.\n\nSeit Oktober 2012 ist im Windows Store eine für Windows-8-Mobilgeräte konzipierte Freeware-Version Namens „TreeSize Touch“ erhältlich.\n\n"}
{"id": "3473765", "url": "https://de.wikipedia.org/wiki?curid=3473765", "title": "Read Only Domain Controller", "text": "Read Only Domain Controller\n\nDer Read only Domain Controller (kurz RODC) ist ein neuer Controllertyp im Domain-Controller-Konzept von Windows Server 2008. Er stellt einen Domain Controller ohne Schreibberechtigung und ohne sicherheitsrelevanten Daten dar. Er ist damit als Domain Controller in potentiell unsicheren Standorten nutzbar.\n\nDer RODC stellt einen vollwertigen Domain Controller dar. Allerdings kann von dem Controllertyp nur gelesen werden. Schreibende Zugriffe auf die Active Directory DS-Datenbank werden unterbunden.\n\nIn der AD DS-Datenbank werden standardmäßig keine sicherheitskritischen Daten und Attribute hinterlegt. (z. B. Kontokennwörter).\n\nDer Domain Controller kann nur unidirektional repliziert werden. Um Daten zu ändern, muss also der Domain Controller verändert werden, von welchem aus der RODC repliziert wird.\n\nFür die Administration des RODC können eigene Rollen vergeben werden, somit ist die administrative Wartung delegierbar. Zusätzlich verfügt der RODC über einen schreibgeschützten DNS.\n\nUm den RODC zusätzlich abzusichern, können Attribute von der Replikation ausgeschlossen werden.\n\nDer RODC ist konzipiert, um in Einheiten mit niedriger Sicherheitsstufe (z. B. Zweigstellen, kleine Außenbüros) eingesetzt zu werden. Durch das Rollenmodell und den verringerten Administrationsaufwand gegenüber einem vollwertigen Domain Controller muss deshalb kein oder wenig IT-Know-how vor Ort gebunden werden. Durch das physische Vorhandensein eines Domain Controller vor Ort bei über WAN angebundenen Standorten wird die Zugriffsgeschwindigkeit auf Ressourcen (z. B. eine Netzwerk-Freigabe für Dateien) verbessert.\n\nEin weiteres Szenario für den Einsatz ist eine Serversoftware, die am physisch identischen Ort einen Domain Controller benötigt. Auch hier wird der Mehrwert bei der niedrigeren Administrationsarbeit und der erhöhten Sicherheit angesetzt.\n"}
{"id": "3475433", "url": "https://de.wikipedia.org/wiki?curid=3475433", "title": "Microsoft Internet Information Services 7", "text": "Microsoft Internet Information Services 7\n\nInternet Information Services 7.0 ist die Implementierung von Internet Information Services in Windows Server 2008 und Vista, in Windows Server 2008 R2 und Windows 7 kommt die leicht überarbeitete Version 7.5 zum Einsatz. Er dient als Webserver. Im Server 2008 ist der IIS 7.0 als Rolle implementiert. Bei Vista ist der Server mit unterschiedlichen Einschränkungen in der Business-, Ultimate- und Home Premium-Variante mitgeliefert. \n\nDer IIS 7.0 ist modular in einzelne Komponenten aufgeteilt. Die Architektur und Funktionsweise unterscheidet sich grundlegend von der Vorgängerversion.\n\nAls Kernaufgabe eines Webservers lauscht der Listener auf eingehende Verbindungen. Dabei können die gängigen Protokolle standardmäßig bereitgestellt werden. Zusätzlich können WCF-connections verwaltet werden.\n\nDieser überwacht HTTP-Anfragen. Diese Funktion ist nativ im Betriebssystem verankert und wird vom IIS adaptiert.\n\nDer Stack managed dabei das Caching, die Verteilung der Anfragen auf die einzelnen Threads und führt diverse Sicherheitsfunktionen des Serversystems aus.\n\nDer www-service unterscheidet sich grundlegend von der Funktionsweise im IIS 6.0. Er verwaltet nur noch die Konfiguration des HTTP-Listener Stacks und ist für das Routing an den WAS verantwortlich.\n\nDer WAS verwaltet die einzelnen Arbeitsthreads und die Applikationspools des IIS. Er übernimmt dabei die Aufgaben des www-service im IIS 6.0. Zusätzlich werden durch den WAS auch die Verbindungen welche nicht auf dem HTTP-Stack aufsetzen verwaltet.\n\nAlle Funktionen können durch die MMC verwaltet werden. Die Oberfläche und Menüstruktur für die Konfigurationsmöglichkeiten wurde dabei komplett neu strukturiert gegenüber früheren Version des IIS. Alle Funktionen sind zudem als Features einzeln installierbar und zentral konfigurierbar.\n\nAls Kernfunktionalität ist die Bereitstellung von Webseiten anzusehen. Der IIS verwaltet dabei die statischen Inhalte und die Auslieferung. Ebenso wird das Fehlermanagement und die Grundkonfiguration durch die Kernmodule bereitgestellt.\n\nDer IIS unterstützt die Verwendung von dynamisch erstellten Webseiten. Dabei wird das komplette Spektrum der gängigen Technologien abgedeckt. Direkt unterstützt der IIS .NET für Anwendungen und ASP.NET für webseitebasierte Anwendungen. Ebenso wird CGI und alle ISAPI-basierten Plugins unterstützt. Server Side Includes lassen sich ebenso nativ einbinden. Skriptsprachen wie PHP sind implementierbar (zum Beispiel cgi oder isapi).\n\nDer IIS stellt unterschiedliche Protokollfunktionen zur Verfügung. Diese lassen eine Auswertung der Performance, des Systemzustands, der Userlogs und der frei definierbaren Logs zu.\n\nDer IIS unterstützt die Erstellung eines FTP-Servers.\n\nUm eine sichere Anmeldung für den Zugriff zu gewährleisten, werden unterschiedliche Authentifizierungsmethoden angeboten:\n"}
{"id": "3476480", "url": "https://de.wikipedia.org/wiki?curid=3476480", "title": "Think Different", "text": "Think Different\n\nThink different („Denke das Andere“) war der Slogan einer Werbekampagne von Apple aus dem Jahr 1997, die von Peter Economides in der Niederlassung der Werbeagentur TBWA in Los Angeles erstellt wurde. Zur Kampagne gehörte ein bekannt gewordener Werbespot, verschiedene Veröffentlichungen in Printmedien und in der Fernsehwerbung. Die Kampagne lief bis 2002.\n\nDer zentrale Text der Kampagne lautete:\n\nDer Wettbewerber IBM bewarb seine Personal Computer mit dem Slogan „Think!“, welcher ursprünglich vom ehemaligen IBM-Vorstandsvorsitzenden Thomas J. Watson um 1910 während seiner Tätigkeit bei der National Cash Register Company (NCR) geprägt wurde.\n\nDie Aussage „Think different“ wird von vielen, auch Muttersprachlern, als grammatikalisch inkorrekt angesehen – als Imperativ mit Adverb müsste es „think differently“ heißen. Gemeint ist hier jedoch „Think different“ in ähnlicher Weise wie „Think victory“ oder „Think profit“, bei denen ein Substantiv folgt, wie Steve Jobs in seiner Biografie betont. In das Deutsche übersetzt bedeutet es somit „Denke das Andere“ und nicht, wie vermutet, „denke anders“. Für Steve Jobs war der Unterschied bedeutsam – die Kampagne knüpft hier an das Motiv der Gegenkultur aus Apples Gründerzeit an.\n\nTBWA hatte für Apple schon den bekannten Werbespot „1984“ erstellt, mit welchem der Macintosh eingeführt wurde. Nach dem einstweiligen Abgang von Steve Jobs 1985 bis zu seiner Rückkehr im Dezember 1996 hatte TBWA keine Aufträge von Apple erhalten.\n\nAm 3. August 1997 stellte TBWA den Entwurf der Think-Different-Kampagne Jobs vor, in welcher ursprünglich Mitarbeiter von Dreamworks gezeigt werden sollten, wie sie an ihren Macs arbeiten. Jobs schlug vor, statt der Mitarbeiter Schwarzweiß-Porträts von bekannten Persönlichkeiten zu verwenden. Jobs kümmerte sich persönlich darum, dass Persönlichkeiten wie Joan Baez, eine Ex-Freundin von Jobs, oder Yoko Ono, eine ehemalige Nachbarin von Jobs, in die Verwendung ihrer Porträts bei dieser Werbekampagne einwilligten und dass Richard Dreyfuss den Text sprach. Jobs schlug auch vor, dass das beworbene Produkt in der gesamten Werbekampagne nicht erscheinen solle.\n\nIm Rahmen der Kampagne wurden Fernseh-Werbespots, Plakate und Veröffentlichungen in Printmedien produziert, welche unter anderem die folgenden Personen zeigten: Albert Einstein, Bob Dylan, Martin Luther King, Jr., Richard Branson, Miles Davis, John Lennon, Richard Buckminster Fuller, Thomas Edison, Muhammad Ali, Ted Turner, Maria Callas, Mahatma Gandhi, Amelia Earhart, Alfred Hitchcock, Martha Graham, Jim Henson (mit Kermit), Jerry Seinfeld (in einer gekürzten Fassung zum Finale der Serie Seinfeld), Frank Lloyd Wright und Pablo Picasso.\n\nDer Slogan „Think different“ wurde auch nach dem Abschluss der Werbekampagne von Apple immer wieder verwendet.\n\nApple ehrte Persönlichkeiten, welche ursprünglich nicht Bestandteil der Kampagne waren, im Internet auf ihrer Startseite im Stil von „Think Different“. Im Einzelnen waren dies:\n\n"}
{"id": "3477166", "url": "https://de.wikipedia.org/wiki?curid=3477166", "title": "Netbook", "text": "Netbook\n\nAls Netbook wird eine Klasse von Notebooks bezeichnet, die besonders auf Mobilität und einen niedrigen Preis ausgelegt sind, und die dafür bei der Leistungsfähigkeit und Ausstattung deutliche Kompromisse eingehen; obwohl sie insbesondere als mobile Internet-Clients konzipiert sind, verfügen nur wenige Modelle über ein integriertes Mobilfunk-Modem. Netbooks waren von 2007 bis ungefähr 2011 populär, Tablets und Convertibles lösten die Netbooks weitgehend ab. \n\nNetbooks wurden entworfen, um mit Browserfunktionen im Intranet oder im Web zu surfen oder Webapplikationen zu verwenden. Sie sind leistungsschwächer als konventionelle Notebooks und Subnotebooks, z. B. mit kleinen Festplatten und schwächeren Prozessoren. Dadurch konnten Preisvorteile gegenüber den teureren Subnotebooks, welche eine ähnliche Displaygröße besitzen, erzielt werden.\n\nDas Display von Netbooks hat in der Regel eine Bilddiagonale von 18 bis 30 cm (7 bis 11,6″), ein optisches Laufwerk ist grundsätzlich nicht integriert. In Abgrenzung zu den UMPC haben Netbooks nur selten einen Touchscreen, dafür aber eine vollwertige QWERTZ-Tastatur und Touchpad.\n\nAls Betriebssystem kamen ursprünglich verschiedene Linux-Distributionen und später Microsoft Windows XP zum Einsatz, denn die Rechenleistung war bei den meisten Netbooks für Windows Vista nicht ausreichend. Mit dem Verkaufsstopp von Windows XP am 22. Oktober 2010 war auch die Auslieferung von neuen Netbooks mit XP nicht mehr möglich, sodass fortan Netbooks überwiegend mit dem verhältnismäßig ressourcenschonenden \"Windows 7 Starter\" ausgestattet waren.\n\nDa jedoch das Netbook von den meisten Kunden einfach nur als günstiges Notebook wahrgenommen und verwendet wurde, existierte schon 2009 ein Trend zu stärkeren Komponenten, um den Kundenerwartungen entgegenzukommen. Mit dieser neuen Geräteklasse verschwamm die Grenze zu den Subnotebooks. Mit neueren, leistungsstärkeren Modellen mit mehr als 2 GB RAM und einem hochauflösenden 12,1″-Display wurde zu jener Zeit auch \"Windows 7 Home Premium\" verwendet.\n\nIn der zweiten Hälfte der 1990er Jahre wurden Notebooks unterhalb der Standardgröße als „Mini-Notebooks“ bezeichnet (siehe z. B. c’t 15/1998: „Bewegung im Mini-Notebook-Markt“). Der erste in Deutschland bekanntere Typ war das \"Libretto\" von Toshiba, das bei Veröffentlichung über 5000 DM kostete. In Ostasien gab es 1998/99 einen Trend zu Mini-Notebooks; diese konnten sich in Europa nicht durchsetzen. Die Kategorie verschwand daraufhin – mit Ausnahme einiger Acer-, Toshiba- und Sony-Modelle – für einige Jahre vom Markt. Diese Gerätekategorie, nach der Preis- und Leistungsdefinition nach wie vor teuer und leistungsfähig, wurden nach wie vor als Subnotebook vermarktet.\n\nDie Idee, ein sehr kleines und mit nur begrenzter Leistung ausgestattetes Notebook anzubieten, gab es bereits lange vor den Netbooks, derartige Geräte spielten aber am Markt nur eine untergeordnete Rolle. Ein Beispiel hierfür ist ein Gerät des Herstellers Psion aus dem Jahr 2000, das erstmals die Bezeichnung „Netbook“ verwendete.\n\nWeitere in Bezug auf ihre geringe Größe und eingeschränkte Leistung mit heutigen Netbooks vergleichbare Geräte waren Intels Classmate PC und der OLPC XO-1. Beide Geräte sind zunächst als günstige und robuste Notebooks für den Einsatz durch Schüler in Schwellenländern gedacht; ein Verkauf an private Kunden war zunächst nicht vorgesehen. Die Geräte sind inzwischen jedoch auch in Europa und den USA erhältlich; zum Teil jedoch nur unter der Bedingung, ein weiteres zu spenden.\n\nAuf einer sehr ähnlichen Basis, jedoch für erwachsene Privatkunden gedacht, entstanden schließlich die Ende der 2000er-Jahre als Netbook bezeichneten Geräte. Das erste Netbook dieser Art war der Asus Eee PC 700, der im Oktober 2007 in Taiwan eingeführt wurde und seit Januar 2008 in Europa erhältlich ist. Der Hersteller hatte anfangs aufgrund hoher Nachfrage mit Verfügbarkeitsproblemen zu kämpfen. Anschließend stellten diverse Anbietern ähnliche Geräte vor. Im ersten Halbjahr 2009 wurden weltweit etwa 13,5 Millionen Geräte verkauft.\n\nDie Bezeichnung Netbook wurde von Chip-Hersteller Intel im Februar 2008 benutzt, obwohl Psion sein „Psion netBook“ bereits seit dem Jahr 2000 vermarktete. Außerdem liegt eine deutsche Wortmarkeneintragung vom 2. Oktober 2008 vor. Psion Teklogix beansprucht die Bezeichnung „Netbook“ für sich. In einem Brief an Webseiten und Portale, die den Begriff in ihrer Berichterstattung verwenden, wies das Unternehmen an, dies künftig zu unterlassen – das Recht an der Marke läge bei ihnen. Anfang Februar 2009 erwirkte Psion die Sperrung des Begriffes bei Google AdWords. Dell und Intel hatten eine Klage zur Löschung des Begriffs Netbook als Marke erhoben, da es sich ihrer Meinung nach um einen Gattungsbegriff handelt. Am 1. Juni 2009 gab Psion auf seiner Website bekannt, dass es jedem Unternehmen erlaubt sei, den Begriff Netbook zu verwenden. Das Unternehmen habe sich im Rechtsstreit mit Intel „friedlich geeinigt“, heißt es in der Erklärung.\n\nNach einer Studie des Marktforschungsanbieters Gartner machten Netbooks bereits im zweiten Quartal 2008 3 % des weltweiten PC-Marktes aus, bevor viele der großen Anbieter überhaupt derartige Geräte anboten. Im dritten Quartal 2008 stieg dieser Anteil im Wirtschaftsraum Europa, Nahost und Afrika (EMEA) zusammen auf 10 %, in den USA auf 5 %. Eine andere Studie berichtet von 7,7 % für EMEA und nennt mit 2,2 Mio. Geräten erstmals konkrete Zahlen.\n\nVon Analysten wurde das Netbook-Konzept als eine Herausforderung für die marktdominierende Position des Betriebssystem-Herstellers Microsoft gewertet, da durch die Konzentration auf grundlegende Funktionen und Internetnutzung das Betriebssystem unwichtiger werden könnte und somit auch die Weiterentwicklung von benutzerfreundlicheren Linuxsystemen von breiterem kommerziellen Interesse sein könnte. Das zeigte sich beispielsweise in den Bemühungen von Intel um das Moblin-Projekt. Diese Vision hat sich bis 2012 jedoch nicht bewahrheitet: ursprünglich eine reine Linux-Domäne, erhöhte sich bei den Netbook-Verkäufen mit vorinstallierten Betriebssystem der Windows-Anteil, mit dem verfügbar werden von Windows XP als Option 2008, bis 2009 auf über 90 %. Netbooks mit vorinstallierten Linux-Distributionen hatten mit überproportionalen Rückgabe- und Umtauschquoten durch die Käufer zu kämpfen.\n\nNach der Einführung der Tablet-PCs fiel das Absatzvolumen für Netbooks deutlich hinter die Erwartungen der Hersteller zurück. Microsoft stellte einen 40%igen Rückgang der Netbook-Verkäufe im ersten Quartal 2011 im Vergleich zum Vorjahresquartal fest. Große Netbook Hersteller wie Acer und Asus haben die Einstellung der Produktion bekanntgegeben.\nDie Wahl des Betriebssystems für Netbooks unterschied sich deutlich von der für andere PCs, da die Leistung begrenzt ist und Lizenzkosten relativ stärker ins Gewicht fallen.\n\nZur Markteinführung der Netbooks schied das zu dieser Zeit aktuelle Windows Vista als Betriebssystem aus, da die Leistung typischer Netbooks nur geringfügig über den Minimalanforderungen des Systems lag und vor allem die Lizenzkosten die Geräte deutlich verteuert hätten. In Europa waren die meisten Geräte noch für längere Zeit mit dem eigentlich eingestellten Vorgänger Windows XP ausgestattet, dessen Verfügbarkeit speziell für \"Ultra Low Cost PCs\" um zwei Jahre zunächst bis Juni 2010 verlängert worden war. Die Lizenz wurde je nach Leistung des Systems weiter verbilligt und die zulässige Ausstattung allerdings nach oben begrenzt. Maximal 1 GB Arbeitsspeicher waren zulässig; darüber sollte nach Microsofts Vorstellungen Windows Vista verwendet werden. De facto wurde diese Begrenzung jedoch entweder ignoriert oder Systeme mit mehr als 1 GB Arbeitsspeicher gar nicht mit Windows angeboten. Netbooks durften von den OEM-Herstellern nur bis 22. Oktober 2010 mit Windows XP vorinstalliert werden, hiernach wurde die neuere Betriebssystemversion \"Windows 7 Starter\" lizenziert.\n\nViele Netbooks wurden mit verschiedenen Linux-Versionen ausgeliefert; der Eee PC beispielsweise mit einer angepassten Version der Linux-Distribution Xandros, Des Weiteren wurden auch das Fedora-Derivat Linpus (beim Acer Aspire One) und SUSE Linux Enterprise Desktop 10 von Novell eingesetzt. Die Geräte von Dell und Toshiba benutzten eine angepasste Ubuntu-Version. Im Zusammenhang mit der Vorstellung des Atom-Prozessors richtete Intel unter dem Namen \"Moblin\" ein Entwicklerportal ein und stellte Linux-Kernel-Patches bereit, die über die Stromsparfunktionen der Plattform längere Akkulaufzeiten ermöglichen.\n\nEin weiteres linuxbasierendes Betriebssystem, das zuvor in Mobiltelefonen und Tablet-PCs eingesetzte Google Android, wurde im Februar 2009 von Google für den Einsatz in Netbooks angekündigt. Doch stattdessen lancierte Google Chromebooks.\n\nDie Hardwareausstattung von verschiedenen Netbooks unterschied sich nur wenig. Ein im Vergleich zu normalen Notebooks sehr langsamer Prozessor wurde mit einem älteren Notebook-Chipsatz mit integrierter Grafikeinheit verbunden. Als Datenspeicher kamen teilweise Solid State Disks, zum größten Teil aber 2,5″-Festplatten zum Einsatz. Die verwendeten Displays haben zumeist eine (eher ungewöhnliche) Auflösung von 1024 × 600 Pixeln; einige Netbooks hatten HD-ready-Displays.\n\nNetbooks, die von Mobilfunkanbietern subventioniert mit Vertrag angeboten worden sind, beinhalteten generell ein eingebautes Mobilfunkmodem mit entsprechender SIM-Karte. Üblich waren weiterhin teilweise Bluetooth und ein integriertes WLAN-Modem.\n\nDer Asus Eee 700 bzw. 701 basierte noch auf einem Celeron-ULV-Prozessor, der von den ursprünglichen 800 oder 900 MHz nochmals um etwa 1/3 verlangsamt wird, um so den Energieverbrauch zu senken. Spätere Modelle verwendeten dann Prozessoren, die speziell für diese Geräte und die in den Anforderungen sehr ähnlichem UMPCs gedacht waren.\n\nDie meisten Netbooks verwendeten hierbei die Intel-Atom-Prozessoren, die zwischenzeitlich mehrfach verbessert wurden. Teilweise wurden diese aufgrund der sehr niedrigen Leistung der integrierten Grafikeinheit mit Nvidia-ION-Grafikkernen kombiniert.\n\nSpäter wurden in High-End-Netbooks aber auch speziell dafür konzipierte, stromsparende Core-i-Prozessoren eingebaut. Sie waren zwar teurer, erzielten aber eine höhere Leistung und boten meist weitere Funktionen wie zum Beispiel Turbo Boost oder integrierte Grafikprozessoren.\n\nVon AMD wurden Netbooks anfangs eher abgelehnt, eingesetzt wurden vereinzelt die veralteten Geode-Prozessoren oder verschiedene AMD-Neo-Versionen. Anfang 2011 stand mit AMD Fusion jedoch eine für diesen Bereich geeignete Architektur zur Verfügung, die sich auch am Markt verbreitete.\n\nEinige Anbieter, unter anderem Hewlett-Packard, setzten auch auf einen C7-Prozessor von VIA, der als preisgünstiger, aber deutlich langsamer als die Intel-CPUs galt. Als Nachfolger erschien hier der pinkompatible VIA Nano, der zumindest mit Intel gleichziehen sollte. Um diesen herum haben VIA und Nvidia eine komplette Plattform für Netbooks und UMPCs entwickelt.\n\nEbenfalls fanden sich Netbooks auf dem Markt, die einen besonders stromsparenden x86 SoC verwendeten. Gleichzeitig fanden sich in besonders günstigen Geräten auch Prozessoren, die die MIPS-Architektur verwendeten. \n\nDurch die Optimierung auf geringen Stromverbrauch, günstigen Preis und kompakte Bauform erscheinen übliche Notebook-Festplatten für Netbooks teilweise überdimensioniert. Aus diesem Grund verfügten die ersten Geräte dieser Klasse ausschließlich über Solid-State-Drives (SSD) mit einer Kapazität von wenigen Gigabyte. Festplatten wurden in diesen Größen nicht mehr angeboten und würden in den Geräten wesentlich mehr Platz verbrauchen. Mit dem Aufkommen größerer Netbooks mit einer Bilddiagonalen von 9 bis 10″ wurden jedoch zunehmend konventionelle 2,5″-Festplatten verbaut – üblicherweise mit 160, 250, 320 oder 500 GB –, die einige Vorteile der Solid-State-Drives wie beispielsweise die bessere Stoßfestigkeit nicht mehr boten, dafür aber wesentlich mehr Datenspeicher für den gleichen Preis boten.\n\nVorgesehen für die mobile Nutzung war der WLAN-Standard. Auch Bluetooth-Module waren vergleichsweise häufig eingebaut. Eher selten (Beispiel: Samsung NC10 BH) waren jedoch integrierte UMTS-Module zu finden, die für den mobilen Internetzugang außerhalb von Hot Spots benötigt werden. Durch das eingebaute UMTS-Modul wurde keine USB-Schnittstelle für einen UMTS-Stick benötigt.\n\nDie Bezeichnung \"Nettop\" wird für vergleichbar ausgestattete Desktop-PCs verwendet, wie sie 2008 z. B. in Form der \"Asus EeeBox b202\" oder dem \"MSI Wind PC\" angeboten waren. Nettops können entweder als Komplettsystem oder auch in Einzelteilen gekauft werden. Mainboards für Nettops sind meist im Mini-ITX-Format mit aufgelötetem Prozessor und bereits aufgesetztem Kühler erhältlich.\n\nDie Geräte basierten, abhängig vom Gehäuse und Mainboard, häufig auf den aus Note- und Netbooks bekannten Bauteilen, wie etwa 2,5″-Festplatten oder SODIMM-Speichermodulen, sowie auf verschiedenen Varianten des Intel-Atom- und Celeron-M-Prozessors beziehungsweise deren Nachfolgern. Zahlreiche Firmen wie Intel bieten Mini-ITX-Mainboards an, die mit einem fest aufgelöteten Atom-Prozessor bestückt sind.\n\nNettops sind in der Regel nur begrenzt aufrüstbar (Festplatte, RAM) und verfügen wie Netbooks lediglich über eine \"ausreichende\" Leistung. Dafür sind sie jedoch sehr preiswert und zeichnen sich durch einen sehr geringen Stromverbrauch aus.\n\nBei Modellen um 2010 konnte durch die Weiterentwicklung des Chipsatzes der Stromverbrauch weiter gesenkt werden. Ein voll ausgestatteter Nettop, basierend auf der Intel-Hauptplatine \"D510MO\", die mit dem Atom-D510-Prozessor ausgestattet war, benötigte selbst unter Last nur 27 Watt. Nettops mit dem weit verbreiteten Vorgänger (Intel Atom 330) benötigten im Schnitt noch 35 Watt unter Last.\n\nDes Weiteren arbeiten Nettops sehr leise oder bei passiv gekühlten Komponenten in Kombination mit einem Solid-State-Drive völlig geräuschlos. Beispielsweise verursachte die 2008 erschienene \"EeeBox b202\" von Asus unter Volllast lediglich einen Geräuschpegel von 26 dB. Darüber hinaus sind bei diesen Geräten spezielle Gehäuseformen mit einem Volumen von maximal zwei Litern marktüblich. Einige Varianten lassen sich sogar an die Rückwand eines Monitors montieren oder sind direkt in das Monitorgehäuse integriert.\n"}
{"id": "3478078", "url": "https://de.wikipedia.org/wiki?curid=3478078", "title": "FxCop", "text": "FxCop\n\nFxCop ist ein Analysewerkzeug für .NET. Es prüft den CIL-Code und den Aufruf-Multigraphen der einzelnen Routinen nach Verstößen gegen die Entwurfsrichtlinien von Microsoft und auf potentielle Schwachpunkte. \n\nDie Software ist prinzipiell konsolenorientiert. Allerdings lässt sich FxCop auch als externes Werkzeug nach dem Buildvorgang aufrufen. Ferner gibt es für Visual Studio 2008 und Visual Studio 2005 ein Plugin. In Visual Studio Team System und SharpDevelop ist es integriert. Es existiert ebenfalls eine GUI für den Standalone-Betrieb.\n\nFxCop analysiert primär sieben Bereiche:\n\nFxCop analysiert die geladenen Assemblys. Dabei wird der CIL-Code überprüft und der Aufrufgraph analysiert. FxCop verwendet dabei Richtlinien, nach denen die einzelnen Kriterien gegengeprüft werden. In der Benutzerkonfiguration kann angegeben werden, wie eine Richtlinie gehandhabt wird. Zusätzlich lässt sich über Präprozessordirektiven die Überprüfung durch den Quellcode steuern. So können z. B. beanstandete, aber benötigte Fehler, z. B. ein bewusster und unumgänglicher Designverstoß von der Prüfung ausgenommen werden, ohne generell die Überprüfung gegen diese Richtlinie zu unterbinden.\n\nFxCop ist sowohl kommandozeilenbasiert als auch über eine GUI steuerbar. Eine Integration in Visual Studio ist möglich. FxCop lässt sich ebenso durch den Team Foundation Server auswerten. Dabei kann z. B. bei Verstoß gegen die Richtlinien ein Checkin der Software unterbunden werden. Policyverstöße lassen sich auch durch Metriken auswerten, durch die Reports in TFS integrieren und fließen somit in die Qualitätsbeurteilung einer Software mit ein.\n\nFxCop benötigt für die Ausführung .NET 2.0, es unterstützt aber die Überprüfung aller Versionen von 1.1 bis 3.5.\n"}
{"id": "3484504", "url": "https://de.wikipedia.org/wiki?curid=3484504", "title": "PitBull LX", "text": "PitBull LX\n\nPitBull ist eine sichere Applikations-Umgebung entwickelt von der Argus Systems Group. Ende des Jahres 2012 wurde das Unternehmen aufgekauft von General Dynamics, einem der größten Rüstungskonzerne der USA. \nBei der Version PitBull LX handelt es sich um einen modifizierten Linux oder Solaris Kernel. Die Standard-Datei-Berechtigungen (DAC), die nur auf Benutzerebene greifen, werden auf Prozessebene erweitert. Das Rechtemodell entspricht dem Lattice- (bzw. compartment-) Modell: Ein Zugriffsrecht (Lesen, Schreiben, Ausführen) auf eine Datei wird nur gewährt, wenn der zugreifende Prozess \"alle\" dafür notwendigen Zugriffsklassifizierungen (hier Domain genannt) bereithält.\n\nDarüber hinaus wird auch die Möglichkeit implementiert, Netzwerkzugriffe auf Prozessebene einzuschränken (sogenannte Netrules). Hier greifen ähnliche Filtermechanismen wie bei Firewalls zwischen getrennten Systemen. Außerdem kann die Interprozesskommunikation über Unix-Sockets ebenfalls eingeschränkt werden.\n\nDie Weiterentwicklung von PitBull LX ist zugunsten von PitBull Foundation eingestellt worden, da PitBull Foundation neben den Merkmalen von PitBull LX auch die insbesondere die im Militär- und Behördenumfeld notwendigen, hierarchischen Berechtigungsstrukturen (Top Secret, Secret, Confidential, Unclassified, ...) zwischen verschiedenen, zusammenarbeitenden Ländern, Organisationen oder Partnerunternehmen Netzwerkübergreifend unter Nutzung des CIPSO IP-Header-Standards ermöglicht.\n\nDie erweiterten Rechtestrukturen für Dateien werden zusätzlich zu den standard DAC Berechtigungen in der Inode-Table des ext3-Dateisystems (Linux-Version) abgelegt. Ein zugreifender Prozess muss für eine Operation (Lesen, Schreiben, Ausführen) die erforderlichen Berechtigungen beider Rechtesysteme (PitBull LX und DAC) besitzen.\n\nDie Netrules werden über eine Konfigurationsdatei (meist /etc/argus/netrules) in das PitBull LX Kernel-Modul geladen. Kriterien für Regeln sind Quell- und Ziel-IP-Adresse, Port, Protokoll (TCP, UDP, ICMP, …) sowie die verwendete Netzwerkschnittstelle.\n\nDie Zugriffsklassifizierungen (Domänen) erhalten Prozesse im Fall ausführbarer Binärdateien beim Starten über weitere in den Inode-Tabellen abgelegte Strukturen. Für Scripte gibt es ein wrapper-Programm (lxexec), welches die Domänen über Parameterangaben an das auszuführende Script weitervererbt.\n\nFür die Vergabe der Domänen an die Ausführungsumgebung bei der Nutzeranmeldungen an der Konsole wird ein PAM-Modul bereitgestellt. Die differenzierte Anpassung der Rechte erfolgt in der Datei /etc/argus/users, wobei hier zwischen einer lokalen Anmeldung und einer entfernten Anmeldung (beispielsweise via ssh) unterschieden werden kann.\n\nDurch die Modifikation ist es möglich, auch als root-User ausgeführte Prozesse einzuschränken, so dass jeder Prozess für sich selbst so ausgeführt wird, als wenn er sich in einer eigenen virtuellen Maschine befinden würde. Somit wird der Schaden im Fall eines erfolgreichen Hacks deutlich reduziert.\nDies wird dadurch erreicht, dass der Zugriff auf Dateien, die sich auf dem gleichen System befinden, ähnlich wie auch durch das chroot-Konzept eingeschränkt wird. Darüber hinaus wird durch die Netrules auch die Kommunikation (IP-Netzwerk und Unix-Sockets) zwischen den Diensten ähnlich einer Firewall eingeschränkt, so dass der Angreifer auch über diesen Umweg keinen weiteren Zugriff erhält.\n\nAuch lassen sich durch Mandatory Access Control sehr viel komplexere Berechtigungsstrukturen realisieren, wie sie oftmals im Militärbereich oder großen Behörden eingesetzt werden.\n\n\n\n\n"}
{"id": "3493223", "url": "https://de.wikipedia.org/wiki?curid=3493223", "title": "WALL·E – Der Letzte räumt die Erde auf", "text": "WALL·E – Der Letzte räumt die Erde auf\n\nWALL·E – Der Letzte räumt die Erde auf (auf Filmplakaten \"WALL·E\") aus dem Jahr 2008 ist der neunte computeranimierte Kinofilm der Pixar Animation Studios und der Walt Disney Company. Er lief am 27. Juni 2008 in den US-amerikanischen Kinos an. Der deutschsprachige Kinostart in Deutschland, Österreich und in der Deutschschweiz war am 25. September 2008.\n\n\"WALL·E\" ist ein Akronym für \"Waste Allocation Load Lifter – Earth-Class\" (deutsch: \"„Müllordner und Lastenheber – Erdklasse“\").\n\nIn einer fernen Zukunft ist die Erde durch Umweltverschmutzung aufgrund des gesteigerten Massenkonsums und der daraus resultierenden Vermüllung unbewohnbar geworden. Die Menschen haben die Erde daher bereits vor Jahrhunderten in mehreren vollständig autarken Raumschiffen, u. a. der \"Axiom\" des Supra-Konsumerkonzerns \"Buy n Large\" (\"BNL\"), verlassen. Ein Heer von Müllrobotern des Typs WALL·E wurde zurückgelassen, um aufzuräumen. Eigentlich war für diese Aktion ein Zeitraum von fünf Jahren angesetzt, doch im Jahre 2805, nach 700 Jahren des Müllsammelns und der Müllverarbeitung, ist der Protagonist WALL·E der einzige noch funktionsfähige dieser Aufräumroboter und die Erde noch immer eine desolate Müllhalde. WALL·E hat über die Jahrhunderte eine Weiterentwicklung durchlaufen und ein eigenes Bewusstsein entwickelt; dies geht so weit, dass er Ersatzteile für sich selbst sucht und besondere Gegenstände, die sein Interesse erwecken, in seine Privatsammlung aufnimmt. Sein einziger Freund ist eine Kakerlake. Von einer Kopie des Filmes \"Hello, Dolly!\" lernt er, dass es die Bestimmung aller hochentwickelten Lebewesen ist, sich zu verlieben.\n\nEines Tages findet WALL·E in einem versteckten Winkel ein kleines Pflänzchen und nimmt dieses in seine Sammlung auf. Bald darauf landet ein Raumtransporter auf der Erde und setzt den Roboter EVE (\"Extraterrestrial Vegetation Evaluator\") aus. WALL·E verliebt sich in EVE, die jedoch zunächst kein Interesse an ihm zeigt. Sie hat den Auftrag, auf der Erde nach photosynthesefähigem, organischem Leben zu suchen. Sie spürt dieses auch auf in Form des Pflänzchens, das WALL·E entdeckt hat. Nachdem sie die Pflanze gefunden hat, stellt sie sie in ein Fach in ihrem Inneren, deaktiviert sich und wird kurz darauf vom Raumtransporter abgeholt. WALL·E klammert sich an die Außenhülle des Schiffs und gelangt so mit ihr ins Raumschiff \"Axiom\", dessen menschliche Passagiere sich nach 700 Jahren Automatisierung, medialer Dauerberieselung und geringer Gravitation zu fettleibigen, degenerierten Wesen entwickelt haben.\n\nAls der Kapitän des Raumschiffes von EVE die Pflanze in Empfang nehmen will, ist diese verschwunden. Der Schiffscomputer der \"Axiom\", Otto (im englischen Original AUTO), schickt daraufhin EVE zur Reparatur. WALL·E glaubt, EVE werde deaktiviert, und unternimmt einen Rettungsversuch, der allerdings in einem Aufruhr endet. Nachdem der Kapitän inzwischen WALL·Es Herkunft herausgefunden und die Aufzeichnungen über das frühere Leben auf der Erde studiert hat, entscheidet er sich, die \"Axiom\" zur Erde zu fliegen. Daran wird er aber von Otto gehindert, der schon lange den Befehl erhalten hatte, nie zur Erde zurückzukehren, da die Wissenschaftler und die Regierung bereits im Jahr 2110, fünf Jahre nach Beginn der Aktion, meinten, die Erde würde nie wieder bewohnbar werden. Aus diesem Grund hat Otto sich auch dazu entschlossen, die Pflanze zu beseitigen, damit sie nicht zum Anlass für die Rückkehr zur Erde wird. Doch WALL·E und EVE gelingt es, die Pflanze zu retten.\n\nNach einem Kampf mit Otto gelingt es dem Kapitän schließlich, diesen zu deaktivieren. Inzwischen ist es EVE gelungen, die Pflanze in den Holodetector zu setzen, wodurch die \"Axiom\" schließlich automatisch den Kurs zur Erde setzt. WALL·E ist jedoch durch den Versuch Ottos, den Holodetector zu deaktivieren, schwer beschädigt worden, und seine letzte Rettung ist die Erde, auf der es die nötigen Ersatzteile gibt.\n\nAuf der Erde angekommen nutzt EVE alle vorhandenen Ersatzteile, um WALL·E zu reparieren. WALL·E befindet sich daraufhin allerdings wieder im „Auslieferungszustand“; seine „Persönlichkeit“ scheint verloren gegangen zu sein. Erst nach einer liebevollen Berührung durch EVE kehren seine Erinnerung und sein entwickeltes Wesen zurück.\n\nDie Menschen beginnen, zusammen mit den Robotern, mit dem Wiederaufbau und der Rekolonialisierung der Erde. Im Abspann sieht man, wie die Erde zu einem Garten wird und sich die Körper der Menschen wieder normalisieren, während WALL·E und EVE ihre weitere Existenz gemeinsam verbringen.\n\nDie FFS Film- & Fernseh-Synchron GmbH war für die Synchronisation verantwortlich. Der Sprecher von WALL·E, Timmo Niesner, schrieb das Dialogbuch und führte die Dialogregie.\nDer Film wurde von Andrew Stanton gedreht, der bereits für \"Findet Nemo\" einen Oscar erhalten hatte. Als Produzenten konnten Jim Morris von Lucasfilm und John Lasseter von Pixar (ebenfalls Oscargewinner für \"Findet Nemo\") verpflichtet werden.\n\nDie Stimmen der Roboter wurden nicht wie üblich direkt von Schauspielern gesprochen, sondern mittels Sounddesign von Ben Burtt resampelt. Zur Generierung der Stimme des Bordcomputers Otto wurde unter anderem die Sprachsynthese-Software MacInTalk von Apple verwendet. An der Gestaltung von EVE war der Apple-Chefdesigner Jonathan Ive beteiligt. Außerdem wird im Film die Beziehung von Pixar zu Apple durch einen alten Apple iPod deutlich, den \"WALL·E\" nutzt, um ein Video abzuspielen, sowie durch den klassischen Startton eines Apple-Computers, der auch bei \"WALL·E\" ertönt, wenn dieser seinen elektrischen Ladevorgang über seine eingebauten Solarmodule vollendet hat.\n\nAndrew Stanton konzipierte \"WALL·E\" bereits vor der 1995 produzierten \"Toy Story\". Die Frage: \"Was ist, wenn die Menschheit die Erde evakuiert und den letzten Roboter nicht ausschaltet?\" bildet dabei den Grundstock. Pete Docter entwickelte den Film 1995 innerhalb von zwei Monaten, nachdem Stanton ihm die Geschichte erzählt hatte. Aber da er unsicher war, eine Liebesgeschichte mit Maschinen zu erzählen, entschloss er sich, zunächst \"Monster AG\" (2001) zu drehen.\n\nStanton entwarf \"WALL·E\" mit Hilfe eines Fernglases – er beschloss, einen Hauptdarsteller zu kreieren, der ohne Mund und Nase und nur mit seinen Augen seine Emotionen vermittelt. Gemäß Stantons eigener Aussage könnte – zumindest „unbewusst“ – \"Nummer 5 lebt!\" Einfluss auf \"WALL·E\" gehabt haben.\n\nInsbesondere die Szenen auf der \"Axiom\" sind mit Zitaten und Referenzen aus Sciencefiction-Klassikern wie \"Krieg der Sterne\" (1977), Ridley Scotts \"Alien\" (1979) und Stanley Kubricks \"\" (1968) gespickt. So wird auch in \"2001\" der Bordcomputer HAL 9000, der sich gegen den Menschen wendet und auch optisch eine Ähnlichkeit besitzt, besiegt; als es dem fettleibigen Kapitän gelingt, auf seinen eigenen Füßen zu gehen, ertönt \"Also sprach Zarathustra\" von Richard Strauss. Die Schräglage des Raumschiffes spielt auf Szenen aus dem Katastrophenfilm \"Die Höllenfahrt der Poseidon\" (1972) an.\n\nDer Alarm, der in der \"Axiom\" ertönt während WALL·E gejagt wird, ist der Alarmton der Enterprise-Serie.\n\nDer einzige Real-Schauspieler des Films mit einer Sprechrolle ist Fred Willard, der den Präsidenten Shelby Forthright der Firma \"Buy n Large\" spielt. Es ist das erste Mal, dass ein Schauspieler in einem Pixar-Film auftritt. Weitere Live-Schauspieler treten im Film in den Werbespots der Firma auf.\n\nDer Vorfilm zu diesem Film heißt \"Presto\".\n\nBei einem Budget von 180 Millionen US-Dollar spielte der Film bis Ende 2008 weltweit über 500 Millionen US-Dollar ein.\n\nDer Film ist Justin Wright, einem Pixar-Mitarbeiter, gewidmet, der im Alter von 27 Jahren im März 2008 verstarb. Er hatte am Abspann von \"Ratatouille\" und an den Storyboards von \"WALL·E\" und \"Presto\" mitgearbeitet.\n\nAuf der Blu-ray- und DVD-Fassung befindet sich der Kurzfilm \"BURN·E,\" der die zeitlich parallel laufende Geschichte eines Reparaturroboters an Bord der \"Axiom\" erzählt. Dieser ist im Hauptfilm in einer Nebenrolle zu sehen.\n\n\"WALL·E\" wurde von der Kritik überwiegend positiv aufgenommen. So bezeichnete A. O. Scott von der New York Times die ersten 40 Minuten des Filmes – die nahezu ohne Sprache auskommen – als „ein cineastisches Gedicht von so viel Esprit und Schönheit, dass seine dunkleren Folgen erst nach einiger Zeit einsickern“. Regisseur und Autor Stanton nutze hier das Medium Film, um Kritik an der von Großunternehmen geprägten Konsumkultur voranzubringen.\n\nDer Kolumnist Kyle Smith von der New York Post ging in seinem Beitrag zum Film auf die dunklen Seiten des Drehbuchs ein: „Ich versuche, mich an einen Zeichentrickfilm von Disney zu erinnern, der derart düster und zynisch ist, wie dieser hier. Mir fällt keiner ein.“ Die Darstellung der Menschen in der Zukunft als fette Idioten sei zugleich nahe dran an den Besuchern von Walt Disney World, die in einer sterilen, künstlichen Umgebung passiv Unterhaltung konsumieren.\n\nTodd McCarthy schrieb in der Variety vom 30. Juni 2008, \"WALL-E\" sei „einfach, aber einfallsreich“. Er hob hervor, dass, obwohl die Handlung in einer Apokalypse wurzele, der Ton und die Botschaft sehr optimistisch seien: „Ja, das Schlimmste steht noch bevor, aber die Menschheit wird über kurz oder lang immer zu ihren Wurzeln zurückkehren. Das ist gut zu wissen.“\n\nGeorg Seeßlen findet, dass es die liebevoll ironischen Details sind, „mit denen der Alltag des ewigen Arbeiters und seine kleinen Freuden geschildert werden, zugleich ein alleingelassenes Kind und ein Relikt einer industriellen Zeit, wegen der man am liebsten noch ein halbes Stündchen mit Wall-E im Müll verbringen wollte.“\n\nDas Lexikon des internationalen Films urteilt: „Ein mitreißender, formal wie inhaltlich radikaler Animationsfilm, der in der ersten Hälfte sein bezauberndes audiovisuelles Abenteuer nahezu ohne (menschliche) Dialoge entwirft. In der zweiten Hälfte fokussiert er nicht weniger begeisternd auf Action, Spannung und seine ökologische Botschaft.“\n\n\n2016 belegte \"WALL·E\" bei einer Umfrage der BBC zu den 100 bedeutendsten Filmen des 21. Jahrhunderts den 29. Platz.\n\n"}
{"id": "3493529", "url": "https://de.wikipedia.org/wiki?curid=3493529", "title": "GRiD Compass 1100", "text": "GRiD Compass 1100\n\nDer GRiD Compass 1100 war der vermutlich erste als Notebook zu bezeichnende Computer und wurde im April 1982 in den Markt eingeführt.\n\nDer Computer wurde vom britischen Industrie-Designer Bill Moggridge 1979 entworfen. Hierfür wurde er am 9. November 2010 mit dem Prince Philip Designers Prize ausgezeichnet. Das Gehäuse bestand aus einer Magnesium-Legierung. Hinzu kamen Designmerkmale, die heute noch in modernen tragbaren Rechnern zu finden sind; wie etwa die schlichte, schwarze Außenhülle mit den abgerundeten Ecken.\n\nDer Computer beinhaltete einen Intel 8086-Prozessor, ein 320×240-Pixel-Elektrolumiszenz-Display (nicht CGA), 340 Kilobyte Magnetblasenspeicher und ein 1200 bit/s Modem. Geräte wie Festplatten und Floppy-Laufwerke konnten über ein IEEE488-I/O-Interface (auch bekannt als GPIB oder General Purpose Instrumentation Bus) angeschlossen werden. Das Gerät wog 5 kg.\n\nDer Grid Compass lief unter GRiD-OS, einem speziellen Betriebssystem. Dieses und der hohe Preis von 8.000 – 10.000 $US limitierte seine Absatzzahlen und Anwendungszwecke. Der Hauptabnehmer war die US-Regierung. Die NASA nutzte ihn in den frühen 80er Jahren insbesondere aufgrund seiner für damalige Verhältnisse hohen Leistung und seines geringen Gewichts. Die militärischen Spezialeinheiten erwarben den Rechner, um ihn bei Fallschirmjägern im Kampfeinsatz mitzuführen.\n\nZusammen mit dem Gavilan SC und Sharp PC-5000, die im folgenden Jahr herauskamen, hat der GRiD Compass viel zum Grunddesign kommender Laptop-Generationen beigetragen – auch wenn das Laptop-Konzept viele Elemente aus dem Dynabook-Projekt übernahm, welches in den späten 1960ern von Xerox PARC entwickelt wurde. Der Hersteller des Compass 1100, GRiD Systems Corporation, wurde 1988 von der Tandy Corporation, die heute als RadioShack firmiert, aufgekauft.\n\nEine bekanntere frühe Form des tragbaren Computers ist der Osborne 1, der aufgrund seines CP/M-Betriebssystems weitere Verbreitung fand, obwohl sein Aussehen und seine Größe dem GRiD Compass nachstanden.\n"}
{"id": "3496279", "url": "https://de.wikipedia.org/wiki?curid=3496279", "title": "Autoplacer", "text": "Autoplacer\n\nDer Autoplacer ist ein Programm aus dem Bereich des Elektronikentwurfes zum automatischen Positionieren der Schaltungselemente während des physikalischen Leiterplatten-, MCM/SiP- und IC-Designs (vgl. Leiterplattenentflechtung, Chipentwurf). Die Autoplacer sind in der Regel in einer ECAD-Entwurfsumgebung eingebettet und im Gegensatz zu den Autorouter selten als separates Programm verfügbar. In ihrer Grundlage beruhen die Autoplacer, die ein mathematisches Optimierungsproblem des Layouts bzw. der Anordnung (vgl. Floorplanning) lösen, auf den Algorithmen der kombinatorischen Optimierung. Wie Autorouter erzeugen auch Autoplacer oft keine Layouts, die \"sofort\" zufriedenstellend sind. Meistens sind weitere manuelle Eingriffe und Anpassungen erforderlich. Oft müssen auch Regeln beachtet werden, die schwer vorzugeben sind, wie bspw. EMV-bedingte Orientierung bestimmter Bauelemente relativ zueinander. Gemessen an den sehr ausgereiften Autoroutern, gelten die heutigen Autoplacer für den Leiterplattenentwurf als relativ unterentwickelt.\n"}
{"id": "3497378", "url": "https://de.wikipedia.org/wiki?curid=3497378", "title": "Apple Design Award", "text": "Apple Design Award\n\nDer Apple Design Award (deutsch etwa: \"Apple-Designpreis\"), abgekürzt ADA, ist eine Auszeichnung für innovative Soft- und Hardware für Apple Mac und das Mobiltelefon iPhone. Er wird jährlich von Apple in verschiedenen Kategorien - darunter beispielsweise besonders innovative Apps - auf der Worldwide Developers Conference an unabhängige Entwickler verliehen.\n\nDie Auszeichnung gibt es seit 1998; in den ersten beiden Jahren noch unter dem Namen „Human Interface Design Excellence Award“. In Anspielung sowohl auf das Akronym HIDE als auch auf Heidi Roizen, damals als Vice President of World Wide Developer Relations bei Apple für das Verhältnis zu den Entwicklern zuständig, wurde der Preis in dieser Zeit auch „Heidi“ genannt (entsprechend zum Oscar).\n\nAls Symbol des Preises erhalten die Preisträger einen Würfel mit einem Apple-Logo das leuchtet, wenn es berührt wird. \n\n"}
{"id": "3498457", "url": "https://de.wikipedia.org/wiki?curid=3498457", "title": "NeXTcube", "text": "NeXTcube\n\nDer NeXT Computer von 1988 und sein sehr ähnliches Nachfolgemodell NeXTcube waren High-End-Workstations der Firma NeXT, die von 1988 bis 1993 gebaut wurden. Als Betriebssystem kam NeXTStep zum Einsatz, ein eigens entwickeltes Unix-Derivat, das später die Basis für Mac OS X bildete. Die Rechner wurden in einem würfelförmigen (engl. \"Cube\") Gehäuse mit „1-foot“ (305 mm) Kantenlänge geliefert, das aus gegossenem Magnesium bestand, und kosteten anfangs rund 6.500 US-Dollar.\n\nDie Rechner stellten in Bezug auf die Leistungsfähigkeit der Hardware, ihre umfangreiche Ausstattung, als auch die Ausgereiftheit und Bedienerfreundlichkeit des Betriebssystems Meilensteine dar. Dies spiegelte sich jedoch im hohen Preis, der die Geräte für Privatanwender und Studenten praktisch unerschwinglich machte.\n\nHeute sind die NeXT-Computer vor allem durch die Tatsache bekannt, dass Tim Berners-Lee auf einem NeXTcube am Forschungszentrum CERN das World Wide Web und den ersten Webbrowser entwickelte. In diesem Zusammenhang entwickelte und betrieb er auch den ersten Webserver der Welt auf einem NeXTCube.\n\nEinzigartig war das verbaute Magneto-Optische Laufwerk statt einer Festplatte, obwohl letztere als Option erhältlich war. Die Workstation kam mit einem 17″-Graustufen-Monitor mit 1120 × 832 Pixeln mit eingebauten Lautsprechern.\n\nAls CPU wurde ein Motorola 68030 mit einem 68882-Co-Prozessor für schnellere mathematische Berechnungen verbaut. Zudem gab es einen 56001 digitalen Signalprozessor (DSP) für Multimedia und zwei spezielle 6-Kanal-DMA-Controller, die die CPU bei Ein/Ausgabe-Prozessen entlasteten.\n\nDie Maus wurde an der Tastatur angeschlossen, diese wiederum am Monitor. Der Ein/Ausschalter war in der Tastatur integriert, so dass nur der Betrieb mit der Originalperipherie möglich war, es sei denn eine besondere Adapterbox kam zum Einsatz, an der ein VGA-Monitor und die Original-Tastatur und Maus angeschlossen werden konnten.\n\n1990 wurde ein geändertes Modell mit dem Namen NeXTcube herausgebracht. Er hatte einen 25 MHz 68040 Prozessor, eine Festplatte statt des bisher verwendeten MO-Laufwerks und ein Diskettenlaufwerk. Ein 33 MHz NeXTcube Turbo kam später.\n\nNeXT brachte außerdem die NeXTdimension für den NeXTcube, eine Platinenerweiterung basierend auf dem Intel i860 Prozessor. Damit konnte z. B. ein 32-Bit PostScript Farbmonitor angeschlossen werden.\n\nDer NeXT Computer und NeXTcube waren, unter anderem wegen der hohen Preise, kein großer kommerzieller Erfolg. Wegen ihres außergewöhnlichen Designs, das von dem auch für Apple tätigen Unternehmen frog design stammte, sind einige in Museen moderner Kunst ausgestellt, etwa in der \"Neuen Sammlung\" der Pinakothek der Moderne in München und im Museum of Modern Art in San Francisco.\n\n\n"}
{"id": "3503607", "url": "https://de.wikipedia.org/wiki?curid=3503607", "title": "Macintosh IIx", "text": "Macintosh IIx\n\nDer Macintosh IIx wurde 1988 von Apple vorgestellt und war eine verbesserte Version des Macintosh II. Die Motorola 68020 CPU und 68881 FPU wurden durch eine je 16 MHz getaktete 68030 CPU und 68882 FPU getauscht. Zudem bekam es ein 1,44 MB Diskettenlaufwerk, statt des bisherigen 800 KB Laufwerks.\n\nDer ursprüngliche Preis des IIx betrug 7.769 bzw. 9.300 US$ für eine Version mit 40 MB Festplatte. Der Mac IIx hatte, wie sein Vorgänger, 0,25 KB L1-Cache, einen 16 MHz Bus (Was der CPU-Geschwindigkeit entsprach) und unterstützte max. System 7.5.5.\n\nDer IIx war der zweite von drei Macintosh Modellen, welche, in dem Fall 6, NuBus Steckplätze hatte. Der letzte war der Macintosh IIfx.\n\nApple's Codenamen für den IIx enthielten \"Spock\" und \"Stratos\". Support und Ersatzteile für das Gerät wurden am 31. August 1998 eingestellt.\n\n"}
{"id": "3506572", "url": "https://de.wikipedia.org/wiki?curid=3506572", "title": "Codepage 949", "text": "Codepage 949\n\nFür Korea gab es zwei verschiedene Zeichensätze: der Wansung-Zeichensatz (koreanisch ), der nur die Hangul-Silbenblöcke kodiert, die in der koreanischen Sprache verwendet werden (2.350), und der Johab-Zeichensatz, der alle möglichen Hangul-Silbenblöcke kodiert, auch die, die nicht in der koreanischen Sprache vorkommen (insgesamt 11.172). Die Codepage 949 belegt 8822 freie Codepunkte des Wansung-Zeichensatzes mit den zusätzlichen Hangul-Silbenblöcken des Johab-Zeichensatzes.\n\nDie Codepage 949 ist genau wie EUC-KR eine variable 16-Bit-Kodierung, d. h. ein Zeichen kann entweder ein oder zwei Byte groß sein. Die Zeichen im Bereich 0x00-0x7F sind identisch mit KS X 1003 und bestehen aus nur einem Byte. Die Zeichen im Bereich 0x81-0xFE hingegen bestehen aus zwei Bytes, im Gegensatz zu EUC-KR kann sich das nachfolgende Byte jedoch auch im Bereich von 0x41-0x7A befinden.\n\nSeit Windows XP wird auch die ursprüngliche Kodierung EUC-KR als Codepage 51949 angeboten.\n\n"}
{"id": "3520487", "url": "https://de.wikipedia.org/wiki?curid=3520487", "title": "Windows-1250", "text": "Windows-1250\n\nWindows-1250 ist eine 8-Bit-Zeichenkodierung, die für das Betriebssystem Microsoft Windows entwickelt wurde. Sie kodiert Zeichen, die für mittel- und osteuropäische Sprachen benötigt werden und deckt Polnisch, Tschechisch, Slowakisch, Slowenisch, Ungarisch, Serbokroatisch (lateinische Orthografie), Rumänisch und Albanisch, aber auch Deutsch ab. Der Zeichensatz basiert auf ISO 8859-2. Wie in Windows-1252 wurden bei Windows-1250 die Steuerzeichen durch typografische Satzzeichen und zusätzliche Buchstaben ersetzt, außerdem wurde jedoch auch die Sortierung der Zeichen geändert.\n\nDie folgende Tabelle stellt das Repertoire von Windows-1250 dar.\nDie folgende Tabelle stellt die Unterschiede zwischen ISO 8859-2 und Windows-1250 dar:\n\n"}
{"id": "3521047", "url": "https://de.wikipedia.org/wiki?curid=3521047", "title": "Chemical Space", "text": "Chemical Space\n\nMit Chemical Space (engl. wörtlich übersetzt „Chemischer Raum“, wobei die deutsche Übersetzung nicht gebräuchlich ist) ist die Gesamtheit aller Moleküle gemeint, die existieren oder hergestellt werden können. Der Chemical Space wird auf 10 Moleküle geschätzt, die thermodynamisch stabil sind. In der Arzneistoffforschung wird mit Chemical Space die Gesamtzahl an möglichen Molekülen, die als Arzneistoffe einsetzbar sind bezeichnet. Man geht anhand einer maximalen Molekülmasse für Arzneistoffe von 600 g·mol davon aus, dass die Gesamtzahl 10 beträgt. Der Chemical Space dient so zum Vergleich der Zahl der bisher synthetisierten Substanzen von etwa 126 Millionen mit der maximal möglichen Zahl synthetisierbarer Substanzen.\n"}
{"id": "3523550", "url": "https://de.wikipedia.org/wiki?curid=3523550", "title": "ASCI Red", "text": "ASCI Red\n\nASCI Red, auch ASCI Option Red oder Janus, war ein seit 1997 in den Sandia National Laboratories installierter Supercomputer. Zwischen Juni 1997 und Juni 2000 führte er die TOP500-Liste der schnellsten Computersysteme an. ASCI Red wurde im September 2005 stillgelegt.\n\nDas System wurde in Zusammenarbeit von Intel und den Sandia National Laboratories im Rahmen des Advanced Simulation and Computing Program entwickelt um Explosionen nuklearer Waffen zu simulieren.\n\nDer ASCI Red war ein auf Multiple-Instruction Multiple-Data-Architektur basierendes System von ursprünglicherweise 9152 auf 200 MHz getakteten Pentium Pro-Prozessoren. Diese wurden später durch 9298 Pentium II OverDrive-Prozessoren, welche auf je 333 MHz getaktet waren, ersetzt.\n\nDes Weiteren verfügte das System über 1212 Gigabyte verteilten Hauptspeicher, sowie 12,5 Terabyte Festplattenspeicher.\n\nAls Betriebssysteme kamen das von Intel entwickelte, auf OSF UNIX basierende \"Teraflops OS\" und das von Sandia entwickelte, auf dem SUNMOS-Kernel basierende \"Cougar\"-System zum Einsatz, welche jeweils auf eigenen Partitionen liefen. Hierbei diente Teraflops OS hauptsächlich zur Konfiguration, Administration und Entwicklung. Die eigentlichen Berechnungen wurden unter dem effizienteren Betriebssystem \"Cougar\" durchgeführt, welches auch die Speicherverteilung verwaltete.\n\nDer ASCI Red war der erste Supercomputer der im MP-LINPACK-Benchmark eine Leistung von über einem TFLOPS erreichte. Nach seiner Aufrüstung auf Pentium-II-Overdrive-Prozessoren erreichte er über zwei TFLOPS.\n\n"}
{"id": "3525102", "url": "https://de.wikipedia.org/wiki?curid=3525102", "title": "Portable", "text": "Portable\n\nDer Begriff Portable bezeichnet einen Computer für den mobilen Einsatz mit besonderer Gehäuseform, die über ein integriertes Display verfügt. Übersetzt heißt portable „tragbar“. Später wurde diese Computerform teilweise humorvoll \"luggable\" genannt, was so viel wie „schleppbar“ heißt. Gerade in der Anfangszeit ging es bei Portables z. T. mehr um die leichtere Transportierbarkeit als um Netzunabhängigkeit, sodass einige dieser Geräte nicht über Akkus oder Batterien verfügten. Solche Systeme dienten dazu an verschiedenen Einsatzorten (z. B. sowohl in der Firma als auch bei Präsentationen oder zu Hause) eingesetzt zu werden.\n\nDer erste Portable, der Xerox NoteTaker, wurde 1976 von Xerox PARC entwickelt. Allerdings ist er nie über den Status eines Prototyps hinweggekommen. Der erste serientaugliche Portable war der IBM PC Portable. Ein erfolgreicheres und wirklich IBM-PC-kompatibles Modell war der Compaq Portable. Auch von Apple wurde ein Modell dieser Klasse entwickelt, der Macintosh Portable, welcher aber nicht genug Vorteile gegenüber den nicht-tragbaren Macintoshs bot, um sich durchzusetzen. Toshiba veröffentlichte 1989 den \"T5200\" als Portable Computer. \n\nHeute sind Portables nicht nur in der Industrie und beim Militär weiterhin im Einsatz, sondern auch in der Forschung und in der forensischen Datenakquise. Ihre Stärke liegt, im Gegensatz zu Notebooks darin, dass Messkarten eingebaut werden können oder Festplatten mit mehreren Terabyte Speicherkapazität.\n"}
{"id": "3525121", "url": "https://de.wikipedia.org/wiki?curid=3525121", "title": "CenterIM", "text": "CenterIM\n\nCenterIM ist ein freier Instant-Messaging-Client für unixoide Betriebssysteme.\n\nDie Software ging als Abspaltung des Clients \"Centericq\" hervor, dessen Entwicklung 2005 eingestellt wurde. Während der Entwicklung der Version 5 von CenterIM wurde der Quelltext jedoch nahezu komplett neu geschrieben. Wie sein Vorgänger entbehrt auch CenterIM einer grafischen Benutzeroberfläche, so dass es einzig über die Konsole bedient wird, was auch per Secure Shell über eine Netzwerkverbindung möglich ist.\n\nEinmal in einer Eingabeaufforderung gestartet, unterteilt CenterIM unter Verwendung von ncurses diese in verschiedene Abschnitte für die Kontaktliste, den Chatbereich und die Protokollierung von Ereignissen. Der Benutzer navigiert mittels der Pfeiltasten der Tastatur oder ruft bestimmte Menüs mit anderen, angegebenen Tasten auf.\n\n\n\n\n"}
{"id": "3526826", "url": "https://de.wikipedia.org/wiki?curid=3526826", "title": "Windows-1253", "text": "Windows-1253\n\nWindows-1253 ist eine 8-Bit-Zeichenkodierung des Windows-Betriebssystems. Sie kodiert die nötigen Zeichen für monotonisches Griechisch und ist inkompatibel zu ISO 8859-7, da unter anderem der Buchstabe Ά und das €-Symbol an einer anderen Stelle kodiert sind.\n\n"}
{"id": "3526881", "url": "https://de.wikipedia.org/wiki?curid=3526881", "title": "Meine Freunde Tigger und Puuh", "text": "Meine Freunde Tigger und Puuh\n\nMeine Freunde Tigger und Puuh () ist eine 44-teilige Animationsserie der Walt Disney Company aus dem Jahr 2007. Die Serie basiert auf den Kinderbüchern von Alan Alexander Milne.\n\nIm Hundertmorgenwald leben Winnie Puuh, Tigger, Ferkel, I-Aah, Eule, Rabbit, Kanga mit ihren Jungen Ruh und deren Menschenfreund Christopher Robin. Sie finden zwei neue Freunde: Die rothaarige, sechsjährige Darby und ihr Hund Buster. Darby liebt es, mit Tigger und Puuh herumzutollen und als Superschnüffler mysteriöse Fälle aufzuklären. Die Superschnüffler sind Winnie Puuh, Tigger und Darby in Superheldenkostümen. Im Laufe der Serie freunden sich die Truppe mit dem femininen Stachelschwein, der langsamen Schildkröte und dem pessimistischen Biber an.\n\nDie Zeichentrickserie mit 44 Folgen wurde 2007 unter der Regie von David Hartman und Don Mac Kinnon produziert. Ab dem 13. März 2007 begann die Ausstrahlung im amerikanischen Disney Channel im Format \"Playhouse Disney\". Die Serie wurde in mehreren Ländern Europas durch den Disney Channel gezeigt und teilweise auch auf DVD veröffentlicht.\n\nAuf Deutsch ist die Serie seit 26. Mai 2007 im Disney Channel zu sehen. Später begann die Ausstrahlung durch SuperRTL. Das Weihnachts-Special \"Das verschwundene Rentier\" ist seit November 2007 und die DVD \"Die Schatzsuche\", die drei normale Episoden enthält, seit Mai 2008 auf DVD erhältlich.\n\nDie Synchronsprecherin Chloë Moretz erhielt für die Rolle der \"Darby\" (US-Fassung) eine Nominierung für den \"Young Artist Award\" 2008. Eine Nominierung für den \"Annie Award\" 2008 erhielt das Drehbuch für die Episode \"Good Night to Pooh\" von Roy Meurin.\n\n\n"}
{"id": "3529462", "url": "https://de.wikipedia.org/wiki?curid=3529462", "title": "Dinosaurier (Film)", "text": "Dinosaurier (Film)\n\nDinosaurier (Originaltitel: \"Dinosaur\") ist ein Computeranimationsfilm und der 39. abendfüllende Zeichentrickfilm von Walt Disney Pictures aus dem Jahr 2000. Er kombiniert Animationen mit echten Naturaufnahmen und hatte in Deutschland Premiere am 16. November 2000.\n\nEine Iguanodon-Mutter wird an ihrem Nest von einem jungen Parasaurolophus belästigt. Sie scheucht ihn weg und er läuft in ein Dickicht, wo er auf einen Carnotaurus trifft. Der Jäger hat es aber nicht auf das Junge abgesehen und greift die anderen Tiere an. Die Iguanodon-Mutter lässt ihre Eier alleine und flüchtet; der Carnotaurus zertritt das Gelege bis auf eins und erlegt einen altersschwachen Pachyrhinosaurus. Durch einen Eierdieb gelangt das Ei in einen Fluss und wird von einem Pteranodon hinfortgetragen. Nach einer längeren Reise landet es auf einer Insel. Das aus dem Ei geschlüpfte Baby wird von den Lemuren Plio, Yar, Sini und Suri aufgezogen. Sie nennen ihn Aladdar. \n\nAm Abend der Balznacht, in der sich alle Lemuren-Jünglinge mit einem Partner zusammentun (außer Sini und der Saurier Aladdar), fallen außergewöhnlich viele Sternschnuppen. Plötzlich regnet es Gesteinsbrocken und ein gewaltiger Meteorit stürzt ins Meer, was eine Druckwelle auslöst, die die Lemuren und Aladdar zur Flucht zwingt. Im letzten Moment springen sie ins Meer und schwimmen in Richtung Festland.\n\nNachdem sie in Sicherheit sind, machen sie sich auf den Weg, Nahrung zu finden. Dabei werden sie von Velociraptoren angefallen. Dadurch stoßen sie mit einer Herde aus Pflanzenfressern zusammen. Deren tyrannischer Anführer Kron hat jedoch für sie nichts übrig. Aladdar verliebt sich in Neera, Krons Schwester. Zudem freundet sich Aladdar mit zwei älteren Dinosaurier-Damen namens Baylene (ein Brachiosaurus) und Eema (ein Styracosaurus) an. Die Herde will ihren Nistplatz wie jedes Jahr erreichen, allerdings hat der Meteoriteneinschlag eine Dürre ausgelöst. Während der Wanderung werden viele Tiere schwach. Nachdem die Herde in der Nacht eine Wüste durchquert hat, kommen am nächsten Tag zwei Carnotauren dort entlang, welche die Spuren der Herde sehen und diesen folgen.\n\nDie Herde erreicht einen See, an dem sie sonst immer Rast gemacht haben, durch den Meteoriteneinschlag ist dieser jedoch ausgetrocknet. Kron drängt die Herde daher, trotz Neeras Bedenken, zum Weiterziehen. Als Aladdar Wasser unter der Erde entdeckt und ein Loch gräbt, kommt es zu Rangeleien zwischen den Sauriern. Krons Helfer Bruton, der die Herde gemeinsam mit einem Kundschafter verlassen hat, um die Gegend zu prüfen, wird von den Carnotauren angegriffen und verletzt, während sein Begleiter umkommt. Bruton warnt Kron und die Herde zieht in doppeltem Tempo weiter. Dabei wagt Aladdar, Kron zu widersprechen und fordert ihn auf, Rücksicht auf die Alten und Schwachen zu nehmen.\nEs kommt zum Kampf. Aladdar unterliegt Kron, der gemeinsam mit der Herde und seiner Schwester Neera weiterzieht. Aladdar entscheidet sich, mit den Lemuren bei Baylene und Eema zu bleiben.\n\nIn der Nacht erreichen die Carnotauren das Wasserloch und beobachten, wie sich Regen den nahe gelegenen Bergen nähert. Die kleine Gruppe um Aladdar entdeckt Bruton. Er schließt sich ihnen an, als sie in einer nahe gelegenen Höhle, während der Nacht rasten. Bei einsetzendem Regen erreichen die Carnotauren die Höhle und entdecken Aladdar. Bruton bringt die Höhle zum Einsturz, rettet der Gruppe damit das Leben, wird aber selbst unter den Trümmern begraben. Nachdem die anderen somit in Sicherheit sind, taucht der männliche Carnotaurus aus dem Schutt auf und verlässt die Höhle in die Richtung, aus der er gekommen ist. \n\nDie Gruppe um Aladdar folgt dem Weg innerhalb des Berges, bis sie eine Sackgasse erreichen. Gemeinsam können sie den blockierten Weg aber freiräumen und gelangen so auf direktem Wege zum Nistplatz. Dabei stellen sie fest, dass der Weg, auf dem die Herde sich noch befindet, durch einen Felsrutsch blockiert ist. Aladdar sucht deshalb die Herde, um sie zu warnen und auf einem anderen Weg in das Tal zu bringen. In der noch dunklen Wüste entdeckt er jedoch einen verendeten Saurier und den männlichen Carnotaurus, woraufhin er sich versteckt. Unbemerkt entkommt er. Der Carnotaurus nimmt jedoch seine Witterung auf und folgt Aladdar.\n\nNachdem Aladdar die Herde auf der anderen Seite des Felshanges gefunden und Kron vor dem Carnotaurus gewarnt hat, will dieser mit der zu schwachen Herde dennoch über den Berg klettern. Als Aladdar die Herde dazu bewegen will sich ihm anzuschließen, kommt es erneut zum Kampf zwischen Aladdar und Kron. Wieder unterliegt Aladdar Kron, doch Neera schreitet ein und stellt sich schützend an Aladdars Seite. Dank ihr folgt die Herde Aladdar, nur Kron bleibt zurück und beginnt mit dem Aufstieg. Plötzlich hören sie ein Brüllen und entdecken den Carnotaurus, der die Herde angreift. Dabei wehren sich die Saurier mit Brüllen und drängen diesen so zurück. Er entdeckt Kron und erklimmt die Felswand. Neera und Aladdar kommen Kron zu Hilfe. Kron flüchtet aufwärts, findet sich aber an einem Abhang wieder. Der Carnosauris packt Kron und verletzt ihn schwer. Neera verhindert, dass Kron getötet wird. Aladdar schiebt den Carnotaurus zum Abgrund, wobei aber der Boden unter dem Gewicht des Carnotaurus zusammenbricht. Er versucht noch Aladdar zu packen und mitzuziehen, aber letzterer kann sich an dem Hang festhalten. Der Raubsaurier stürzt in den Tod. Als der Staub sich legt, müssen Neera und Aladdar aber feststellen, dass Kron seinen schweren Verletzungen erlegen ist.\nGemeinsam führen sie dennoch die Herde zum Nistplatz.\n\nEinige Zeit später erwacht die Kinderstube der Saurier zum Leben. Auch im Nest von Aladdar und Neera schlüpft das erste Junge aus dem Ei. ein Männchen, dass, laut Jar, genauso aussieht, wie sein Vater.\n\nAnders als bei gewöhnlichen Computeranimationsfilmen entstanden die Szenen nicht komplett im Computer, sondern sind eine Kombination aus echten Naturaufnahmen und Computeranimationen, die auch teilweise um gefilmte, physische Spezialeffekten (z. B. reale Explosionen) ergänzt wurden. Viele reale Naturaufnahmen wurden nachbearbeitet oder ergänzt.\n\nInsgesamt sechs Jahre investierten die Disney-Studios, um die Dinosaurier auf der Kino-Leinwand wieder auferstehen zu lassen. 350 Mitarbeiter, ca. 550 Computer und insgesamt ca. 3,2 Millionen Rechenstunden waren notwendig, um die Urzeittiere lebensecht zu animieren. Die Landschaftsaufnahmen, die in den Film eingearbeitet wurden, stammen unter anderem aus Australien, Venezuela und Samoa.\n\nIn den USA brachte der Film an seinem ersten Wochenende 38,85 Millionen Dollar in die Kassen – dies war bis dahin das drittbeste Eröffnungswochenende für einen Disney-Film. Nur Der König der Löwen und der Pixar-/Disney-Film Toy Story 2 starteten besser. Insgesamt konnte der Film mehr als 349 Millionen Dollar einspielen. Der Film hat nach offiziellen Angaben 127,5 Millionen Dollar gekostet und war somit der teuerste Film des Jahres 2000.\n\n\n"}
{"id": "3529798", "url": "https://de.wikipedia.org/wiki?curid=3529798", "title": "Sun-4", "text": "Sun-4\n\nSun-4 bezeichnet eine seit 1987 von Sun Microsystems produzierte Reihe von Unix Workstations und Servern. Die ursprüngliche Sun-4 Reihe bestand wie die Sun-3 Reihe aus VMEbus-Systemen, die aber - anstelle von 68k-Prozessoren - mit auf Suns eigener SPARC V7 RISC Architektur basierenden Mikroprozessoren ausgestattet waren. Die Sun-4 Architektur wird von SunOS ab der Version 3.2 unterstützt.\n\n1990 wurde die Sun-4 Reihe zugunsten der SPARCstation und -server Reihe eingestellt. Allerdings erhielten einige der frühen SPARCstation und -server-Modelle bis 1991 noch Sun-4 Modellbezeichnungen.\n\nDer Begriff Sun-4 wurde weiterhin verwendet, um in der Computerentwicklung die grundlegende Architektur eines SPARC-basierten Systems zu beschreiben. \n\nDie Modelle der Sun-4 Reihe sind hier in annähernd chronologischer Reihenfolge aufgelistet.\n\nSun 4/110, 4/150, 4/260 und 4/280 Systeme, welche mit dem \"Sun 4300 CPU board\" aufgerüstet wurden, wurden entsprechend als Sun \"4/310\", \"4/350\", \"4/360\" und \"4/380\" bezeichnet.\n\n\n\n\n\n\n\n\n"}
{"id": "3529960", "url": "https://de.wikipedia.org/wiki?curid=3529960", "title": "Quartz Composer", "text": "Quartz Composer\n\nQuartz Composer ist ein Programm von Apple, mit dem man sogenannte \"Patches\" entwickeln kann, die dann von der Quartz-Grafik-Engine ausgeführt werden.\n\nDie Patches basieren dabei auf anderen Patches, die über Ein- und Ausgänge miteinander verbunden werden. Patches können Funktionen wie Filter oder Effekte bereitstellen, einfache Bilder und Animationen generieren und dabei Eingaben wie Maus und Tastatur miteinbeziehen.\n\nDas System nutzt selbst an einigen Stellen Quartz Composer Patches, z. B. bei den Visualizern in iTunes, den Effekten in iChat oder bei den Bildschirmschonern.\n\nDie fertigen Patches können in einer weiteren Anwendung, dem Quartz Composer Visualizer, geladen und ausgeführt werden. Weiterhin ist damit das Anzeigen und Ausführen der Patches über ein Netzwerk möglich. Damit können mehrere Rechner mit mehreren Bildschirmen zu einer „Bildschirmwand“ zusammengeschlossen werden, von denen jeder nur einen Teil des Patches zeigt.\n\nQuartz Composer ist seit Mac OS X 10.4 Teil der Xcode Tools. Seit Xcode 4.3 (das nur noch in einem einzigen App-Bundle ausgeliefert wird) ist Quartz Composer nicht mehr offizieller Teil der Xcode Tools, kann jedoch von registrierten Entwicklern im Download-Bereich kostenlos heruntergeladen werden.\n\n"}
{"id": "3538118", "url": "https://de.wikipedia.org/wiki?curid=3538118", "title": "SPARCstation", "text": "SPARCstation\n\nSPARCstation, SPARCserver und SPARCcenter waren Bezeichnungen für eine seit 1989 von Sun Microsystems produzierte Reihe von Workstations und Servern, die auf der SPARC-Architektur basierten. Sie sind die Nachfolger der Sun-4-Reihe.\n\nDie SPARCserver-Modelle entsprachen genau den SPARCstations mit der gleichen Modellbezeichnung, verfügten aber nicht über Grafikkarte und Monitor. Bis zur Veröffentlichung der Reihe 600MP begannen die Modellbezeichnung mit \"4/\" (für Sun-4); danach begannen sie mit \"S\".\n\nDas erste Modell der SPARCstation-Reihe war die SPARCstation 1 (\"Sun 4/60\") von 1989. Diese Reihe führte die \"Sun-4c\"-Architektur ein, eine Variante der früheren \"Sun-4\"-Architektur. Die SPARCstation-Serie war international sehr erfolgreich, was zum Teil daran lag, dass Motorola erst verspätet modernere Prozessoren entwickelte. 1995 wurde die SPARCstation-Reihe durch die Sun Ultra-Reihe abgelöst, das letzte Modell war die SPARCstation 20.\n\nIm Gegensatz zu früheren Modellen von Sun und anderen Computerherstellern wurden die meisten der Desktop-Varianten der SPARCstation und SPARCserver Modelle in „pizza box“ oder „lunch box“ Gehäusen ausgeliefert.\n\nDie SPARCserver-Modelle, deren Bezeichnung mit „30“ oder „70“ endeten, waren in Hochkant-Desktopgehäusen mit 5 oder 12 VME-Steckplätzen untergebracht. Die SPARCcenter 2000 und Geräte mit der Bezeichnungsendung „90“ besaßen hingegen Rackgehäuse.\n\nSpätere Versionen der SPARCstation-Reihe, wie Modell 10 und 20, konnten dank ihrer MBus-Systeme auch als Mehrprozessorsystem konfiguriert werden.\n\nDie Modelle sind in ihren jeweiligen Kategorien in annähernd chronologischer Reihenfolge aufgelistet.\n\nAufgelistet sind lediglich Modelle, die offiziell von Sun Microsystems unterstützt wurden. Zahlreiche andere Anbieter haben Aufrüstmöglichkeiten hergestellt.\n\nEinige der als SPARCstation aufgelisteten Modelle waren auch als SPARCserver verfügbar und umgekehrt.\n\n"}
{"id": "3539273", "url": "https://de.wikipedia.org/wiki?curid=3539273", "title": "KGDB", "text": "KGDB\n\nKGDB ist ein Debugger für den Linux-Kernel.\nEr benötigt zwei miteinander verbundene Linux-Rechner. Die Verbindung kann über RS-232 via Nullmodem-Kabel oder über UDP/IP (KGDB over Ethernet, KGDBoE) erfolgen.\n\nKGDB ist seit der Kernel-Version 2.6.26-rc1 Bestandteil von Linux.\n\nFreeBSD benutzt den Namen kgdb für einen modifizierten GDB, der ebenfalls als Debugger für den Kernel dient. Es lassen sich sowohl Speicherabzüge nach einem Systemabsturz damit analysieren als auch über miteinander verbundene Rechner (RS-232 oder FireWire) ein aktiver Kernel debuggen.\n\n"}
{"id": "3545026", "url": "https://de.wikipedia.org/wiki?curid=3545026", "title": "Flatland (Film)", "text": "Flatland (Film)\n\nFlatland ist ein US-amerikanischer Computeranimationsfilm von 2007 und Verfilmung des im Jahr 1884 erschienenen Buches \"Flatland\" von Edwin Abbott Abbott.\n\nDie Bewohner von Flatland, einer Welt mit nur zwei Raumdimensionen, sind regelmäßige Polygone, wobei die Anzahl der Ecken den sozialen Rang darstellt. So bilden Dreiecke die soziale Unterschicht und Quadrate die Mittelschicht.\n\nEines Nachts hat der Hauptdarsteller des Films, A Square, einen Traum von einer eindimensionalen Welt. In dieser Welt leben Punkte und unterschiedlich lange Linien auf einer Geraden. A Square versucht dem König der Welt das Prinzip von zwei Dimensionen zu erklären, stößt aber auf Ignoranz.\n\nKurze Zeit später erscheint ein mysteriöser Gast aus der dritten Dimension in Flatland. Dieser nimmt A Square mit in die dreidimensionale Welt und erteilt ihm den Auftrag, in seiner zweidimensionalen Welt die frohe Botschaft der dritten Dimension zu verbreiten.\n\nAls A Square in seiner zweidimensionalen Welt die Nachricht einer dritten Dimension zu verbreiten versucht, wird er als Ketzer verfolgt.\n\nDer Film entstand nach einem Drehbuch von Tom Whalen und unter der Regie von Ladd Ehlinger Jr. Als Studio fungierte das eigens gegründete Flatland Productions. Der verantwortliche Produzent war F.X. Vitolo. Die Musik komponierte Mark Slater und das Editing übernahm Regisseur Ehlinger selbst, ebenso wie die Hauptrolle A Square. A Hexagon wurde von Megan Colleen gesprochen, President Circle von Greg Trent und A Sphere von Simon Hammond. \n\nDie Arbeit am Film begann 2004 und dauerte drei Jahre an, wobei die Animationen im Wesentlichen von Ehlinger selbst angefertigt wurden. Das Projekt ging auf Ehlingers Initiative zurück. Er wollte das Buch Edwin Abbott Abbott modernisiert umsetzen, dabei die Gesellschaftskritik erhalten, aber die von ihm als viktorianisch empfundenen Themen aktualisieren. So fokussierte er sich in seiner Verfilmung auf Technologische Entwicklung und Angst vor dem Krieg – vor allem in Spaceland.\n\nEine erste Fassung wurde am 14. Januar 2007 auf DVD veröffentlicht, am 12. Mai folgte die Vorführung beim New Haven Underground Film Festival. Es folgten Vorführungen auch in Kanada, Großbritannien und Russland. Für die Vermarktung der DVDs nutzte Ehlinger, der diese Aufgabe zunächst selbst übernahm, auch Soziale Netzwerke wie Youtube und MySpace. Der internationale Vertrieb liegt seit 2010 bei IndieFlix.\n\nBei SciFi.com schreib Paul Di Fillippo über \"Flatland\", dass es dem Team um Ehlinger außerordentlich gut gelungen sei, die Vorlage als Film umzusetzen. \"Flatland\" sei „unterhaltsam, erhellend und bildend“. Trotz der begrenzten Möglichkeiten Ehlingers sei die optische Umsetzung der verschieden-dimensionalen Figuren „durchdacht, wunderschön und leicht verständlich“. Nur der zu häufige Einsatz textlicher Erläuterungen sei störend. Insgesamt sei die Umsetzung dennoch im Geist ihrer Vorlage treu und passe sie behutsam an die heutige Zeit an. Ähnlich störend empfand die Zwischentitel Aylish Wood, die den Film für \"Science Fiction Film and Television\" rezensierte. Trotz des gut funktionierenden Einsatzes von Farbe und Musik zur Vermittlung von Emotionen der Charaktere sei der Film durch zeitweise missglücktem Schnitt, zu großer Länge und inhaltlicher Überladenheit nur mittelmäßig.\n\nLila Marz Harper vergleicht den Film in \"Mathematics in Popular Culture\"mit dem im gleichen Jahr erschienenen Kurzfilm \"\": Ehlingers Werk sei radikaler, indem es näher an der Vorlage sei und die darin thematisierte Prägung der Kinder und Diskriminierung von Frauen zeige, sowie mehr Details aus dem Alltag der geometrischen Formen.\n"}
{"id": "3549935", "url": "https://de.wikipedia.org/wiki?curid=3549935", "title": "Overtime (Film)", "text": "Overtime (Film)\n\nOvertime ist ein französischer Computeranimationsfilm von Oury Atlan, Thibaut Berland (also known as Breakbot) und Damien Ferrie aus dem Jahr 2004. Er entstand an der Supinfocom Valenciennes.\n\nDer Film zeigt eine Gruppe von identischen Stoffpuppen, die große Ähnlichkeit mit Kermit dem Frosch haben. Sie entdecken ihren Erschaffer, einen älteren Animationsfilmer, leblos in seinem Atelier. Seinen Tod nicht wahrhaben wollend, feiern sie mit dem toten Körper eine Party, kochen und essen gemeinsam und gehen ins Kino. So wie zuvor ihr Schöpfer sie bewegt hat, so bewegen nun die Kreaturen ihren toten \"Vater\". \n\nDer Film wird vielfach als Hommage an Jim Henson und an die klassische Puppenfilmanimation verstanden: Die Ära der handgemachten Stop-Motion-Animation neigt sich dem Ende und wird von der neuen, digitalen Computeranimation abgelöst – gleichwohl ihre Geschöpfe (zum Beispiel Kermit der Frosch) in den Herzen der Fans weiterleben. \n\n\n"}
{"id": "3555121", "url": "https://de.wikipedia.org/wiki?curid=3555121", "title": "OpenVAS", "text": "OpenVAS\n\nOpenVAS (\"Open Vulnerability Assessment System\") ist ein Framework aus verschiedenen Diensten und Werkzeugen und bildet eine Lösung für Schwachstellen-Scanning und Schwachstellen-Management.\n\nDer eigentliche Sicherheits-Scanner wird ergänzt durch einen täglich aktualisierten Feed-Service mit sogenannten Network Vulnerability Tests (NVTs). Mit Stand Juni 2016 beinhaltet dieser Feed über 47.000 NVTs.\n\nSämtliche OpenVAS Komponenten sind freie Software. Die meisten davon sind unter der GPL lizenziert.\n\nOpenVAS wird auch vom BSI angeboten.\n\nOpenVAS (initial gestartet als \"GNessUs\", dann aber sehr bald umbenannt) ist eine freie Sicherheitssoftware.\n\nOpenVAS wurde von Nessus abgespalten, als Nessus 2005 zu einer proprietären Lizenz wechselte. Um weiterhin eine freie Version zu haben, wird OpenVAS seitdem auf Basis der letzten freien Version von Nessus weiterentwickelt. Version 1.0 erschien im Oktober 2007.\n\nIm Jahr 2006 war die offizielle Webpräsenz einige Zeit lang nicht erreichbar. Mitte 2007 wurde OpenVAS wieder in die Liste der Projekte von SPI aufgenommen.\n\nDie Version 8.0 wurde im April 2015 veröffentlicht. Nach fast 24 Monaten wurde im März 2017 die aktuelle Version 9.0 veröffentlicht.\n\nDie Umstellung auf GitHub, sowie die Einrichtung eines Community Forums wurde 2018 abgeschlossen.\n\n2019 wurde die Abgrenzung des eigenen Brandings abgeschlossen. OpenVAS stellt nun, wie ursprünglich eigentlich definiert und gestartet, den eigentlichen Schwachstellen-Scanner dar. Der OpenVAS Scanner ist nun in die Grundstruktur des Greenbone Vulnerability Management (GVM) eingebettet.\n\n"}
{"id": "3558446", "url": "https://de.wikipedia.org/wiki?curid=3558446", "title": "HTC Touch", "text": "HTC Touch\n\nUnter der Produktreihe HTC Touch vertreibt der taiwanische Hersteller HTC (High Tech Computer) seit Juni 2007 eine Serie von Mobiltelefonen, welche über einen Touchscreen mittels Fingergesten gesteuert werden. Derzeit gibt es acht Modelle mit den Bezeichnungen \"Touch\", \"Touch Cruise\", \"Touch Dual\", \"Touch Diamond\", \"Touch Pro\", \"Touch HD\", \"Touch 3G\" und \"Touch Viva\". Besonderes Merkmal der HTC-Touch-Reihe ist die Gerätebedienung durch Fingergesten über die sogenannte TouchFLO-Oberfläche, die die Stylus-Bedienung auf dem Windows-Mobile-Betriebssystem weitgehend überflüssig machen soll (Diese wurde Ende 2009 durch die neue \"HTC Sense\"-Oberfläche abgelöst). HTC konkurriert mit diesem Konzept direkt mit dem Apple iPhone.\n\nDas HTC Touch (Modell-ID Elf) ist ein Triband-Handy mit Touchscreen, die Abmessungen sind 99,9 mm × 58 mm × 13,9 mm, bei einem Gewicht von 112 g. Der 2,8\" QVGA-Touchscreen hat eine Auflösung von 240 × 320 Pixeln. Der Prozessor ist ein TI OMAP850, getaktet mit 201 MHz. Es stehen 64 MB RAM und 128 MB ROM zur Verfügung. Außer der Bluetooth-Funktion und einem microSD-Speicherkartenslot, der auch SDHC Karten unterstützt, bietet das Gerät W-LAN. Auf dem Gerät läuft das Betriebssystem Microsoft Windows Mobile 6.0; ein Upgrade auf 6.1. wurde angekündigt, ist aber nie offiziell erschienen. In diversen Foren kursieren jedoch inoffizielle Upgrades auf Windows Mobile 6.1, 6.5 sowie 6.5.3. Die Energie liefert ein auswechselbarer 1100-mAh-Akku. Das Gerät wurde kurze Zeit nach der Einführung auf 128 MB RAM und 256 MB ROM erweitert und von einigen Mobilfunkprovidern als XL-Version vermarktet (Modell-ID Elfin).\n\nDas HTC Touch Cruise ergänzt das Konzept des ersten HTC Touch um einen GPS-Empfänger. Die Abmessungen betragen 110 mm × 58 mm × 15,5 mm bei 130 g Gewicht. Das Gerät hat einen Qualcomm MSM7200 mit 400 MHz, 128 MB RAM und 256 MB ROM. Ein microSD-Speicherkartenslot (unterstützt laut HTC microSDHC mit unbegrenztem Speicher) und ein wechselbarer 1350 mAh-Akku sind eingebaut.\n\nDas HTC Touch Dual kombiniert das Konzept der HTC-Touch-Telefone mit einer ausziehbaren Tastatur. Die Abmessungen 107 mm × 55 mm × 15,8 mm und das Gewicht von 120 g sind dem der anderen HTC-Touch-Geräte sehr ähnlich, ebenso die Auflösung von 320 × 240 Pixeln. Der Prozessor ist ein Qualcomm MSM7200 mit 400 MHz. Die Speicher betragen 128 MB RAM und 256 MB ROM. Als Verbindungsnetze stehen neben dem üblichen GPRS das schnellere EDGE und UMTS mit HSDPA zur Verfügung. Eine W-LAN-Schnittstelle fehlt. Das Gerät hat einen microSD-Speicherkartenslot und einen wechselbaren 1100 mAh-Akku. Das Betriebssystem ist Windows Mobile 6.\n\nDas HTC Touch Diamond ist ein Smartphone mit VGA-Touchscreen. Als Betriebssystem wird Windows Mobile in der Version 6.1 verwendet. Die grafische Oberfläche ist eine Eigenentwicklung von HTC, auf der auch der Webbrowser Opera Mobile in der Version 9.5 läuft. Der Prozessor Qualcomm MSM7201a ist mit 528 MHz getaktet. Der Chipsatz enthält zusätzlich 64 MB RAM zu den eingebauten 128 MB RAM. Somit verfügt das Gerät über 192 MB RAM und ein 256 MB + 4096 MB großes Flash-ROM, das als interne Speicherkarte angesprochen wird. Wie das iPhone kann das Gerät nicht über zusätzliche Speicherkarten erweitert werden. Es kann sich über GPRS, EDGE, UMTS, HSDPA, HSUPA und W-LAN verbinden und ist mit einem GPS-Empfänger mit erweiterter A-GPS-Funktionalität versehen. Die eingebaute Frontkamera kann unter UMTS für Videoanrufe genutzt werden. Die Abmessungen liegen bei 102 mm × 51 mm × 11,5 mm und das Gerät wiegt 110 g. Der 2,8 Zoll große Bildschirm hat eine Auflösung von 480 × 640 Pixeln. Das Gerät wird mit einem 900 mAh-Akku geliefert.\n\nDas HTC Touch Pro ist beinahe baugleich zum HTC Touch Diamond. Durch eine bessere Ausstattung ist es jedoch mit 18,05 mm etwas dicker und mit 165 g etwas schwerer. Das Touch Pro ergänzt bei der Ausstattung das Touch Diamond mit einer ausziehbaren QWERTY-Tastatur, einem größeren wechselbaren Akku mit einer Kapazität von 1360 mAh und besitzt mit 288 MB RAM + 512 MB ROM + microSD-Speicherkartenslot deutlich mehr Speicherreserven als der Bruder. Weiterhin verfügt das Touch Pro über ein LED-Blitzlicht und eine TV-Out-Funktion.\n\nDas HTC Touch HD verfügt über einen 3,8\" großen WVGA-Touchscreen mit einer Auflösung von 480 × 800 Pixeln. Der Prozessor ist wie beim HTC Touch Diamond und HTC Touch Pro ein Qualcomm MSM7201a mit 528 MHz und 288 MB RAM + 512 MB ROM und einen microSD-Speicherkartenslot. Im Lieferumfang ist eine 8 GByte micro SDHC-Karte enthalten, sie kann bei Bedarf durch eine Karte bis zu 32 GByte Größe ersetzt werden. Des Weiteren verfügt es über einen integrierten GPS-Empfänger (mit optionaler QuickGPS und A-GPS-Unterstützung zur schnelleren Signalverarbeitung), eine 5 Megapixel auflösende Kamera und kann sich per WLAN (802.11b/g) oder Bluetooth verbinden. Das Touch HD hält sich (im Gegensatz zum iPhone) an den Bluetooth-Standard Advanced Audio Distribution Profile A2DP, deshalb ist eine Zusammenarbeit mit entsprechenden Bluetooth-Stereo-Geräten wie Kopfhörern und Autoradioschnittstellen anderer Hersteller möglich. Auch zahlreiche andere Bluetooth-Profile wie Modemfunktionen für ein angekoppeltes Notebook sind vorhanden. Es unterstützt Breitbandzugang per UMTS oder HSDPA. Die eingebaute Frontkamera erlaubt die Nutzung von Videoanrufen unter UMTS. Der Telefonteil unterstützt GSM/GPRS/EDGE und Quadband 850/900/1800/1900 MHz. Der Klinkenstecker für das Headset entspricht dem gängigen 3,5 mm-Format, so dass auch Headsets von Fremdanbietern ohne Probleme betrieben werden können. Wenn das Headset eingesteckt ist, dient es gleichzeitig als Antenne für das eingebaute FM-Radio. Das Touch HD läuft unter Windows Mobile 6.1. Ein 1350 mAh-Akku ist wechselbar eingebaut. Mit den Maßen von 115 mm × 62,8 mm × 12 mm und einem Gewicht von 147 g ist von allen Geräten aus HTCs Touch-Familie dem iPhone am ähnlichsten, wobei das Touch HD über eine 2,5-fach höhere Auflösung und einen 7,6 mm (0,3\") größeren Bildschirm verfügt. Das Touch HD wurde am 15. September 2008 von HTC vorgestellt.\n\nDas HTC Touch 3G ähnelt mit 2,8\" QVGA-Touchscreen, den Abmessungen von 102 mm × 53,6 mm × 14,5 mm und 96 g Gewicht sehr dem Touch der ersten Generation. Es bietet aber mit einem Qualcomm MSM7225 Prozessor mit 528 MHz und GPRS, EDGE, UMTS, HSDPA, HSUPA, sowie WLAN einen schnelleren Zugang ins Internet. Der wechselbare Akku hat wie der des ersten Touch eine Kapazität von 1100 mAh.\n\nDas HTC Touch Viva zeigt am deutlichsten Ähnlichkeiten mit dem Touch der ersten Generation. So hat es wie das erste Touch der Serie nur einen TI OMAP850 Prozessor mit 201 MHz, WLAN, GPRS, EDGE und ohne UMTS. HTC vermarktet das Touch Viva deshalb in der Low-Budget-Klasse. Der 2,8\" QVGA-Touchscreen und der wechselbare 1100 mAh-Akku sind genauso groß wie beim ersten Touch. Mit 104,5 mm × 59 mm × 15,75 mm und einem Gewicht von 110 g ist es nur geringfügig größer.\n\nDer erste Nachfolger des Touch Diamond kam im zweiten Quartal 2009 mit vergrößertem Display (3,2\") und der für Windows Mobile untypischen Auflösung von 480 × 800 Pixel auf den Markt. Das Handy kann über Bluetooth, WLAN, GPRS, EDGE, UMTS, HSDPA und HSUPA mit der Außenwelt kommunizieren. Auch ist bei diesem Smartphone die \"HTC Push Internet Technologie\" mit integriert. Wie das Vorgängermodell hat das Touch Diamond II einen integrierten GPS-Empfänger. Des Weiteren befindet sich eine 5-Megapixelkamera mit Autofokus sowie eine Zoomleiste unter dem Bildschirm. Mit 528 MHz bleibt die Prozessorleistung wie beim Vorgänger. Die Oberfläche \"TouchFLO 3D\" ist schon von den Vorgängern bekannt. Die Abmessungen halten sich mit 107,85 mm (L) × 53,1 mm (B) × 13,7 mm (T) im Rahmen. Es wiegt mit Akku 118 Gramm.\n\nDer Nachfolger des Touch Pro unterscheidet sich äußerlich von seinem Vorgänger in der Display-Größe (jetzt 3,6\"), des damit vergrößerten Gewichts (187,5 g), der Tastatur und Gerätegröße (116 mm × 59,2 mm × 17,25 mm (L×B×T)), und durch den Wegfall des D-Pads, welches einer Zoomleiste gewichen ist. Die Zoomleiste ermöglicht das Zoomen von Bildern, Webseiten oder Google Maps™. Zum Schreiben von Nachrichten lässt sich der schwenkbare Bildschirm zur Seite schieben und eine vollständige QWERTZ-Tastatur kommt zum Vorschein. HSDPA und WLAN ermöglichen das schnelle Surfen im World Wide Web. Außerdem wurde wie beim Touch Diamond II die Software und die Oberfläche erweitert. Zusätzlich wurden neue Business-Funktionen wie \"Straight Talk\" (Konferenzfunktion) hinzugefügt.\n\n"}
{"id": "3559635", "url": "https://de.wikipedia.org/wiki?curid=3559635", "title": "Kung Fu Panda", "text": "Kung Fu Panda\n\nKung Fu Panda ist eine US-amerikanische CGI-Animations-Actionkomödie von DreamWorks Animation aus dem Jahr 2008. Regie führten Mark Osborne und John Stevenson, das Drehbuch schrieben Jonathan Aibel und Glenn Berger.\n\nDer Pandabär Po arbeitet im Nudelrestaurant seines Vaters. Bald soll er es übernehmen und die berühmte \"Geheim-Nudelsuppe\", deren letzte geheime Zutat bislang nur der Vater kennt, zubereiten. Nachts aber träumt Po davon, zusammen mit den \"Furiosen Fünf\" (Tigress, Crane, Monkey, Mantis und Viper), den Schülern des berühmten Kung-Fu-Meisters Shifu, gegen das Böse seiner Welt zu kämpfen.\n\nEines Tages wird verkündet, dass der alte Oogway, der Vorsteher des Jadepalastes und Shifus Lehrer, endlich den legendären \"Drachenkrieger\" auswählen wird. Der Überlieferung zufolge wird nur der Drachenkrieger gegen die furchtbare Gefahr bestehen können, die dem Landstrich droht, wenn der böse Kung-Fu-Krieger Tai Lung zurückkehrt. Po, der eigentlich nur der Auswahlzeremonie zusehen will, versehentlich aber ausgesperrt wird, gerät durch seine Bemühungen, doch noch einen Blick auf seine Idole erhaschen zu können, mitten ins Geschehen und wird vom weisen Oogway ausgewählt. Widerwillig nimmt Shifu ihn als sechsten Schüler auf, doch alle Versuche, ihm etwas beizubringen (oder ihn zum Aufgeben zu bewegen), scheitern; nur Pos Hartnäckigkeit und Leidensfähigkeit nötigen den harten Kämpfern nach und nach Respekt ab. Shifus Widerwille hängt vor allem damit zusammen, dass Tai Lung sein Schüler und geliebter Pflegesohn war, der auf der Suche nach mehr Macht jedoch dem Bösen verfiel. Dieser Verrat hat Shifu innerlich erschüttert und seinen inneren Frieden verlieren lassen; daher weigert er sich, sich wieder zu eng an einen Schüler zu binden – worunter auch Tigress als Kind schwer litt.\n\nIn einem langen nächtlichen Gespräch kann Oogway Shifu überzeugen, sich die Ausbildung von Po zum Drachenkrieger zu Herzen zu nehmen; daraufhin tritt der alte Meister seine letzte große Reise an. Schließlich erkennt Shifu, wie er Po erfolgreich trainieren kann: Er beobachtet den dicken Panda, wie dieser unter instinktivem Einsatz akrobatischer Fähigkeiten auf der Suche nach Essen die Schränke in der Vorratskammer aufbricht. Mit Hilfe von Leckerbissen als Kampfpreis kann Pos Talent geweckt werden. Die beiden beginnen mit dem Training, und Po steigert sich in nur kurzer Zeit zum talentierten Kung-Fu-Kämpfer.\n\nNach seinem Ausbruch aus dem speziell für ihn erbauten Chorh-Gom-Gefängnis macht sich Tai Lung auf den Weg, um Rache zu nehmen und den Titel des angesehenen Drachenkriegers zu beanspruchen. Die Furiosen Fünf werfen sich ihm entgegen, werden aber schmählich geschlagen. Um gegen Tai Lung zu bestehen, bekommt Po die heilige Drachenrolle, die nur der Drachenkrieger lesen darf und die das tiefste Geheimnis des Kung Fu enthalten soll. Doch der einzige Inhalt der Rolle ist seltsamerweise eine blanke, goldene Spiegeloberfläche.\n\nVerzweifelt darüber, dass selbst das große Geheimnis der Drachenrolle sie im Stich gelassen hat, befiehlt Shifu die Evakuierung des Dorfes; er allein wird sich Tai Lung, seinem größten Fehler, stellen. Po gesellt sich zu seinem Vater, und dieser verrät ihm noch die geheime Zutat seiner Geheim-Nudelsuppe: „Nichts“ – denn damit etwas etwas Besonderes wird, muss man nur glauben, dass es besonders ist. Durch diese Worte versteht Po die Weisheit hinter der Drachenrolle. Er eilt Shifu im letzten Moment zu Hilfe, als dieser schon vom Kampf mit Tai Lung geschwächt ist. Mit seiner neu gewonnenen Zuversicht, unter Einsatz seines kräftigen Körperbaus und schließlich durch Anwendung des geheimnisvollen \"Wuxi-Fingergriffs\" gelingt es ihm, Tai Lung im Kampf zu besiegen. Die Dorfbewohner können in ihr Dorf zurückkehren, die Furiosen Fünf erkennen Po als einen der Ihren an, und Shifu findet endlich nach Jahren seinen inneren Frieden wieder.\n\nTodd McCarthy schrieb in der \"Variety\" vom 15. Mai 2008, der Film wirke optisch \"„nett“\", sei aber \"„schwer formelhaft“\". Geschichte und Klamauk seien \"„perfekt“\" an ein Kinderpublikum angepasst, was für das erwünschte Einspielergebnis sorgen könne. Die Situationen, die Charaktere und deren Motive seien allerdings \"„extrem rudimentär“\" und ohne Nuancen.\n\nKirk Honeycutt schrieb in der Zeitschrift \"The Hollywood Reporter\" vom 15. Mai 2008, der Film sei ebenso witzig wie hektisch und könne in unterschiedlichen Altersgruppen und Kulturen ein Publikum ansprechen. Die Animation sei sauber und lebendig.\n\nDer Film war 2008 für den Teen Choice Award nominiert. Er erhielt im selben Jahr den Box Office Germany Award in Gold für mindestens zwei Millionen Kinobesucher in 20 Tagen. 2009 war \"Kung Fu Panda\" für einen Golden Globe und den Oscar nominiert.\n\nDie Deutsche Film- und Medienbewertung FBW in Wiesbaden verlieh dem Film das Prädikat wertvoll.\n\nDie Weltpremiere fand am 15. Mai 2008 auf den Internationalen Filmfestspielen von Cannes (außerhalb des Wettbewerbes) statt. Die breite Veröffentlichung in den Kinos startete in Russland am 5. Juni 2008 und in den USA einen Tag später. Öffentlich der deutschen Presse vorgestellt wurde der Film in Berlin am 23. Juni 2008 im Shaolin Tempel Deutschland. Der deutsche Kinostart folgte am 3. Juli 2008. Der Film spielte weltweit ca. 632 Millionen US-Dollar ein. Die DVD zum Film von Paramount Home Entertainment wurde in Deutschland am 21. November 2008 veröffentlicht.\n\nEin Manga des japanischen Autors Hanten Ōkuma und des Mangakas Takafumi Adachi zum Film sollte ab September 2008 in Japan im Magazin \"Kerokero Ace\" des Verlages Kadokawa Shoten erscheinen.\n\nZum Film wurde auch ein Videospiel veröffentlicht.\n\nDie Titelsequenz zeigt die einzelnen Figuren des Films, versehen mit den Namen der entsprechenden prominenten, englischsprachigen Sprecher und einem entsprechenden chinesischen Schriftzeichen.\nIn der deutschen Fassung sind jedoch nicht alle Sprecher der Hauptfiguren bekannt aus Film, Funk und Fernsehen, sondern professionelle Synchronsprecher, weswegen man sich bei DreamWorks dafür entschieden hat, den Namen dieser Personen wegzulassen und an dieser Stelle neben den Figuren ein freies Feld zu lassen.\n\nDie Synchronisation erfolgte durch die Berliner Synchron GmbH Wenzel Lüdecke. Tobias Meister schrieb das Synchronbuch und führte die Dialogregie.\n\nAm 16. Juni 2011 startete die in 3D gedrehte Fortsetzung in Deutschland unter dem Titel \"Kung Fu Panda 2\".\n\nIm September 2012 teilte DreamWorks mit, dass im März 2016 ein dritter Teil des Films in die Kinos kommen werde. Die zwischenzeitliche Ankündigung, der Film werde bereits Ende 2015 veröffentlicht, wurde wieder zurückgenommen; laut DreamWorks soll der Film nicht in Konkurrenz treten mit \"\", dem siebten Teil der Star-Wars-Reihe, der im Dezember 2015 veröffentlicht wurde. Am 16. Januar 2016 feierte Kung Fu Panda 3 seine Premiere und startete regulär am 29. Januar in Nordamerika in den Kinos.\n\nEine Serie mit dem Titel \"Kung Fu Panda: Legenden mit Fell und Fu\" lief im Jahr 2011 auf Nickelodeon an. 2018 startete mit \"\" eine zweite Serie.\n\n\nEbenfalls produziert wurde eine Fernsehserie für die Nickelodeon-Studios mit dem Titel \"Kung Fu Panda – Legenden mit Fell und Fu\" (engl. \"Kung Fu Panda: Legends of Awesomeness\"). Drei Staffeln mit insgesamt 80 Folgen wurden produziert. Die erste Folge, \"The Scorpion's Sting\" („Der Stich des Skorpions“), wurde am 19. September 2011 als Sneak Preview auf Nickelodeon ausgestrahlt.\n\n"}
{"id": "3559968", "url": "https://de.wikipedia.org/wiki?curid=3559968", "title": "Splashtop", "text": "Splashtop\n\nSplashtop ist eine Produktfamilie mit \"Instant-on\"-Betriebssystem und verschiedenen Remote-Desktop-Lösungen des Softwareentwicklungsunternehmens \"Splashtop Inc\".\n\nIm Jahr 2006 als DeviceVM gegründet, brachte das Unternehmen zunächst eine spezielle Linux-Distribution heraus, die als besonders schnelles und vor Schadsoftware geschütztes „Virtual Appliance Environment“ (VAE) von PC-Hauptplatinen-Herstellern ins BIOS ihrer Produkte integriert werden konnte.\n\nInzwischen ist das Unternehmen aber vor allem durch „Splashtop Remote“ bekannt, eine Fernsteuerungs-Software für die Betriebssysteme Windows und Mac OS, die als Clients auch iPhone, iPod touch und iPad, den Kindle Fire oder Geräte mit Android bzw. HP webOS ermöglicht.\n\nDie neuere Produktlinie aus Desktop-Remote-Software besteht aus folgenden Anwendungen:\n\n\n\"Splashtop\" war zunächst als Embedded-Lösung für einen festen Einbau in Hardware vorgesehen. Anwender können sich noch ein oder mehrere zusätzliche Betriebssysteme installieren, haben aber auch ohne Installationsaufwand schon beim Systemstart eine Umgebung für verschiedene Anwendungen zum Surfen und Telefonieren übers Internet zur Verfügung. Weitere Vorteile bietet die Lösung bei Wartung und Reparatur, zum Beispiel für die Beseitigung von Schadsoftware.\n\nDas System ist in nur etwa fünf Sekunden einsatzbereit und wird deshalb auch als „instant-on“ vermarktet.\n\nVorteile der „Instant-on“-Technologie sind:\n\nNachteile sind unter anderem:\n\nAktuellere Versionen werden von größeren Computerherstellern in eigenem Design und mit eigener Markenbezeichnung, als \"Insta-boot\" (Acer), \"ExpressGate\" (Asus), \"Latitude ON\" (Dell), \"QuickWeb\" (HP), \"Quick Start\" (Lenovo), \"Smart ON\" (LG) und \"QuickWeb\" (Sony) vertrieben.\n\nSplashtop besaß schon in der Betaversion eine grafische Benutzeroberfläche, einen Webbrowser auf Basis des Mozilla Firefox 3 mit Unterstützung des Adobe Flash Player 10 und IP-Telefonie mit Skype. Es verwendete Bootsplash, SquashFS, Blackbox (Fenstermanager), SCIM und den Linux-Kernel 2.6.20. Im Januar 2008 kamen LinDVD, ein Software-DVD-Player von Corel, Fotomanager, NTFS-Unterstützung für das Lesen und Schreiben auf Windows-Partitionen und Pidgin Instant Messaging hinzu. Die darauf folgende Version 2.0 verbesserte den Umgang mit UMTS, erlaubte benutzerspezifische Anpassungen des Desktop und ließ sich erstmals auch über Touchscreen steuern. Das Netbook Lenovo Ideapad S10-3t mit \"QuickStart\", der Asus EeePC T91MT mit \"ExpressGate\" und weitere Geräte von Acer, LG und HP mit \"QuickWeb\" gehörten zu den ersten, die mit Splashtop 2.0 ausgeliefert wurden.\n\nSplashtop erforderte zunächst einen eigenen mindestens 512 MB großen Flash-Speicher auf dem Mainboard, weshalb er vor allem Premium-Boards vorbehalten war, wie das ASUS \"P5E3 Deluxe/WiFi-AP\", auf dem er erstmals eingesetzt wurde. Eine proprietäre Kernsoftware startet mit dem BIOS und lädt eine angepasste Linux-Distribution namens \"Virtual Appliance Environment\" (VAE) nach. Während diese VAE läuft, kann der Anwender „Virtuelle Applikationen“ (VA) starten. Für das niedrigere Preissegment können die Linux-Dateien auf der Festplatte abgelegt und über das BIOS von dort geladen werden.\n\nIm Februar 2011 entschied man sich, mit Splashtop OS auf die Plattform \"MeeGO\" zu wechseln und damit die Möglichkeit zu eröffnen, Anwendungen über den Intel App-Store nachinstallieren zu können. Kurz darauf stand das System als kostenloser Download auch für Endanwender zur Nachinstallation auf Festplatte zur Verfügung. Im November folgte die Umstellung des Browsers von Mozilla Firefox auf Google Chrome und der Standardsuche von Google zu Bing. Seit Frühjahr 2012 steht diese Version des Splashtop OS jedoch nicht mehr zur Verfügung.\n\n\n\n"}
{"id": "3560471", "url": "https://de.wikipedia.org/wiki?curid=3560471", "title": "XScreenSaver", "text": "XScreenSaver\n\nXScreenSaver ist ein freier Bildschirmschoner für unixoide Betriebssysteme, die das X Window System verwenden. Er wurde von Jamie Zawinski entwickelt und steht unter der MIT-Lizenz.\nDer XScreenSaver ist insbesondere auf den freien Unix-Systemen (GNU/Linux u. a.) verbreitet. Allerdings haben die Desktop-Umgebungen KDE und GNOME eigene Bildschirmschoner (\"kscreensaver\" bzw. \"gnome-screensaver\") entwickelt.\n\nEin Grund für die Popularität des XScreenSaver ist auch die Möglichkeit für Programmierer, eigene Module zu entwickeln. So lassen sich zum Beispiel OpenGL-basierte Animationen wie \"GLMatrix\" hinzufügen. Ein Verwaltungssystem ermöglicht es die Module zu konfigurieren, zum Beispiel mit welcher Geschwindigkeit die Animationen abgespielt werden sollen. Manche Module verarbeiten auch Bilddateien oder Texte, die frei einstellbar sind.\n\nNeben der Veränderung des Bildschirminhaltes, lässt sich die aktuelle X-Session sperren, so dass der Benutzer sein Passwort eingeben muss, um wieder an die Bildschirminhalte zu kommen.\n\nDie Paketierer von Debian entfernten 2016 gegen den Willen des Autors eine Warnung aus dem Programm, die beim Start einer alten Programmversion erschien.\n\n"}
{"id": "3567181", "url": "https://de.wikipedia.org/wiki?curid=3567181", "title": "Altova XMLSpy", "text": "Altova XMLSpy\n\nXMLSpy ist ein XML-Editor und eine integrierte Entwicklungsumgebung von Altova. XMLSpy ermöglicht Entwicklern das Erstellen von XML-basierten Webservice-Applikationen unter Verwendung von Technologien wie XML, XML Schema, XSLT, XPath, XQuery, WSDL und SOAP. XMLSpy ist auch als Plug-In für Microsoft Visual Studio und Eclipse erhältlich.\nXMLSpy ist mit geschätzten 2,3 Millionen Installationen Marktführer im Bereich der XML-Editoren, das Produkt wird 2- bis 3-mal pro Jahr aktualisiert, um neuen Technologien wie zum Beispiel der viel diskutierten OOXML-Spezifikation von Ecma International Rechnung zu tragen.\n\nXMLSpy bietet mehrere Ansichten und Editierfunktionen für die folgenden Zwecke:\n\n\nXMLSpy ist ein lizenziertes Software-Produkt, das gegen nicht lizenzkonforme Verwendung mit einem Keycode geschützt ist, Upgrades auf neue Versionen sind über ein optionales Support- und Wartungspaket möglich. Service Pack Releases mit Fehlerbehebungen stehen allen Benutzern der aktuellen Software-Version zur Verfügung.\n"}
{"id": "3567504", "url": "https://de.wikipedia.org/wiki?curid=3567504", "title": "GNAT", "text": "GNAT\n\nGNAT ist der Ada-Compiler des GNU-Projektes. Ursprünglich war der Name ein Akronym für \"GNU NYU Ada Translator\", dies gilt heute allerdings nicht mehr. Eine Besonderheit von GNAT ist, dass sowohl das Frontend als auch die Laufzeitbibliotheken vollständig in Ada geschrieben sind. Als Backend nutzt GNAT Komponenten der GNU Compiler Collection, deren Bestandteile daher zur Laufzeit verfügbar sein müssen.\n\nDer Compiler wird unter den Bedingungen der GNU General Public License (GPL) vertrieben. Die Laufzeitbibliotheken unterliegen einem dualen Lizenzierungsmodell. Entweder findet die GPL (\"GNAT GPL Edition\" von \"AdaCore\"), oder die GNAT Modified General Public License (GCC, \"GNAT Pro\"), neuerdings die GCC Runtime Library Exception Anwendung. GNAT ist über die offiziellen Repositorys aller größeren Linux-Distributionen verfügbar, ebenso über die FreeBSD-Ports.\n\njGNAT war eine GNAT-Version, welche JVM-Bytecode aus Ada-Quellen erzeugt. Ihre Entwicklung stockte, wurde aber 2008 von AdaCore wieder aufgenommen und zur Ada-Java Interfacing Suite (AJIS) weiterentwickelt und mit zusätzlichen Funktionen zum Interfacing mit Java ausgestattet. Sie wird in der GPL-Edition von GNAT Libre 2009 unter GPL auch zur Entwicklung von FLOSS zur Verfügung stehen. 2013 ist die GPL-Edition für die JVM auf Windows herausgegeben worden.\n\nDas Projekt startete im Jahr 1992, als die United States Air Force die New York University (NYU) beauftragte, einen quelloffenen und frei verfügbaren Compiler zu entwickeln, um die Standardisierung der Ada-9x-Spezifikation zu unterstützen. Der Drei-Millionen-Dollar-Vertrag mit der Universität verlangte eine Publikation des Quellcodes unter GNU GPL und die Abgabe der Rechte an die Free Software Foundation. Im Jahr 1995 wurde GNAT erstmals validiert und war damit die erste freie Referenzimplementierung von Ada 95.\n\nIn den Jahren 1994 und 1996 gründeten die ursprünglichen GNAT-Entwickler zwei Firmen, \"Core Technologies\" in New York City und \"ACT-Europe\" in Paris, um GNAT fortlaufend weiterzuentwickeln und kommerzielle Unterstützung für Firmen anzubieten. Im Jahr 2004 fusionierten die beiden Unternehmen und firmieren seitdem als \"AdaCore\".\n\nDie GNAT-Quelltexte wurden anfangs getrennt von den GCC-Quellen vertrieben. Erst seit dem 2. Oktober 2001 sind die GNAT-Programmteile im GCC-CVS-Repository enthalten. Die letzte eigenständige GNAT-Veröffentlichung war \"GNAT 3.15p\", welche auf \"GCC 2.8.1\" aufsetzte und am 2. Oktober 2002 das Licht der Öffentlichkeit erblickte. Angefangen bei GCC 3.4, erreichten alle folgenden Versionen der GNU Compiler Collection 100 % des Ada-Standardisierungstests ACATS auf den offiziell unterstützten Architekturen. Ab GCC 4.0 gelang es sogar mehreren exotischen Architekturversionen 100 % im ACATS zu erreichen.\n\n"}
{"id": "3571236", "url": "https://de.wikipedia.org/wiki?curid=3571236", "title": "Mac OS X Leopard", "text": "Mac OS X Leopard\n\nLeopard, vollständig Mac OS X Leopard 10.5, ist die sechste Hauptversion von macOS, dem Desktop-Betriebssystem von Apple, das seinerzeit unter dem Namen Mac OS X eingeführt wurde. Es folgte auf Mac OS X Tiger 10.4 und wurde ab dem 26. Oktober 2007 ausgeliefert, nachdem es aus Kapazitätsgründen (zur rechtzeitigen Fertigstellung des Mobiltelefons iPhone) um etwa ein halbes Jahr verschoben werden musste.\n\nZu den größten Neuerungen gehörte, abgesehen von einer modernisierten Oberfläche (transparente Menüleiste, dreidimensionales Dock mit „“, virtuelle Arbeitsbereiche „“, einheitliches Layout, neue Icons), unter anderem ein neuer Finder mit neuer Seitenleiste, der „“-Ansicht, der Dateivorschau „“ sowie mit der Möglichkeit, auch andere Rechner im Netz zu durchsuchen. Außerdem ist eine Datensicherungssoftware namens Time Machine integriert. Boot Camp ermöglicht auf mit Intel-Prozessoren ausgerüsteten Apple-Computern die Installation von Microsoft Windows ab Windows XP mit Service Pack 2 parallel zu Mac OS X auf einer abgetrennten Partition. Leopard ermöglicht außerdem den 64-Bit-Betrieb bei Applikationen mit grafischer Benutzeroberfläche. Weitere größere Neuerungen finden sich zudem in Safari, Mail, iCal, iChat, Vorschau und der Kindersicherung.\n\nLeopard darf als erstes Derivat der Berkeley Software Distribution offiziell den Markennamen „UNIX“ (in Großbuchstaben oder Kapitälchen) tragen, da es konform mit der Single UNIX Specification UNIX 03 der Open Group ist.\n\nDie letzte Version 10.5.8 wurde am 5. August 2009 veröffentlicht, das Nachfolge-Betriebssystem heißt Mac OS X Snow Leopard (Version 10.6) und wurde am 28. August 2009 veröffentlicht. Leopard ist die letzte Mac-OS-X-Version, die den PowerPC-Prozessor unterstützt. Alle neueren Versionen laufen nur noch auf Intel-Macs.\n\nMac OS X 10.5 „Leopard“ enthält zahlreiche Programme, die häufig gebrauchte Funktionen erfüllen. Multimedia-Anwendungen fehlen im Lieferumfang des Systems zwar fast vollständig, werden aber mit einem Programmpaket namens \"iLife\" zusammen mit der Apple-Hardware ausgeliefert. Für Büroanwendungen enthält Mac OS X lediglich das einfache Textverarbeitungsprogramm TextEdit und den Kalender iCal.\n\nDie Classic-Umgebung wurde mit Mac OS X 10.5 „Leopard“ eingestellt, sodass das Virtualisieren von Mac OS 9 und damit das Ausführen älterer Macintosh-Programme damit nicht mehr möglich ist. Es gibt jedoch alternative Virtualisierungsprogramme bzw. Emulatoren, die auch unter Leopard laufen, wie etwa der freie SheepShaver.\n\nMac OS X 10.5 bietet laut Apple im Vergleich zu 10.4 über 300 neue Funktionen. Dazu zählen unter anderem:\n\n\n\nApple gibt für \"Leopard\" die folgenden Systemvoraussetzungen an:\n\n\n\"Leopard\" liegt als Universal Binary vor und unterstützt somit die Prozessorarchitekturen vom Typ PPC (Motorola PowerPC G4/IBM PowerPC G5) und x86 (Intel Core Solo/Core Duo/Core 2 Duo/Xeon).\n\nZum Thema Sicherheit, siehe den Hauptartikel.\n\n"}
{"id": "3571989", "url": "https://de.wikipedia.org/wiki?curid=3571989", "title": "GNAT Programming Studio", "text": "GNAT Programming Studio\n\nDas GNAT Programming Studio (GPS, ehemals bekannt als GNAT Programming System) ist eine Integrierte Entwicklungsumgebung (IDE) der Firma AdaCore für die Programmiersprache Ada. In neueren Versionen unterstützt die IDE allerdings auch weitere Programmiersprachen. Das GPS verwendet die Compiler der GNU Compiler Collection als Backend. Der Name \"GNAT Programming Studio\" leitet sich von der Bezeichnung des GNU Ada Compilers, GNAT, ab.\n\nDas GPS ist für verschiedene Architekturen und Plattformen erhältlich, darunter Linux, Solaris und Windows. Zur Darstellung der Benutzerschnittstelle (GUI) verwendet das GPS die GTK+-Bibliothek. Durch die Veröffentlichung unter der GNAT Modified General Public License ist das GPS freie Software.\n\nGPS unterstützt neben Ada eine Reihe weiterer Programmiersprachen wie C, C++, JavaScript, Pascal und Python. Ferner unterstützt die IDE weitere Dateiformate für Autoconf und Make.\n\nWeitere Funktionen sind Remote-Edit, Remote-Debug und die Fähigkeit, als Cross-Compiler Programme für Architekturen zu erstellen, auf denen GPS nicht nativ verfügbar ist. Weitere Funktionen aktueller Versionen sind, unter anderem, erweiterte Möglichkeiten zum Editieren von Quellcode und eine Autovervollständigen-Funktion.\n\nAls Versionskontrollsysteme werden CVS, Rational ClearCase, Subversion und Git unterstützt.\n\nDas GPS kann alle Dateien öffnen, deren Codierung GNAT unterstützt. Im Einzelnen sind das: ISO-8859-1, ISO-8859-2, ISO-8859-5, ISO-8859-6, ISO-8859-7, KOI8-R, Shift JIS, GB2312, UTF-8, UTF-16 und UTF-32.\n\nGPS benutzt Python als Skriptsprache.\n\n"}
{"id": "3572226", "url": "https://de.wikipedia.org/wiki?curid=3572226", "title": "Windows Small Business Server 2008", "text": "Windows Small Business Server 2008\n\nMicrosoft Windows Small Business Server 2008 ist wie sein Vorgänger Windows Small Business Server 2003 ein Netzwerkbetriebssystem für kleine Unternehmen. Es basiert auf Microsoft Windows Server 2008 und beinhaltet Microsoft Exchange Server 2007 sowie optional Microsoft SQL Server 2008.\n\nMicrosoft Windows Small Business Server 2008 ist in zwei Editionen verfügbar, Standard und Premium.\n\n\n\nOutlook ist im Gegensatz zur Vorversion, Small Business Server 2003, in beiden Versionen nicht mehr enthalten.\n\nIm Lieferumfang befinden sich fünf sogenannte CALs (Client Access License). Das bedeutet, dass fünf Benutzer oder Computer Zugriff auf die vom Server zur Verfügung gestellten Ressourcen haben. Gängig ist es, die Lizenzierung pro Benutzer und nicht pro Gerät zu betreiben. In Betrieben, die Schichtbetrieb betreiben, kann jedoch die Lizenzierung pro Gerät von finanziellem Vorteil sein.\n\nEine CAL für einen Benutzer erlaubt es, jede der im Lieferumfang enthaltenen Technologien des Small-Business-Servers 2008 in vollem Umfang zu nutzen. Da der Small-Business-Server auf kleine und mittelgroße Betriebe ausgelegt ist, beinhaltet die Software eine Beschränkung auf 75 CALs.\n\nBei mehr als 5 CALs sollte die empfohlene Systemanforderung eingehalten werden, da sonst eine Verlangsamung des Servers auftreten kann.\n\nMit Hilfe des Transitionpacks kann der Small-Business-Server 2008 in seine Einzelkomponenten überführt werden. Nach Anwendung des Transitionpacks besitzt man sowohl technisch als auch lizenzrechtlich die Basiskomponenten des SBS2008 in seiner jeweiligen Version. Die Beschränkungen des SBS2008 gegenüber den Basiskomponenten sind dann aufgehoben. Das betrifft vor allem die Lizenzanzahlbeschränkung auf 75 CAL und die Rollenbeschränkung des Servers.\n\nKleine Unternehmen mit je nach Lizenzierung bis zu 75 Mitarbeitern oder bis zu 75 Clientcomputern erhalten ein Werkzeug, das die zentrale Verwaltung der im Unternehmen eingesetzten EDV-Mittel ermöglicht. Gegenüber einer Lösung aus unabhängigen Einzelcomputern mit Desktopbetriebssystemen erhält man\n\nFolgende Besonderheiten gegenüber einem einzelnen Windows Server 2008 Standard gelten:\n\n\n"}
{"id": "3575656", "url": "https://de.wikipedia.org/wiki?curid=3575656", "title": "Jitsi", "text": "Jitsi\n\nJitsi (französische Schreibweise für das bulgarische жици, \"Drähte\", ausgesprochen []), früher \"SIP Communicator\", ist eine multiprotokollfähige IP-Telefonie- und Instant-Messenger-Anwendung, mit der auch Videotelefonie möglich ist. Jitsi ist Free/Libre Open Source Software (FLOSS) und komplett in Java implementiert, daher auf allen Plattformen mit entsprechender Java-Laufzeitumgebung einsetzbar.\nInzwischen basiert Jitsi hauptsächlich auf WebRTC.\n\nDie Entwicklung von Jitsi, damals noch „SIP Communicator“ (SIP ist ein Akronym für Session Initiation Protocol), startete im Jahr 2003. Ende 2006 war das Projekt so weit fortgeschritten, dass eine erste Alpha-Version veröffentlicht werden konnte. Eine erste grafische Benutzeroberfläche war vorhanden, die wichtigsten Protokolle bereits implementiert und alle gängigen Betriebssysteme (Windows, Linux und OS X) wurden unterstützt.\n\nIm November 2007 erschien die zweite Alpha-Version, die schon wesentlich mehr Funktionalität bot. Neben der Einbindung weiterer Protokolle wurde auch stark auf dem OSGi-Framework aufgesetzt, das für neue Entwickler sehr einstiegsfreundlich ist. Durch Neuerungen wie Benutzer-Avatare oder erweiterte Verlaufs-Funktionalität wurde auch der Komfort erhöht.\n\nDie Zahl an Entwicklern, die an dem Open-Source-Projekt mitarbeiteten, stieg drastisch an. Dementsprechend konnte die dritte Alpha-Version bereits im Februar 2008 erscheinen. Hierbei wurde der Schwerpunkt auf Konferenz-Chats und Internationalisierung (Einbindung der bekanntesten Sprachen) gelegt.\n\n2011 wurde das Projekt in Jitsi umbenannt, da es sich nach der hinzugefügten Unterstützung von Audio- und Video-Gesprächen nicht mehr nur um ein SIP-Softphone, sondern um einen Instant Messenger mit Audio- und Videotelefoniefunktion handelt. Nightly Builds wurden im März 2011 zu Beta-Versionen erkoren.\n\nDen Audio-Codec \"SILK\" unterstützt Jitsi seit 2011.\n\nBeim Vergleich von Jitsi mit Skype, Ekiga, Empathy und NullTeam Yate q14 durch Chip Linux im März 2014 war Jitsi Testsieger mit 5 von 5 Punkten.\n\nDas Projekt wechselte im Jahr 2015 von der LGPL zur Apache-Lizenz, um die Hürden bei der Integration von Jitsi in andere Software zu senken.\n\nSeit 2015 kann Jitsi auch als JavaScript Library direkt im Browser ohne Softwareinstallation ausgeführt werden, einstweilen für Chrome und Opera sowie für Mozilla Firefox ab der Version 40. Mit der Jitsi Videobridge können Konferenzräume realisiert werden, die mit geringen Server-Ressourcen auskommen und sparsam mit der Bandbreite sind. Ein solcher Service wird von den Jitsi Entwicklern auch selbst angeboten.\n\nJitsi unterstützt in allen Protokollen Präsenz und Sofortnachrichten. In den meisten Fällen ist auch Dateiübertragung möglich. (Video-)Telefonie ist derzeit mit SIP und Jingle möglich. Die Unterstützung der Google-Variante von Jingle (Google Talk) ermöglicht auch Verbindungen von einem Gmail-Konto zu Android-Geräten. Darüber hinaus bietet Jitsi einige interessante Merkmale:\n\nJitsi unterstützt alle gängigen Protokolle bekannter Instant Messenger. Teilweise befindet sich die Implementierung der unterstützen Protokolle allerdings noch in der Entwicklung und kann daher eventuell nicht zu einhundert Prozent die gewünschten Ergebnisse liefern. Zu den bereits implementierten Protokollen gehören:\n\n\n\n\n\nSIP-Communicator beziehungsweise Jitsi ist ein langjähriger Teilnehmer beim Google Summer of Code. Dieses Förderprogramm vergibt Programmierstipendien an Studenten aus aller Welt, um Weiterentwicklungen für Freie-Software-Projekte zu erarbeiten. Ein beträchtlicher Teil der Jitsi-Codebasis wurde von GSoC-Teilnehmern beigesteuert, so auch 2011\n\n\n"}
{"id": "3580234", "url": "https://de.wikipedia.org/wiki?curid=3580234", "title": "Series/1", "text": "Series/1\n\nDie IBM Series/1 Computer waren 16-bit Minirechner, welche 1976 vorgestellt wurden als Konkurrent zur PDP-11 von Digital Equipment Corporation und ähnlichen Geräten von Data General und HP. Series/1-Rechner wurden gewöhnlich zur Steuerung und Betrieb externer elektromechanischer Komponenten eingesetzt. \n\nFür die Series/1 gab es 2 unterschiedliche Betriebssysteme: Event Driven Executive (EDX) oder Realtime Programming System (RPS). Systeme mit EDX wurden meist mittels Event Driven Language (EDL) programmiert, obwohl es mit Fortran IV, PL/1 und Cobol auch Hochsprachen gab. \n\nDie Series/1 nutzte intern die Zeichenkodierung EBCDIC und lokal angeschlossene EBCDIC-Terminals, ASCII-basierte, entfernte Terminals und Geräte konnten über eine I/O-Karte mit einer RS-232-Schnittstelle verbunden werden. \n\nDie Produktion der IBM Series/1 Rechner wurde Ende der 1980er Jahre eingestellt. \n\n\nSeries/1-Rechner wurden unter anderem in der Fertigung bei General Motors eingesetzt.\nAuch das United States Marine Corps war ein Großkunde für Series/1. Die Deutsche Bundespost setzte bis Ende der 1980er-Jahre flächendeckend Series/1-Computer als Zugangsrechner für das Bildschirmtext-System ein. Durch einen GAO-Report wurde im Mai 2016 publik, dass die militärischen nuklearen Kräfte der USA im Verteidigungsministerium aktuell noch über eine Series/1-Anwendung koordiniert werden und dieses System erst Ende 2017 aktualisiert werden soll.\n\n"}
{"id": "3580534", "url": "https://de.wikipedia.org/wiki?curid=3580534", "title": "Windows-1257", "text": "Windows-1257\n\nWindows-1257 ist eine 8-Bit-Zeichenkodierung des Betriebssystems Microsoft Windows und wird benutzt, um estnische (auch möglich mit Windows-1252), lettische und litauische Zeichen zu kodieren. Vom ISO-typischen Block 80–9F abgesehen, ähnelt Windows-1257 stark der Kodierung ISO 8859-13.\n\nDie obere Hälfte ist identisch mit ASCII. Die Stellen, in der die Kodierung von ISO 8859-13 abweicht, sind hellblau markiert.\n\n"}
{"id": "3588758", "url": "https://de.wikipedia.org/wiki?curid=3588758", "title": "XML Localization Interchange File Format", "text": "XML Localization Interchange File Format\n\nDas XML Localization Interchange File Format (engl. für „XML-Dateiformat zum Austausch von Lokalisierungsdaten“), abgekürzt XLIFF, ist ein XML-Format zur Darstellung hierarchisch strukturierter Inhalts-Daten in CAT-Werkzeugen. Das Format ist speziell für den verlustarmen Austausch von Übersetzungsdaten und den dazugehörigen Kontextinformationen gedacht. Es ist XML-basiert und damit erweiterbar.\n\n\n"}
{"id": "3606701", "url": "https://de.wikipedia.org/wiki?curid=3606701", "title": "MindView", "text": "MindView\n\nMatchWare MindView Pro/Business (ehemals OpenMind) ist eine kommerzielle Anwendungssoftware zur Erstellung von Mind Maps.\n\nMindView erlaubt das Erstellen und Editieren sog. Business Maps. MindView bietet zahlreiche Funktionen zur Visualisierung bzw. zum optischen Verknüpfung von Mind-Map-Inhalten und zusätzlichen Dateien (Anhängen beliebiger Dateien).\n\nDie Erstellung von sogenannten Business Maps basiert ursprünglich auf der Papier-und-Bleistift-Methode „Mind Mapping“, bietet allerdings die computertypischen Vorzüge des schnellen Änderns von Beziehungen (Drag & Drop von Zweigen).\n\nBis MindView 5 gab es die Versionen „Pro“ und „Business“, wobei letztere zusätzlich eine Gantt-Ansicht enthielt und damit auch zur Aufgaben- bzw. Projektplanung einsetzbar war. Inzwischen gibt es nur noch eine Version mit sämtlichen Möglichkeiten\n\nZwischen folgenden Ansichten kann jederzeit umgeschaltet werden:\n\nMind Maps können in folgende Formate exportiert werden:\n\nAußerdem können Projekte aus Power Point und Microsoft Project importiert werden.\n\nDie Mind Map kann mit der Aufgabeliste in Microsoft Outlook synchronisiert werden. Projektpläne können in MS Project importiert und dort weiterbearbeitet werden.\n\n\n"}
{"id": "3610601", "url": "https://de.wikipedia.org/wiki?curid=3610601", "title": "SURF", "text": "SURF\n\nSURF (englisch, \"Speeded Up Robust Features\", frei übersetzt: „Beschleunigte, robuste Merkmale“) ist ein Algorithmus von Herbert Bay et al. zur schnellen und robusten Erkennung von Bildmerkmalen für maschinelles Sehen. Die Anwendung von diesem Algorithmus ist patentiert. SURF ersetzt die in SIFT verwendeten Gauß-Filter durch Mittelwertfilter, welche durch die Verwendung von Integralbildern mit konstantem Zeitaufwand berechnet werden können.\n\n\n"}
{"id": "3610760", "url": "https://de.wikipedia.org/wiki?curid=3610760", "title": "Part-of-speech-Tagging", "text": "Part-of-speech-Tagging\n\nUnter Part-of-speech-Tagging (POS-Tagging) versteht man die Zuordnung von Wörtern und Satzzeichen eines Textes zu Wortarten (). Hierzu wird sowohl die Definition des Wortes als auch der Kontext (z. B. angrenzende Adjektive oder Nomen) berücksichtigt.\n\nDie Erfassung und Kennzeichnung der Wortarten wurde ursprünglich manuell durchgeführt, im Laufe der Zeit wurde das Verfahren zunehmend durch die Computerlinguistik automatisiert. Die verwendeten Verfahren können in überwachtes maschinelles Lernen und unüberwachtes maschinelles Lernen unterteilt werden. Beim überwachten Lernen werden z. B. Hidden Markov Models oder Eric Brills Verfahren oder Entscheidungsbäume (nach Helmut Schmid) verwendet, und alle Wortart-Tags stammen aus einem vordefinierten so genannten Tagset. Für das Deutsche wird oft das Stuttgart-Tübingen-Tagset (STTS) verwendet. Beim unüberwachten Lernen steht das Tagset nicht vorher fest, sondern es entsteht durch ein stochastisches Verfahren.\n\nDer Satz \"Petra liest einen langen Roman.\" wird mit dem Stuttgart-Tübingen-Tagset (kurz: STTS) wie folgt getaggt:\nHinter jedem Wort bzw. Satzzeichen steht das Tag nach einem Schrägstrich. Um das Wort \"einen\" im gegebenen Kontext richtig zu taggen, muss man es von den Formen des gleich lautenden Verbs unterscheiden; diese würden mit VVINF (für den Infinitiv) bzw. VVFIN (für die finite Form) getaggt.\n\nBeim überwachten Lernen wird das Tag für \"einen\" mit Hilfe des Kontextes ausgewählt: Aus einem bereits getaggten Textkorpus wurden vorher z. B. die Wahrscheinlichkeiten für die Tag-Folgen VVFIN-ART, VVFIN-VVINF und VVFIN-VVFIN berechnet (so genanntes Training des Taggers). Da VVFIN-ART deutlich häufiger ist als die anderen beiden Folgen, wird \"einen\" in diesem Satz als ART getaggt. (Die häufige Folge \"kann lesen\" wird nicht mit VVFIN-VVINF, sondern mit VMFIN-VVINF getaggt.)\n\nBeim unüberwachten Lernen gibt es kein vorheriges Training, sondern aus den zu taggenden Sätzen selbst wird errechnet, dass z. B. \"einen\" häufig nach \"liest\" oder \"lese\" steht, aber auch häufig am Satzende. \"Den\" dagegen steht häufig nach \"liest\" oder \"lese\", aber nie oder selten am Satzende. \"Lesen\" steht häufig am Satzende und nie nach \"liest\" oder \"lese\". Deswegen erzeugt der Tagger eine Wortart, zu der z. B. \"den\" gehört, und eine andere, die \"lesen\" enthält. \"Einen\" gehört zu beiden Wortarten. Dass es im gegebenen Satz wie \"den\" getaggt werden sollte, ergibt sich nach derselben Argumentation wie für den Tagger, der mittels überwachtem Lernen trainiert wurde.\n\nSoftware im Bereich Computerlinguistik (NLP) ist häufig in der Lage, ein POSTagging automatisiert durchzuführen. Die auf den Bildungsbereich ausgerichtete Software NLTK kann standardmäßig englischsprachige Texte mit dem Tagset Penn Treebank versehen. Zusätzlich ist ein individuell gestaltetes Training mit Hilfe passender Textkorpora möglich.\n\nPOSTagging ist sprachabhängig. Pro Sprache können ein oder mehrere Tagsets existieren. Für deutsche Texte wird von der Open-Source-Software OpenNLP der Tagset STTS benutzt, für englisch Texte der Tagset Penn Treebank. Der für 14 europäische Sprachen entwickelte PAROLE TagSet wird ebenfalls unterstützt. OpenNLP verfügt über eine Auswahl von bereits trainierten Modellen für diese verschiedene Sprachen (Deutsch, Englisch, Spanisch, Portugiesisch, Dänisch usw.). Mit Hilfe dieser Modelle kann dann ein Textkorpus in einer dieser Sprachen automatisch mit den entsprechenden Tags versehen werden.\n\nTreeTagger ist ein von Helmut Schmid am Institut für Maschinelle Sprachverarbeitung der Universität Stuttgart entwickeltes Werkzeug. Mit ihm können Texte aus ca. 16 verschiedenen Sprachen automatisch mit POSTags versehen werden. TreeTagger ist das in der Forschung wohl am häufigsten benutzte sprachunabhängige Werkzeug in diesem Bereich.\n\n\n"}
{"id": "3611089", "url": "https://de.wikipedia.org/wiki?curid=3611089", "title": "Kis Vuk", "text": "Kis Vuk\n\nKis Vuk (englischer Titel: „A Fox’s Tale“) ist eine 3D-animierte Fortsetzung des Zeichentrickfilms \"Vuk\". \n\nDas Budget für \"Kis Vuk\" betrug umgerechnet fünf Millionen Euro.\nUrsprünglich für 2007 geplant, wurde der Kinostart mehrfach bis letztlich auf den 17. April 2008 verschoben. \"Kis Vuk\" war ein Flop mit miserablen Kritiken und befand sich lange auf der IMDb-Liste der 100 schlechtesten Filme. Auf Deutsch ist der Film nicht erschienen. \n\nVuks Sohn verliebt sich in eine Zirkus-Füchsin, und versucht sie zu befreien, die Kinder Alex und Carmella helfen ihm dabei. Auch der Opa vom kleinen Vuk ist im Zirkus gefangen.\n"}
{"id": "3611202", "url": "https://de.wikipedia.org/wiki?curid=3611202", "title": "WinDirStat", "text": "WinDirStat\n\nWinDirStat ist ein freies Programm für Microsoft Windows, welches die Festplattenstruktur analysiert. Dabei wird vom Programm entweder eine Partition oder ein Dateiordner nach Objekten durchsucht und entsprechend angeordnet. So können große Dateien oder große Ordner mit mehreren Teilarchiven aufgespürt werden. WinDirStat wird unter der GNU General Public License veröffentlicht. \n\nIm unteren Bereich wird nach dem erfolgreichen Durchsuchen der Partition oder des Ordners bildlich gezeigt, wie die Festplatte mit welchen Dateien belegt ist. Von WinDirStat wird diese Grafik \"Baumkarte\" (nach engl. \"Treemap\") genannt. Dabei wird jede Datei durch ein Rechteck dargestellt. Dateien im selben Ordner werden selbst auch in Rechtecken angeordnet.\n\nDurch einen Kisseneffekt wird versucht, die Zugehörigkeit zum selben Ordner zu verdeutlichen. So haben Dateien im selben Ordner eine gemeinsame Lichtquelle. Erkennbar wird dies in dem Screenshot bei den GCF-Dateien (violett dargestellt).\n\nZusätzlich zur Baumkarte gibt es auch eine Verzeichnisliste in einer Baumstruktur, welche die Verhältnisse in Zahlen fasst. So kann ermittelt werden, welche Ordner am meisten Platz benötigen. Diese Ordner werden weiter oben dargestellt. In diesem Verzeichnis selber gibt es weitere Verzeichnisse. Zum einen \"<Dateien>\", welche alle Dateien in diesem Ordner zusammenfassen, und zum anderen die Unterordner des Ordners. Dabei sind diese der Größe nach sortiert und der Wert in den Spalten \"Teilbaum-Anteil\" und \"Anteil\" bezieht sich auf den Ordner.\n\nOben rechts befindet sich dann die Typenliste, welche, nach der Größe der Vorkommen sortiert, alle Dateitypen aufzählt. Es werden dabei folgende Werte erfasst:\n\n"}
{"id": "3613190", "url": "https://de.wikipedia.org/wiki?curid=3613190", "title": "Fn-Taste", "text": "Fn-Taste\n\nDie Fn-Taste ist eine spezielle Taste, die man meist nur auf Notebooks oder Netbooks findet. Dabei steht „Fn“ für „Funktion“ bzw. „function“ im Englischen. Mithilfe dieser Taste kann herstellerspezifisch eine weitere Ebene erreicht werden, vergleichbar mit der Alt-Taste oder Steuerungstaste. Dabei hat die Funktionstaste keine Kombinationen mit der Umschalt-, Alt- oder Steuerungstaste.\n\nBei fast jedem Notebook, bei dem kein separater Ziffernblock vorhanden ist, schaltet die Fn-Taste einen Block auf die Funktion des Ziffernblocks um. Dieser Block befindet sich, angenähert an normale Tastaturen, meistens im rechten Bereich, wobei die Tasten in der Regel Buchstabentasten sind. Auch befindet sich meistens in der Zeile der Funktionstasten die Möglichkeit, die Bildschirmhelligkeit zu steuern, den Videoausgang zu wählen und anderes.\n\nDie Einstellung, ob zusätzliche Funktionen über die Fn-Taste aktiviert werden können oder aber über die Tasten bis , erfolgt im BIOS (UEFI)-Menü.\n\nDie Position der Fn-Taste variiert je nach Hersteller und Modell: in der Regel rechts von der linken Strg-Taste, oder alternativ in der linken unteren Ecke der Tastatur.\n\nWenn eine Tastatur ohne Fn-Taste an einem Notebook angeschlossen ist, kann manchmal mit der Tastenkombination + + bis die jeweilige Funktion aktiviert werden.\n\nWenn die Fn-Taste eingerastet ist, gibt es mehrere Möglichkeiten, um diese wieder zu lösen. Einerseits wäre dies die Kombination + + , bei vielen anderen Modellen ist es die Kombination + . Manchmal reicht es auch einfach, zu betätigen, oder aber nur +. In seltenen Fällen gibt es ähnlich der Umschaltsperre (Caps lock) eine Taste .\n"}
{"id": "3614444", "url": "https://de.wikipedia.org/wiki?curid=3614444", "title": "ShiftN", "text": "ShiftN\n\nShiftN ist eine freie Grafiksoftware zur automatischen Korrektur stürzender Linien sowie zur automatischen Drehung von Fotos. Sie stammt von Marcus Hebel. Der Name der Software lehnt sich an die Bezeichnung „Shiften“ für die Verwendung eines Shift-Objektivs zum Beispiel in der Architekturfotografie an.\n\nDie Software simuliert die Verwendung eines sogenannten Shift-Objektivs. ShiftN ermittelt zunächst im Foto Kanten und Linien, wobei nur solche beachtet werden, die im Winkel nicht zu sehr von den vertikalen Bildrändern abweichen. Anhand dieser Linien wird das Foto derart perspektivisch verzerrt, dass tatsächlich senkrechte Kanten auch im Bild senkrecht dargestellt werden. Die zugrunde liegende Abbildung (Projektionsmatrix) bewirkt eine Verschiebung des Bildhauptpunktes. Sollte die Automatik aufgrund falsch gewählter Linien unnatürlich wirkende Ergebnisse zeigen, ist es möglich, die Auswahl der Bezugslinien zu beeinflussen. Das Ergebnis der automatischen Berechnung lässt sich nachträglich verändern. Außerdem können manuell einfache tonnen- oder kissenförmige Verzeichnungen des Objektivs ausgeglichen werden, jedoch keine komplexen wellenförmigen Verzeichnungen.\n\nShiftN erlaubt die Voreinstellung einiger Parameter, wie den Grad der Entzerrung und der Nachschärfung. Für die Bearbeitung bereits zugeschnittener Bilder ist ShiftN nur eingeschränkt verwendbar, es arbeitet am besten mit der vollständigen Datei. Beim Speichern des Ergebnisses kann gewählt werden, ob die gesamte Datei, also inklusive der durch das Verzerren entstandenen leeren Bereiche abgelegt werden soll, oder ob das Programm automatisch die größtmögliche Fläche ohne Ränder ermittelt. Auch kann manuell ein Rahmen aufgezogen werden, um nur einen Ausschnitt zu speichern.\n\nShiftN liest die Grafikformate JPG, BMP und TIFF (8 und 16 Bit Farbtiefe) und kann auch in diesen Formaten speichern. Ein Nachteil des Programms ist, dass es Exif-Unterstützung nur für das JPG-Format bietet, jedoch bei TIFF-Bildern diese Informationen nicht auf das Ergebnisbild überträgt. Neben der interaktiven Verarbeitung ist auch ein Batchbetrieb und der Aufruf mit Kommandozeilenparametern möglich, wodurch sich ShiftN relativ leicht auch in andere Software einbinden lässt.\n\nShiftN ist auch international verbreitet, die Benutzerführung lässt sich auf die Sprachen Deutsch, Englisch, Französisch oder Spanisch einstellen.\n\nIm deutschen Markt lag die Software seit 2006 knapp 30 Fach- und Publikumszeitschriften aus dem Bereich digitale Fotografie und PC bei, meist gekoppelt mit einer kurzen Besprechung. Der Schweizer Tages-Anzeiger zählte ShiftN 2007 neben Adobe, Picasa, Photoshop Starter Edition, Helicon Filter und PhotoFiltre zu den besten Gratis-Tools für Fotografen.\n\n"}
{"id": "3617550", "url": "https://de.wikipedia.org/wiki?curid=3617550", "title": "MobileMe", "text": "MobileMe\n\nMobileMe war ein kostenpflichtiger Online-Dienst von Apple, der Daten auf mobilen und stationären Geräten synchron hielt. Dabei konnte es sich um E-Mails, Termine, Adressen oder Fotos handeln. Zu den unterstützten Geräten gehörten das iPhone, der iPod touch, das iPad sowie Computer mit macOS, Microsoft Windows oder einem aktuellen Browser. MobileMe wurde am 1. Juli 2012 eingestellt und durch iCloud ersetzt.\n\nMobileMe wurde am 9. Juni 2008 auf der Konferenz WWDC 2008 als Nachfolgedienst von .Mac angekündigt. Mit dem neuen Namen verbindet sich ein Wandel des Schwerpunktes auf sofortige Synchronisierung von Daten auf verschiedenen mobilen und stationären Geräten mittels der Push-Technik und des Zugriffs auf eigene Daten, unabhängig vom eigenen Standort.\n\nWegen der starken Last konnte Apple nach Start des Angebotes den Push-Dienst jedoch nur für mobile Geräte bereitstellen. Als Entschädigung verlängerte Apple die Laufzeit des Vertrages für bestehende Kunden kostenlos um 30 Tage und später um weitere 60 Tage. Außerdem berichtete Apple, dass circa 1 % der MobileMe-Benutzer den integrierten E-Mail-Dienst mindestens eine Woche lang nicht nutzen konnten.\n\nAm 6. Juni 2011 gab Apple auf der WWDC bekannt, dass der seit dem 12. Oktober 2011 angebotene Dienst iCloud MobileMe ab dem 1. Juli 2012 komplett ersetzt.\n\nMobileMe ersetzte ab dem 10. Juli 2008 den .Mac-Service. Die Konvertierung für die Nutzer geschah automatisch.\n\nWährend .Mac hauptsächlich auf Desktop- und Notebooknutzung ausgerichtet war, sollte MobileMe Internetdienste sowohl für Mac-OS- als auch für iPhone- und Windows-Nutzer bereitstellen, die meisten von .Mac bekannten Leistungen behalten und zusätzliche Dienste (z. B. Push-Dienste) bereitstellen.\nEinige Funktionen fielen ersatzlos weg oder waren zumindest nicht mehr in der Form integriert, wie sie es bei .Mac gewesen waren:\n\nMobileMe umfasst sogenannte Push-Dienste für E-Mail, Kalender und Adressen. Zum Start von MobileMe stehen die Push-Dienste jedoch nur für den iPod touch und das iPhone sowie die Browser-basierten Web-Anwendungen zur Verfügung. Auf Computern findet eine Synchronisation in einem Intervall von minimal 60 Sekunden statt.\n\nMobileMe soll Daten (E-Mail-Konten (inkl. aller Regeln und Signaturen), Adressen, Kalender, Bookmarks, Dashboard-Widgets, Dockobjekte, Schlüsselbunde, Notizen, Einstellungen) zwischen mehreren Geräten (Mac, iPhone, iPad, iPod touch und Windows-PC) synchron halten. Dies geschieht über eine Anbindung an das Internet. Unterstützt werden die in Mac OS X integrierten Programme Mail, Apple Safari, Adressbuch und iCal, aber auch Microsoft Outlook, Internet Explorer und Safari unter Windows.\n\nZusätzlich erhalten Kunden durch die \"MobileMe-Webapps\" auf der Webseite des Dienstes die Möglichkeit, von jedem beliebigen Rechner aus über einen Web-Browser auf die eigenen Daten zuzugreifen und diese teilweise auch zu editieren.\nDer Benutzer erhält Zugriff auf die folgenden Webanwendungen:\n\nDer Dienst beinhaltet ein E-Mail-Konto und eine E-Mail-Adresse (z. B. vorname.nachname@\"me.com\"), die über IMAP oder POP3 abgefragt werden kann.\n\nÜber die Galeriefunktion können Bilder in das Internet gestellt und selektiv anderen Nutzern zur Verfügung gestellt werden. Es gibt auch die Möglichkeit, dass andere Nutzer einer Galerie weitere Bilder hinzuzufügen. Dies kann via Browser oder E-Mail geschehen. Die hinzugefügten Bilder werden automatisch mit dem lokalen iPhoto synchronisiert.\n\niDisk stellt auf Servern 20 oder mehr Gigabyte Speicherplatz zur Verfügung, auf den über das WebDAV-Protokoll oder einen Browser zugegriffen werden kann.\n\nFür Mac OS X wird \"Zugang zu meinem Mac\" zur Verfügung gestellt. Dabei handelt es sich um einen Dienst, welcher den Datenzugriff auf den heimischen Mac von beliebigen Internetzugängen aus erlaubt.\n\nAb 17. Juni 2009 ergänzt Apple eine neue Funktion, mit der sich ein verlegtes iPhone oder iPad wiederfinden lassen. Per Mausklick lässt sich das Gerät orten, die Position auf einer Karte anzeigen und einen Klingelton, auch wenn es auf stumm geschaltet ist, mit einer Nachricht auslösen. Seit November 2010 ist, mit Einführung von iOS 4.2.1, dieser Dienst zumindest für Besitzer eines iPhone 4, iPod touch (4G) oder iPad kostenlos.\n\nSollte das eigene iPhone, iPod touch oder iPad gestohlen worden sein, kann man die kompletten Daten löschen lassen. Damit wird das Gerät unbrauchbar und persönliche Daten können nicht in fremde Hände gelangen. Seit November 2010 ist, mit Einführung von iOS 4.2.1, dieser Dienst zumindest für Besitzer eines iPhone 4, iPod touch(4G) oder iPad kostenlos.\n\nMit der Einführung von OS 3.1 für das iPhone gibt es eine neue Funktion, mit der sich das iPhone oder iPad mit einem vierstelligen Zugangscode von der Ferne aus sperren lässt. Diese Option ist nützlich, wenn man nicht gleich eine komplette Löschung durchführen möchte und trotzdem den Finder oder Dieb des iPhones aussperren möchte. Seit der Einführung von iOS 4.2.1 im November 2010 ist dieser Dienst für Besitzer eines iPhone 4 oder iPads kostenlos verfügbar.\n\niCloud wurde am 6. Juni 2011 im Rahmen der Worldwide Developers Conference (WWDC) vorgestellt und am 12. Oktober 2011 offiziell gestartet. Der neue Dienst ersetzt seit dem 1. Juli 2012 das Angebot von MobileMe vollständig. Apple hat Nutzer bis zu diesem Datum immer wieder gebeten, ihre Daten auf iCloud umzuziehen, da bis auf iWebPublishing, Gallery und iDisk alle Funktionen des alten Angebots zur Verfügung stehen. Aufgrund der anhaltenden Kritik an iCloud und der Tatsache, dass noch längst nicht alle Nutzer ihr Konto gewechselt haben, hat Apple im Oktober 2012 die Frist für den Umzug der Daten um zwölf Monate bis September 2013 verlängert. Außerdem erhalten ehemalige MobileMe-Kunden zusätzlichen Speicherplatz ohne Gebühren.\n\nBei den Versionen 7.7 bis 8.2 der iTunes-Software für Windows wurde der MobileMe-Dienst ungefragt als Systemdienst installiert. Eine Möglichkeit der Abwahl bei der Installation gab es nicht.\n\nDie Einführung von MobileMe war mit großen Problemen verbunden. So war der Dienst teilweise nicht oder nur eingeschränkt verfügbar und wies besonders als Webanwendung ungewöhnlich hohe Antwortzeiten auf.\n\nApple-Gründer Steve Jobs äußerte 2011 anlässlich der Einführung von iCloud, dass MobileMe nicht Apples „Sternstunde“ gewesen sei.\n"}
{"id": "3619526", "url": "https://de.wikipedia.org/wiki?curid=3619526", "title": "Tomorrow (deutsche Zeitschrift)", "text": "Tomorrow (deutsche Zeitschrift)\n\nTomorrow war eine deutsche Zeitschrift für Internet- und Technik-Themen, die bis 2009 monatlich im Verlag Hubert Burda Media erschien. Sie wurde 1998 von der Hamburger Verlagsgruppe Milchstrasse gegründet, entwickelte sich um die Jahrtausendwende zu einem der populärsten Titel in ihrem Bereich und war Namensgeber für die Tomorrow Focus AG. Tomorrow beschäftigte sich vor allem mit populärer Technologie, enthielt aber auch Lifestyle- und Science-Fiction-Themen.\n\nIn den 1990er Jahren konzipierte die Verlagsgruppe Milchstrasse unter der Leitung von Dirk Manthey mehrere neue Zeitschriften. Dazu gehörte neben Amica, Fit for Fun und TV Spielfilm auch Tomorrow. Das von der NZZ als „Computerillustrierte“ beschriebene Blatt richtete sich nicht an Experten, sondern technisch interessierte Laien. Die erste Ausgabe der Zeitschrift wurde am 10. September 1998 in einer Auflage von 500.000 Exemplaren veröffentlicht, davon verkauften sich rund 40 % innerhalb der ersten zehn Tage. Beobachter stuften Tomorrow vor allem als Angriff auf die konkurrierenden Titel Konr@d und Online Today von Gruner + Jahr ein. Eine umfangreiche Werbekampagne begleitete die Einführung der Zeitschrift, außerdem wurde eine Webpräsenz mit einem Nachrichtenticker eingerichtet. Der Nachrichtensender n-tv strahlte die Sendung „Net News“ fortan unter der Marke „Tomorrow“ aus. Für größere Aufmerksamkeit sorgte außerdem eine 1999 gemeinsam mit Mobilcom gestartete Internet-Flatrate.\n\nErster Chefredakteur von Tomorrow war Willy Loderhose. Gemessen an der Anzeigenstatistik 1999 entwickelte sich Tomorrow zu einer der erfolgreichsten neuen Zeitschriften im deutschsprachigen Markt. Aufgrund dessen stellte die Verlagsgruppe Milchstrasse den Erscheinungsrhythmus von vier auf zwei Wochen um, ab September 2000 wurde das Blatt für drei Deutsche Mark verkauft. Außerdem gründete man eine B2B-Variante namens \"Tomorrow Business\". Die Internet-Portale von Tomorrow und einigen anderen Zeitschriften aus der Verlagsgruppe Milchstrasse wurden 1999 in die Tomorrow Internet AG eingebracht, das noch im selben Jahr an der Börse debütierte. 2000 startete mit \"Internet Schnell & Einfach\" eine Variante von Tomorrow für Einsteiger, von der vierteljährlich rund 200.000 Exemplare verkauft wurden. Ab dem Jahr 2000 trat die Verlagsgruppe Milchstrasse als Hauptsponsor des Hamburger SV auf: Beginnend mit der Bundesliga-Saison 2001/2002 war das Logo der Zeitschrift auf den Trikots der Fußballer zu sehen.\n\nWährend Tomorrow im ersten Quartal 1999 noch rund 350.000 verkaufte Exemplare aufwies, musste die Zeitschrift in den Jahren 2000 und 2001 eine rapide sinkende Auflage hinnehmen. Im Mai 2001 fing Georg Altrogge als neuer Chefredakteur bei Tomorrow an, ab Mitte 2001 erschien das Blatt wieder monatlich. Aufgrund sinkender Anzeigenerlöse musste die Hälfte der Redaktion entlassen werden. Noch vor der Fusion der Focus Digital AG mit der Tomorrow Internet AG kaufte Hubert Burda Media 49 % der Zeitschrift Tomorrow. Die restlichen Anteile an der Tomorrow Verlag GmbH & Co. KG blieben bei Dirk Manthey. 2002 änderte Tomorrow seine Strategie: Statt Internet-Themen wurde die Zeitschrift als „multimediales Männermagazin“ positioniert. 2003 erfolgte ein weiterer Relaunch, der Testberichte wieder stärker in den Vordergrund rückte. Trotz der Veränderungen verlor Tomorrow von 2001 bis 2002 weitere 25 % der Auflage. 2005 strebte Tomorrow mit einem neuen Design und veränderten Inhalten die Rückkehr in die Gewinnzone an.\n\nZum Jahreswechsel 2004/2005 übernahm Hubert Burda Media die Verlagsgruppe Milchstrasse komplett, sodass sich auch Tomorrow fortan vollständig im Besitz des Medienhauses mit Sitz in Offenburg und München befand. Das Blatt wurde organisatorisch in die Vogel Burda Group integriert, zu der auch das Computermagazin Chip gehörte. Das bedeutete praktisch einen Wechsel der Zentrale von Hamburg nach München. Im Frühjahr 2005 übernahm Jürgen Bruckmeier die Chefredaktion von Georg Altrogge. Unter seiner Führung wurde Tomorrow abermals neu ausgerichtet, außerdem wechselte im Jahr 2008 die Redaktion von München nach Berlin. Trotz aller Bemühungen attestierten Beobachter dem Blatt bereits 2005 einen beispiellosen Niedergang. 2009 verkündete Hubert Burda Media schließlich die Einstellung von Tomorrow. Zuvor betrug die Auflage der Zeitschrift 56.000 verkaufte Exemplare, davon 25.000 Bordexemplare. Die letzte Ausgabe von Tomorrow erschien im März 2009.\n"}
{"id": "3635735", "url": "https://de.wikipedia.org/wiki?curid=3635735", "title": "NetworkManager", "text": "NetworkManager\n\nDer NetworkManager ist eine Anwendungssoftware zur Verwaltung von Netzwerkverbindungen für Linux und andere unixähnliche Systeme. Er soll den Umgang mit Rechnernetzen, insbesondere mit drahtlosen Netzwerken erleichtern, unterstützt mittlerweile aber auch kabelgebundene Netzwerkzugänge.\n\nDer NetworkManager stellt über unterschiedliche Zugangstechnologien nach Möglichkeit automatisch eine Netzwerkverbindung her. Er unterstützt\n\nIm Rahmen eines Förderprogrammes wurde im Sommer 2008 auch ein Assistent für das Verbinden über Mobilfunk-Geräte entwickelt, mit dem sich Verbindungen über Technologien wie GPRS, EDGE, UMTS, CDMA einrichten lassen.\n\nSoweit nicht anders verordnet, stellt der NetworkManager möglichst automatisch eine Netzwerkverbindung her. Er verfährt dazu nach einem opportunistischen Ansatz, indem er versucht, die beste verfügbare Verbindung zu benutzen, sobald Unterbrechungen auftreten oder der Benutzer sich zwischen verschiedenen drahtlosen Netzwerken umherbewegt. Dabei werden kabelgebundene Verbindungen (Ethernet) gegenüber schon einmal benutzten drahtlosen Netzen bevorzugt, welche wiederum Netzen vorgezogen werden, zu denen der Benutzer noch nie verbunden war. Wenn nötig, wird der Benutzer nach WEP- oder WPA-Schlüsseln gefragt.\n\nMit der Version 1.2 wird die MAC-Adresse bei der Suche nach drahtlosen Netzwerken verschleiert, um das Tracking von Benutzern zu erschweren. Bei nicht vertrauenswürdigen Netzwerken bleibt die eigentliche Adresse auch nach der Suche verschleiert, bei vertrauenswürdigen Netzwerken ist dies nicht der Fall.\n\nNach dieser Version wurde eine Nutzerbefragung durchgeführt.\n\nDie Version 1.4 führte allgemeine Unterstützung für MAC-Spoofing ein. Außerdem wurden eine API für Konfigurations-Snapshots sowie die Eigenschaft \"dns-priority\" eingeführt. Canonical steuerte Unterstützung für oFono als Modemmanager bei.\n\nDer NetworkManager besteht aus zwei Komponenten: einem Daemon, der Netzwerkverbindungen verwaltet und Informationen über Veränderungen bereitstellt. Ein Anwender kann wahlweise in der grafischen Oberfläche über ein Desktop-Applet oder über die Kommandozeile in die Konfiguration und das aktuelle Verhalten eingreifen.\nAls eine der ersten größeren Komponenten des Linux-Desktops wird beim NetworkManager ausgiebiger Gebrauch von D-Bus und der Hardwareabstraktionsschicht (HAL) des freedesktop.org-Projektes gemacht. Das Applet benutzt das \"System Tray Protocol\" von freedesktop.org und funktioniert mit allen Desktop-Umgebungen, die dieses befolgen. Darunter Gnome, K Desktop Environment / Plasma und Xfce. Sowohl der NetworkManager selbst, als auch seine Benutzerschnittstellen sollen weitgehend portabel sein. Da die Komponenten über D-Bus kommunizieren, können andere Anwendungen mit Informationen über den Online-Status versorgt werden oder auch das ursprüngliche Applet vollständig ersetzt werden, wie bei dem KDE-Frontend \"KNetworkManager\", das Novell für openSUSE entwickelt hat.\n\nDas Projekt wurde 2004 von Red Hat ins Leben gerufen. Jenseits von Linux ist NetworkManager in Ermangelung einer passenden Hardwareabstraktionsschicht auf vielen Betriebssystemen nicht verfügbar.\n\n\n"}
{"id": "3636571", "url": "https://de.wikipedia.org/wiki?curid=3636571", "title": "Schärfe (Fotografie)", "text": "Schärfe (Fotografie)\n\nDie Unterscheidbarkeit von Details in einem Bild wird Schärfe genannt. Schärfe ist bei der technischen Umsetzung der Fotografie ein wichtigstes Ziel.\n\n\nIn der Alltagsfotografie spielt Schärfe nur eine zweitrangige Rolle, dominierend bei der Qualitätsbeurteilung eines Fotos ist immer der Schärfeeindruck.\n\nNur durch Schärfe kann Bildinformation dargestellt werden. Können viele Details unterschieden werden, verfügt ein Bild über hohe Schärfe. Unterschiede zwischen Details können bei der Helligkeit, dem Farbton und der Farbintensität wahrgenommen werden.\nNur durch Schärfe kann ein Foto Informationen transportieren. Die Qualität dieses Informationstransportes wird in der Fotografie auf drei Arten bewertet:\n\nEs wird die Menge der Linien gezählt, die noch unterscheidbar sind. Als Ergebnis entsteht ein Zahlenwert. Mit dieser Methode werden oft Objektive, Drucker, Kameras und andere technische Geräte bewertet.\nIn der Kunst ist Schärfe ein besonderes Kontrastmittel. Erst durch den Unterschied zwischen scharf und unscharf werden die zusätzlichen Bildinformationen übertragen.\nÄsthetisch betrachtet kann in einem Bild weniger Schärfe (= weniger Details) zu sehen sein, trotzdem kann dieses Bild mehr Informationen transportieren.\nDiese Schärfebewertung dominiert unsere tägliche Sehgewohnheit. Abhängig von den konkreten Umständen (Medienauflösung, Bildgröße, Betrachtungsabstand und -zeit, Bildaussage und eigene Erwartung) entsteht ein individuelles Schärfemaß.\n\nDiese Art der Schärfebeurteilung wird von sehr vielen subjektiven Dingen beeinflusst – daher auch der Name \"subjektiver Schärfeeindruck\".\n\nIn der Optik, Fotografie und Kinematografie wird damit ein spezielles Kriterium bezeichnet, das sich an Kanten beobachten lässt. Je abrupter dabei die Übergänge von Dunkel zu Hell sind, desto schärfer ist die Abbildung. Das Ausmaß der Kantendarstellung wird in der Fotografie als Akutanz bezeichnet.\n\nDie Kantenschärfe ist dabei eine Angelegenheit der (Licht-) Öffnung: Ein unendlich kleines Loch in der Camera Obscura lässt ein unendlich scharfes (ohne Berücksichtigung der Beugungseffekte), aber auch unendlich dunkles Bild entstehen. Wenn man das Loch endlich groß macht, wird das Bild heller, aber auch weniger scharf.\n\nKantenschärfe hat nichts mit Auflösung zu tun.\n\nNeben geeigneten technischen Geräten sind bei der Messung von Schärfe drei Voraussetzungen wichtig:\n\nDas Testbild wird reproduziert - und als Referenz mit dem fertigen Bild verglichen. Durch den Unterschied zwischen beiden lässt sich Schärfe messen. Nur eine geeignete Referenz bietet optimale Voraussetzungen zur Schärfemessung.\nSchärfe wird von sehr vielen Faktoren beeinflusst. Da Schärfemessung ein vergleichbares Ergebnis liefern soll, müssen die Randbedingungen festgelegt werden. Als Ergebnis entsteht ein Zahlenwert. Diese idealisierten Bedingungen haben den Nachteil, dass sie in der Alltagsfotografie ohne großen Nutzen sind, denn hier ändert sich ständig irgendetwas – selten herrschen ideale Bedingungen. Die Änderung einer wesentlichen Bedingung, beispielsweise der Temperatur, macht den ermittelten Zahlenwert oft unbrauchbar.\n\nVon der gewählten Messmethode (Messgenauigkeit) und deren Auswertung hängt das Ergebnis ab.\n\nSchärfemessung benötigt immer einen Vergleich. Die Schärfe der oberen Bilder kann auf verschiedene Arten verglichen werden:\n\nAuch bei \"Schärfemuster 5\" würde von der Messgenauigkeit das Ergebnis abhängen. Da die Artefakte \"nur\" bei der mittleren Bildauflösung zu erkennen sind, kann der Messwert verfälscht werden.\n\nIm Alltag ist der Mensch immer von einer Mischung zwischen tatsächlicher (physikalisch vorhandener) Schärfe und dem Anschein von Schärfe, verursacht durch die subjektive Wahrnehmung des Menschen, umgeben. Bei letzterem spricht man vom Schärfeeindruck.\nMithilfe des Schärfeeindrucks kann die Schärfe von Fotografien verändert werden. Die Techniken, den Schärfeeindruck zu beeinflussen, werden unter dem Begriff Bildoptimierung zusammengefasst.\n\nDie Technik, den subjektiven Schärfeeindruck eines Bildes verändern zu können, ist bereits mehr als hundert Jahre alt. Mittels feiner Pinsel und spezieller Farbe wurden die für den Schärfeeindruck wichtigen Bilddetails (meist Augen und Konturen) zart nachgezeichnet. Diese Methode war so erfolgreich, dass sie bis Ende des 20. Jahrhunderts von den meisten Profifotografen für derartige Zwecke benutzt wurde. Auch heute lassen sich mit der entsprechenden Software Bilder so retuschieren, dass sich ihr Schärfeeindruck verbessert.\n\nDie zweite, ebenfalls sehr alte Methode ist die der Kontrastveränderung. Mittels verschiedener Entwicklerchemie und Papiersorten sowie gezieltem Nachbelichten konnte der Kontrast global im gesamten Bild manipuliert werden.\nIn der zweiten Hälfte des 20. Jahrhunderts erhielt die Kontrastveränderung einen entscheidenden Qualitätssprung: die Unscharfmaskierung (dabei wird der Kontrast nur lokal an den Kanten innerhalb eines Bildes verstärkt). Beide Techniken konnten sich erfolgreich im digitalen Fotozeitalter behaupten.\n\nMit Beginn der 1990er Jahre wurde (sehr unspektakulär) ein neues Zeitalter in der Fotografie eingeläutet. Zu diesem Zeitpunkt wurden 95 % aller Fotos in Großlaboren gefertigt. Die Großlabore führten damals die Technik der automatischen Bildoptimierung ein. In Sekundenbruchteilen wurde jedes (damals analoge) Foto analysiert und manipuliert. Diese Technik konnte in Zehntelsekunden jedes Bild individuell abwedeln, nachbelichten, unscharf maskieren usw.\nDie Software, die diesem Verfahren zugrunde lag, benötigte eine jahrelange Feinjustierung (sie musste „trainiert“ werden). Daher entstand im Laufe der Zeit ein allmählicher Übergang zu schließlich völlig anderen Sehgewohnheiten.\nAm besten kann man diese Veränderung an einem Vergleich zwischen Fotos der 1980er und 1990er Jahre beobachten. Obwohl sich die Technik des (von den meisten Fotografen benutzten) Kleinbildfilms in dieser Zeitspanne nicht gravierend geändert hat, erkennt man an den Fotos dieser Zeit deutlich den Unterschied.\n\nIn den Jahren des Jahrtausendwechsels begann die massenhafte Verbreitung der Digitalfotografie. Um die damals gravierenden Schärfemängel der digitalen Technik zu kaschieren, wurde die gesamte Bildfertigungsstrecke den Nutzerprofilen angepasst.\nEin typisches Beispiel dafür ist das Profil des \"„Knipsers“\": Urlaubs- und Familienmotive sollen in der Größe 10 cm × 15 cm für das Fotoalbum ausgedruckt werden.\nDie Drucker wurden so eingestellt, dass sie den Schwarzanteil wesentlich mehr erhöhten, als es notwendig gewesen wäre. Das hatte zur Folge, dass die Bildmotive sehr kräftig und kontrastreich aussahen (Schärfeeindruck). Dadurch wurde gleichzeitig die Schwäche der Drucker kaschiert, zarte Farben nur sehr schlecht drucken zu können. Durch die Beschränkung auf 10 cm × 15 cm wurde der physikalische Schärfemangel digitaler Bilder kaum sichtbar.\n\nDa sich unsere Sehgewohnheit, wie oben erwähnt, inzwischen an die allgegenwärtige, automatisierte Bildoptimierung angepasst hat, spielt die eigentliche Schärfe eines Bildes nur noch eine zweitrangige Rolle – wichtig ist hauptsächlich der Schärfeeindruck.\n\n1. Den wichtigsten Einfluss auf den Schärfeeindruck hat die Bildoptimierung.\n\n2. Den zweitwichtigsten Einfluss hat die physikalische Schärfe.\n\n3. Den drittwichtigsten Einfluss hat die Auflösung.\n\nZur Beurteilung des Schärfeeindrucks sind Tests notwendig. Werden diese unter Berücksichtigung der typischen Problembereiche durchgeführt, werden die Mängel sichtbar, die sich gravierend auf die Minderung des Schärfeeindrucks auswirken.\n\nEs gibt kein bildgebendes Verfahren ohne Mängel.\n\nAls \"Zeichnung\" bezeichnet man die Erkennbarkeit von Einzelheiten und Strukturen in lokalen Bereichen eines Bildes. Ist keine Zeichnung mehr vorhanden, enthält das Bild in diesen Bereichen statt der Strukturen nur noch kontrastarme Flächen. \"Zeichnung\" ist ein Begriff zur subjektiven (Schärfe-) Bewertung eines Fotos.\n\n100%ige Kantenschärfe wäre der Idealzustand eines scharfen Fotos - dieses ist aber unmöglich.\n\nWas diesem Ideal (der 100%igen Schärfe) am nächsten kommt, ist die Vektorgrafik. Allerdings können damit nur Linien und Kanten dargestellt werden.\n\nJedes Foto enthält Unschärfe.\n\nHelligkeitsunterschiede sind die stärksten Unterschiede, die das menschliche Auge in einem Foto wahrnehmen kann. Diese Helligkeitsunterschiede können sowohl global (das ganze Bild betreffend) als auch lokal (zwischen einzelnen Bildelementen) wahrgenommen werden. Diese Unterschiede werden Kontrast genannt. Mit einem hohen Kontrast lässt sich der subjektive Schärfeeindruck erhöhen – meist zu Lasten der physikalischen Schärfe.\n\nDer größte Kontrastunterschied in unserem Alltag existiert zwischen tiefster Nacht und gleißendem Sonnenschein. Man spricht hierbei von einem großen Kontrastumfang. Das menschliche Auge passt sich diesem Unterschied durch Öffnen und Schließen der Pupille an. Fotoapparate benutzen durch das Öffnen und Schließen der Blende eine vergleichbare Methode.\n\nEin hoher Kontrastumfang liefert eine hohe Schärfe – allerdings gibt es kein Bildsystem, das den Kontrastumfang der Natur vollständig in einem Bild fixieren kann. Daher muss der natürliche Kontrastumfang eines Motivs vor der Bildspeicherung reduziert werden.\nDer Kontrastumfang eines Fotos wird mit dem Bereich zwischen hellster und dunkelster Stelle definiert. In der technischen Darstellung werden dazu Schwarz und Weiß benutzt.\n\nJe nach Anzahl der Differenzierungsstufen ist ein Foto in der Lage, Bildinformationen zu transportieren (= scharf zu sein). In der Digitalfotografie werden meist 8 bit (= 256 Abstufungen pro Farbkanal) benutzt. Damit ist im Alltag ein ausreichendes Maß an Schärfe gewährleistet.\n\n> wörtliche Übersetzung: „Beschaffenheit eines Lichtbildes“\n\nUnter Fotoqualität versteht man die Gesamtheit aller Einflussfaktoren, die die Qualität eines Fotos beeinflussen. Die Art der Aufnahme (analog oder digital) hat nur bedingt Einfluss auf die Fotoqualität. Man kann mit alten analogen Kameras, die über keinerlei Elektronik verfügen, exzellente Bilder machen und mit einer teuren Digitalkamera unbrauchbare Schnappschüsse.\n\nNeben der Schärfe, die für Laien oft einziges Qualitätsmerkmal ist, spielen auch viele andere Kriterien eine Rolle:\n\nJede Bedingung, die die Fotoqualität beeinflusst, beeinflusst auch die Schärfe. Daher kann die differenzierte Beurteilung von Schärfe exemplarisch für die Beurteilung der Fotoqualität betrachtet werden.\n\n> wörtliche Übersetzung: „zusammendrücken“\n\nHierbei wird zwischen \"verlustfreier\" und \"verlustbehafteter\" Komprimierung unterschieden.\n\n\"Verlustbehaftete Komprimierung ist immer mit einem Schärfeverlust verbunden.\"\n> sinngemäß für: den Qualitätseindruck eines Fotos verbessern\n\n\nDie Bildoptimierung versucht, jede dieser Veränderungen zu steuern. Die wichtigsten Methoden sind:\nAlle genannten Methoden können analog und digital angewendet werden.\n\n\n"}
{"id": "3640613", "url": "https://de.wikipedia.org/wiki?curid=3640613", "title": "Binfmt misc", "text": "Binfmt misc\n\nbinfmt_misc ist eine Fähigkeit des Linux-Kernels, beliebige ausführbare Dateien zu erkennen und einem bestimmten Programm im User-Mode zu übergeben, wie beispielsweise einem Interpreter oder einem Programmstarter, der das Programm in den Arbeitsspeicher lädt.\n\nEs handelt sich um ein optionales Kernel-Modul, durch welches im Prinzip jede Datei als Programm ausgeführt werden kann. Dadurch grenzt es sich gegenüber anderen Techniken, zum Beispiel dem Shebang-Mechanismus, ab.\n\nDie ausführbaren Formate werden in einer zentralen datenbankartigen Form in einem virtuellen Dateisystem, der sogenannten Registrierung, gespeichert, welches ähnlich wie devfs, procfs oder sysfs arbeitet. Standardmäßig wird dieses Dateisystem unter /proc/sys/fs/binfmt_misc eingebunden.\n\nEine alternative Möglichkeit, die ein ähnliches Ziel wie \"binfmt_misc\" verfolgt, ist der Shebang-Mechanismus. Bei ihm wird in der ersten Zeile der auszuführenden Datei der Interpreter mit der Zeichenkombination codice_1 festgelegt. Diese Technik ist auf Unix-Derivaten vor allem für Skripte sehr weit verbreitet, eignet sich allerdings auch nur für Textdateien.\n\nDie \"binfmt_misc\"-Technik hingegen benötigt keine spezielle Kennzeichnung in der Datei, es muss sich daher auch nicht (zwingend) um eine reine Textdatei handeln. Dadurch, dass die Assoziation zwischen Dateien und Interpretern in einer systemweit zentralen Datenbank festgehalten wird, könnte beispielsweise das Problem des Speicherortes beim Shebang-Mechanismus gelöst werden.\n\nDer Speicherort der Assoziation zwischen Dateitypen und Interpretern ist die Datei register, die sich direkt im Stammverzeichnis des virtuellen \"binfmt_misc\"-Dateisystems befinden muss. Da das \"binfmt_misc\"-Dateisystem meist unter /proc/sys/fs/binfmt_misc gemountet wird, ist der Name der Datei auf den meisten Linux-Systemen /proc/sys/fs/binfmt_misc/register. Es handelt sich dabei um eine Textdatei, in der jede Zeile einen Eintrag darstellt, der festlegt, wie ausführbare Dateien behandelt werden sollen. Jede Zeile hat dabei die folgende Form:\n\nDabei stehen die Felder\n\n\nZu jedem registrierten Dateityp erstellt \"binfmt_misc\" eine Datei in dem virtuellen Dateisystem. Diese Datei kann später gelesen werden, um Informationen über das Dateiformat zu erhalten.\n\nCLI- und Java-Anwendungen können dank \"binfmt_misc\" direkt dem richtigen Interpreter übergeben werden und so direkt über die Shell oder durch andere Programme gestartet werden.\n\nEs ist auch üblich, Portable-Executable-Dateien (Dateien mit den Dateiendungen .exe und .dll, die für MS-DOS oder Microsoft Windows kompiliert wurden) auf diese Art und Weise mit Wine auszuführen. In der Registrierungsdatei würde man dann folgende Zeile notieren:\n\nDurch den \"magic string\" MZ im \"type_code\" wird der Dateityp einer \"Portable Executable\" dabei erkannt und die Datei mit der Windows-Laufzeitumgebung ausgeführt.\n\n"}
{"id": "3643671", "url": "https://de.wikipedia.org/wiki?curid=3643671", "title": "Bernard (Fernsehserie)", "text": "Bernard (Fernsehserie)\n\nBernard (im asiatischen Raum auch \"Backkom\", abgeleitet aus dem Hangeul: , \"Ppaekkom\") ist die Hauptfigur einer computergenerierten Animationsfilm-Serie, die seit 2004 produziert wird.\n\n\"Bernard\" ist ein neugieriger, ungeduldiger und tollpatschiger Eisbär, der in jedes Fettnäpfchen tritt und nie ohne Blessuren davonkommt.\n\nDie Kurzfilme sind mit CGI-Animationssoftware gerendert und nur mit Musik, Umgebungsgeräuschen und Lauten (z. B. Knurren, Seufzen) unterlegt.\nWährend in einigen Episoden verschiedene andere Lebewesen wie eine Kuh oder Außerirdische \"Bernard\" das Leben schwer machen, tauchen einige Nebencharaktere wiederholt auf, etwa die Eidechse \"Zack\", der Chihuahua \"Goliat\" oder die Pinguine \"Lloyd\" und \"Eva\".\n\nIn Deutschland werden die Kurzfilme mehrmals wöchentlich im Fernsehen einzeln oder in Blöcken von 2 bis 3 Episoden im Kinderkanal ausgestrahlt.\n\nDie Serie erhielt mehrere Auszeichnungen und Nominierungen, so erhielt, unter anderen, die Episode \"Contact2\" auf dem \"Indi-Anifest 2006\" in Seoul den Spezialpreis der Jury. Im Jahr 2004 errang die Serie den Ersten Platz bei der \"MIPCOM JR. Licensing Challenge 2004\" als Beste Fernsehserie.\n\n"}
{"id": "3644462", "url": "https://de.wikipedia.org/wiki?curid=3644462", "title": "Small-Scale Experimental Machine", "text": "Small-Scale Experimental Machine\n\nDie Small-Scale Experimental Machine war der erste auf der Von-Neumann-Architektur basierende Computer, aufgebaut mit Röhren.\n\nEntwickelt wurde er ab 1945 an der Victoria University of Manchester von Frederic Calland Williams, Tom Kilburn und Geoff Tootill um Phänomene der Williamsröhre zu untersuchen. Diese wurde als Speicher nutzbar, nachdem Williams 1946 ein Verfahren entwickelte, den Speicher innerhalb von Millisekunden zu regenerieren. Im Juni 1948 wurde damit der Nachweis erbracht, dass sowohl Programm- als auch Arbeitsspeicher im Computer-Speicherwerk gehalten werden können.\n\nDaraus entwickelte sich der Prototyp des späteren Manchester Mark I.\n\n"}
{"id": "3650116", "url": "https://de.wikipedia.org/wiki?curid=3650116", "title": "TurboDemo", "text": "TurboDemo\n\nTurboDemo ist eine Software des Schweizer Softwarehauses balesio zur Erstellung von Software-Demos, Online-Präsentationen und animierten Bedienungsanleitungen auf Basis von einer Serie von aufgenommenen Screenshots. Mit dem Programm lassen sich somit einzelne Arbeitsschritte am Bildschirm aufnehmen, die anschließend als Folien bearbeitet werden. Neben deskriptiven Bearbeitungselementen und Klang bietet das Programm auch die Möglichkeit, interaktive Elemente hinzuzufügen. Abschließend kann das Projekt in verschiedenen Formaten, darunter Flash, Java, selbstlaufende Exe-Datei, GIF, AVI, Windows Media Player-Format und Word bzw. PDF exportiert werden.\n\nDas Programm wurde erstmals im September 2001 auf der Messe \"Entwickler-Konferenz\" in Frankfurt präsentiert und wurde kontinuierlich weiterentwickelt.\n\n\n\n\n\n\n\nDas Programm eignet sich um animierte Präsentationen von Bildschirminhalten zu erstellen und wird gerne eingesetzt, um andere Programme, Webseiten sowie Prozesse visuell zu erklären. Durch das Einfügen interaktiver Elemente können auch Tutorials und E-Learning-Kurse erstellt werden und ist eine Alternative zu Adobe Captivate. \n\nIm Oktober 2005 gewann TurboDemo den 1. Preis beim 6. Learning Management Congress in München und setzte sich in der Kategorie Software-Simulation unter anderem gegen Learn Cube von X-Pulse und Captivate von Macromedia durch.\n\n"}
{"id": "3653807", "url": "https://de.wikipedia.org/wiki?curid=3653807", "title": "Futex", "text": "Futex\n\nEin Futex ( etwa „schneller gegenseitiger Ausschluss im Userspace“) ist ein Mutex-Lockingmechanismus, der vom Betriebssystem Linux unterstützt wird. Die Besonderheit der Futex-Implementierung liegt darin, dass ein Großteil der Operationen im Userspace ausgeführt wird, und dadurch teure Aufrufe des Kernels vermieden werden. Im Gegensatz zum Mutex dient ein Futex deshalb zur Synchronisation von zu einem Prozess gehörenden Threads.\n\nDie Implementierung stammt von Hubertus Franke (IBM Thomas J. Watson Research Center), Matthew Kirkwood, Ingo Molnár (Red Hat) und Rusty Russell (IBM Linux Technology Center), und wurde ab Version 2.5.7 Teil des Linux-Kernels.\n\nWie ein Mutex ist ein Futex eine Speicherstelle, die von verschiedenen Prozessen durch Sperr- und Entsperroperationen \"(lock/unlock)\" atomar verändert wird. Die Sperroperation stellt dabei sicher, dass alle Prozesse blockiert werden, die einen schon gesperrten Futex selbst sperren möchten \"(gegenseitiger Ausschluss)\". Im Gegensatz zu Mutexen werden die Futex-Operationen aber in den statistisch relevanten Fällen im User Space (d. h. direkt vom laufenden Programm) ausgeführt und der Kernel nur dann aufgerufen, wenn ein Prozess blockiert oder aktiviert werden muss.\n\nLinux unterstützt Futexe für die meisten, aber nicht für alle Prozessor-Architekturen.\n\nIm Allgemeinen implementiert ein User-Programm die Futex-Operationen nicht direkt, sondern es verwendet die \"pthread_mutex...()\" Funktionen in libc, die automatisch je nach Prozessor-Architektur Futexe oder eine interprozess-Mutex-Implementierung verwenden.\n\nDadurch, dass nicht bei jedem Aufruf das Betriebssystem involviert ist, sind Futexe im Allgemeinen effizienter. Da das Betriebssystem aber nicht mehr die Kontrolle über alle Locks hat, kam es in der Anfangsphase teilweise zu Problemen, wenn Prozesse mit gesperrtem Futex abgestürzt sind. Diese Probleme wurden durch \"robuste Futexe\" behoben, die Verwendung von Futexen verlangt aber trotzdem erhöhte Aufmerksamkeit.\n"}
{"id": "3654891", "url": "https://de.wikipedia.org/wiki?curid=3654891", "title": "COMIT (Programmiersprache)", "text": "COMIT (Programmiersprache)\n\nCOMIT war die erste Programmiersprache, die Verarbeitung von Zeichenketten erlaubte. Sie wurde unter Federführung von Dr. Victor Yngve in den Jahren 1957–1965 am Massachusetts Institute of Technology auf einem Großrechner der Serie IBM 700/7000 entwickelt. Yngve legte die Sprache für den Einsatz in der Linguistik aus, genauer gesagt für den Bereich der computergestützten Übersetzung von natürlichen Sprachen. Die Entwicklung von COMIT gab den Anstoß für die Entwicklung von SNOBOL durch David J. Farber, Ralph E. Griswold und Ivan P. Polensky.\n\n"}
{"id": "3674153", "url": "https://de.wikipedia.org/wiki?curid=3674153", "title": "Macintosh Classic", "text": "Macintosh Classic\n\nDer Macintosh Classic ist ein Personal Computer der Firma Apple und wurde vom 15. Oktober 1990 bis 14. September 1992 hergestellt. Es war der erste Apple Macintosh, der für weniger als US$1,000 verkauft wurde.\n\nDer Classic ersetzte die populären Modelle Macintosh Plus und Macintosh SE. Er verfügte wie diese beiden Modelle über einen Prozessor vom Typ Motorola 68000, war aber zugleich das letzte Modell von Apple, bei dem dieser verwendet wurde. In Architektur, Leistung und sonstigen Eigenschaften ist er dem SE sehr ähnlich, konnte aber aufgrund von Erneuerungen in der Fabrikation günstiger hergestellt werden. Wie der SE hat er einen monochromen 9\" CRT-Bildschirm mit einer Auflösung von 512×342 Pixeln. Der Arbeitsspeicher war fest auf der Hauptplatine verlötet und war ein Megabyte groß, konnte aber mit einer speziellen Erweiterungskarte auf bis zu 4 MB erweitert werden.\nDer Classic bot gegenüber dem Macintosh Plus ein paar Erweiterungen und ersetzte diesen als Apples Low-End-Computer. Er war bis zu 25 % schneller und beinhaltete standardmäßig ein Apple SuperDrive-Diskettenlaufwerk. \n\nEr war eine Adaptation von Jerry Manocks und Terry Oyamas Design des Macintosh 128k. Der Classic erschien in zwei Versionen im Preissegment von knapp unter $1,000 bis zu $1,500. Die Reaktionen der Computernutzer waren gespalten; viele störten sich am verhältnismäßig langsamen Prozessor und an den fehlenden Erweiterungsmöglichkeiten. Die verbreitete Ansicht war, dass er sich nur für Textverarbeitung und Tabellenkalkulation eignen würde. Jedoch machten der günstige Preis und die gute Verfügbarkeit von Lernsoftware den Classic zu einem beliebten Computer für den Schulsektor.\n\n\n"}
{"id": "3683788", "url": "https://de.wikipedia.org/wiki?curid=3683788", "title": "Xtext", "text": "Xtext\n\nXtext ist ein Open-Source-Framework für die Entwicklung von Programmiersprachen sowie domänenspezifischen Sprachen (, \"DSL\") und ein Teil des Eclipse-Modeling-Framework-Projekts (EMF). Im Gegensatz zu normalen Parsergeneratoren wird bei Xtext nicht nur ein Parser generiert, sondern auch ein EMF Metamodell (ein Klassenmodell für den abstrakten Syntaxbaum) und ein in Eclipse integrierter Texteditor sowie die notwendige Infrastruktur für die Implementierung einer modernen Entwicklungsumgebung für die entwickelte Sprache bereitgestellt.\n\nDie erste Version von Xtext wurde im Jahr 2006 im Rahmen des Projektes openArchitectureWare (oAW) veröffentlicht. In der darauf folgenden Zeit wurden mit jeder Version neue Funktionen und Konzepte integriert. In der Version 4.3 von oAW ist die letzte oAW-Version von Xtext enthalten. Seit Anfang 2008 wird Xtext unterhalb des Eclipse Modeling Project im \"Textual Modeling Framework (TMF)\" weiterentwickelt. Mitarbeiter der Firmen itemis AG und TypeFox entwickeln hier eine neue Version des Frameworks. Das erste offizielle Release erfolgte am 16. Juni 2009, als Xtext in der Version 0.7.0 erschien.\n\nIm Juni 2010 hat das Framework die Version 1.0 erreicht. Version 2.0 erschien am 27. Juni 2011 zusammen mit Eclipse 3.7 (Indigo).\n\nIm Juni 2012 wurde mit Version 2.3 die \"Expression Language\" Xbase vorgestellt. Xbase ermöglicht einerseits eine nahtlose Integration einer DSL in das \"Java Typsystem\" und stellt andererseits typische Ausdrücke (Expressions) in einer Java-ähnlichen Syntax zur Integration in eine DSL zur Verfügung. Eine weitere Neuerung von Version 2.3 ist die Programmiersprache Xtend, eine in Xtext selbst entwickelte, an Java angelehnte Programmier- und Templatesprache. Sie wird seitdem zur Implementierung vieler Sprach- und IDE-Konzepte propagiert.\n\nDie nächste große Neuerung von Xtext wurde am 1. Dezember 2015 mit Version 2.9 vorgestellt. Xtext steht nun ebenfalls als Plug-In für die Entwicklungsumgebung IntelliJ IDEA zur Verfügung. Sowohl in der Version für Eclipse als auch in der Version für IntelliJ können Plug-ins für beide Plattformen entwickelt werden. Weiterhin besteht die Möglichkeit einen DSL Editor für eine Webanwendung zu generieren. Auch die Verwendung der Buildsysteme ist nun (teilweise) möglich.\n\nMit Xtext werden (domänenspezifische) Programmiersprachen entwickelt. Es ist weit mehr als ein einfacher Parsergenerator wie z. B. ANTLR, welcher dennoch Verwendung in Xtext findet um Quelltexte einzulesen und einen abstrakten Syntaxbaum (AST) abzuleiten. Es bietet Möglichkeiten, den AST zu analysieren und zu validieren, den AST in eine beliebige andere (textuelle) Repräsentation zu überführen oder Javaklassen daraus abzuleiten. Zusätzlich zu diesen typischen Compilerfunktionen bietet Xtext Möglichkeiten viele IDE Konzepte für die Sprache zu implementieren. So kann am Ende des Entwicklungsprozesses ein Plug-In für Eclipse oder IntellJ stehen, welches mit Hilfe dieser Plattformen eine komplette IDE zur Verfügung stellt.\n\nIm Folgenden werden die verschiedenen Funktionen und Konzepte, die mit Xtext implementiert werden können, kurz beschrieben.\n\nXtext verwendet einen MWE2 Workflow um die Sprache zu konfigurieren. Wird der Workflow ausgeführt, so wird der Parser und das Metamodell generiert. Weiterhin werden Klassen- und Methodenrümpfe erzeugt, um verschiedene Compiler- und IDE-Konzepte zu implementieren. Werden die Konzepte nicht implementiert, werden die Standardimplementierungen von Xtext verwendet.\n\nDieser Abschnitt fasst Konzepte zusammen, die typischerweise Teil des Compilers sind. Ein Compiler parst den Quelltext und erstellt einen abstrakten Syntaxbaum (AST, Abstract Syntax Tree). Bevor dies geschieht, wird eine Syntaxanalyse durchgeführt um ggf. Syntaxfehler zu finden. Nur wenn der Quelltext frei von Syntaxfehlern ist, wird der AST erzeugt. Auf dem AST werden oft statische Analysen durchgeführt (z. B. Typüberprüfung, Überlagerung/Schattierung, Sichtbarkeit, etc.). Diese Analysen können Fehler aufdecken. Sollte der AST fehlerfrei sein, so wird er in die Ausgabesprache überführt. Dies kann eine beliebige (textuelle) Repräsentation sein oder Quelltext in einer beliebigen (Java) anderen Programmiersprache.\n\nXtext generiert vollautomatisch einen ANTLR-Parser für die Sprache. Hierfür wird der ANTLR-Parsergenerator verwendet. Es muss für dieses Konzept lediglich die Grammatik der Sprache (syntaktische Regeln) definiert werden. Hierfür stellt Xtext eine eigene Sprache zur Verfügung. Das Besondere an der Grammatiksprache von Xtext ist, dass damit nicht nur die konkrete, sondern auch die abstrakte Syntax beschrieben wird. Üblicherweise wird aus der Grammatik nicht nur der ANTLR Parser generiert, sondern auch ein Ecore Metamodell und seine Implementierung in Java. Es wird als AST verwendet. Der generierte ANTLR TreeParser erzeugt eine Instanz dieses Modells und gibt es als AST zurück. Existiert bereits ein Metamodell und es soll nur noch die textuelle Syntax definiert werden, so kann das Metamodell in der Xtext-Grammatiksprache importiert werden. Dann wird auch kein eigenes Metamodell erzeugt.\n\nDie Syntax der Grammatiksprache wird im Folgenden beschrieben. Jede Grammatik verfügt über einen Header, in dem der Name der Sprache und die Vatergrammatik definiert werden. Weiterhin können Metamodelle der Vatersprachen importiert werden, um ihre Regeln nutzen können. Sofern kein existierendes Metamodell verwendet wird, wird hier ebenfalls das Ecore Metamodell definiert:\ngrammar org.eclipse.mydsl.MyDsl with org.eclipse.xtext.xbase.Xbase\n\nimport \"http://www.eclipse.org/emf/2002/Ecore\" as ecore\nimport \"http://www.eclipse.org/xtext/common/JavaVMTypes\"\nimport \"http://www.eclipse.org/xtext/xbase/Xtype\"\ngenerate MyDSL \"http://www.eclipse.org/xtext/mydsl/MyDsl\"\nAuf den Header folgt die oberste (Root)-Regel der Sprache. Alle weiteren Regeln folgen in beliebiger, aber sinnvoller Reihenfolge auf die Rootregel. Eine Regel hat einen Namen, der mit einem Großbuchstaben beginnt, gefolgt von einem Doppelpunkt. Beendet wird eine Regel mit einem Semicolon. Zeichen, die in der definierten Sprache stehen, wie z. B. Kommas, Punkte, aber auch Schlüsselwörter, werden in einfache Anführungszeichen gesetzt. Enthält eine Regel weitere, (komplexe) Regeln oder hat bestimmte Attribute, so wird zunächst der Name des Attributes geschrieben (beginnend mit einem kleinen Buchstaben), gefolgt von einem Gleichheitszeichen und dann dem Namen der Kindregel. Eine typische Regel könnte z. B. so aussehen:\nEs ist zwar nicht bekannt, wie die Regel \"Value\" aussieht, aber der hier beschriebene Quelltext könnte etwa so aussehen:\nEin Attribut muss grundsätzlich genau einen Wert enthalten, es sei denn es ist als optional gekennzeichnet oder es enthält eine Liste von Werten. Um ein Wert als optional zu kennzeichnen, wird ihm ein Fragezeichen angehängt, eine Liste von Werten, die auch leer sein darf, wird durch einen Stern gekennzeichnet und eine Liste, die mindestens ein Element enthalten muss, durch ein Plus. Der Zuweisungsoperator für Listenattribute ist das Plusgleich:\nNachdem die Grammatik der Sprache implementiert oder geändert wurde, muss der MWE2-Workflow ausgeführt werden, damit Parser und Metamodell (neu) generiert werden.\n\nSobald der MWE2 Workflow das erste Mal ausgeführt wird, erzeugt Xtext die Klasse codice_1. Diese ist zunächst leer und es können beliebig viele Methoden implementiert werden, die einzelne Knoten des AST's prüfen oder analysieren können. Xtext findet diese Methoden und führt sie kontextabhängig automatisch aus. Um Xtext mitzuteilen, das eine bestimmte Methode eine Analyse enthält, muss diese einerseits mit der Annotation codice_2 gekennzeichnet werden und andererseits darf sie nur genau einen Parameter haben, dessen Typ ein Typ des Metamodells ist. Eine solche Analysemethode könnte in etwa so aussehen:\nDiese Methoden haben keinen Rückgabewert (void). Wenn Fehler gefunden werden, sollte eine der codice_3 Methoden aufgerufen werden. Diese empfängt einige Informationen und erstellt so einen Fehlermarker.\n\nAnalog zu Fehlern können (für den Editor, siehe IDE Konzepte) auch Warnungen (codice_4) oder Informationen (codice_5) erzeugt werden. Diese Ergebnisse werden vom Editor im Quelltext angezeigt.\n\nXtext erzeugt für jede Sprache eine Klasse, mit der die Sprache in eine bestimmte Ausgabe übersetzt werden kann. Hierzu gibt es ab Version 2.3 zwei Möglichkeiten. Wird mit der Sprache z. B. eine Datenstruktur aufgebaut, so kann eine XML-Ausgabe oder ein JSON Object erzeugt werden, oder eine andere beliebige (textuelle) Repräsentation. Oft kommt es aber auch vor, dass eine Sprache einen kleinen Teilaspekt von einem großen Programm abdeckt. Dann ist es sinnvoll, Java-Klassen zu erzeugen. Hierzu wird der ModelInferrer verwendet. Er leitet eine Java-Klasse ab und integriert die Sprache somit nahtlos in das Javatypsystem (große Vorteile auf IDE-Ebene).\n\nStandardmäßig wird erst einmal eine Klasse codice_6 erzeugt. Diese enthält genau eine Methode codice_7. Hier kann der Generationsprozess implementiert werden, der die Ausgabe des Compilers berechnet. Diese Methode wird beim Speichern der Datei im Eclipse Editor automatisch ausgeführt. Die erzeugten Dateien werden in der Regel im Verzeichnis codice_8 abgelegt, es sei denn etwas anderes wird eingestellt. Das Ausgabeformat ist hier beliebig.\n\nBei der Verwendung von Xbase (z. B. um Cross-Referencing zu Java-types zu benutzen) wird anstatt der Generatorklasse ein ModelInferrer erzeugt, den es zu implementieren gilt. Dieser ermöglicht das Ableiten von Javatypen aus der Sprache. Hier muss auch nicht die Datei an sich erzeugt werden, sondern lediglich eine Transformation des Sprach-AST's hin zu einem Java-AST implementiert werden. Die eigentliche Serialisierung und Übersetzung in Bytecode übernimmt Xtext.\n\nXtext bietet auch die Möglichkeit, einen Interpreter anstelle eines Codegenerators zu implementieren, sodass der Code der Sprache direkt ausgeführt werden kann.\n\n(tbd) Neben den Compilerkomponenten, die teilweise auch IDE Konzepte implementieren, bzw. dem Editor zuarbeiten, können für Xtext-Sprache diverse IDE-Konzepte implementiert werden.\n\nDer MWE2 Workflow erzeugt automatisch ein Editor-View mit einem Basis-Syntax-Highlighting.\n\nDieses kann sowohl syntaktisch als auch semantisch angepasst/verfeinert werden. Es sind keine Grenzen bezüglich Farben, Schriftarten, Textdekorationen gesetzt. Mit einem entsprechenden Aufwand kann eine Seite für die Spracheinstellungen implementiert werden, in der der Endnutzer das Highlighting an die eigenen Bedürfnisse anpassen kann (z. B. Dark Theme oder Rot-Grün-Sehschwäche).\n\nDas aus Eclipse bekannte Feature der Autoformatierung von Quelltext kann ebenfalls für eine Xtextsprache implementiert werden. Xtext stellt hierfür eine API bereit. Die hier definierten Patterns werden auf den Quelltext angewendet, wenn der entsprechende Befehl gegeben wird bzw. wenn eine Quelltextdatei erzeugt wird, indem der AST Stück für Stück aufgebaut und dann serialisiert wird.\n\nDas aus Eclipse bekannte Feature der Hyperlinks zu Feld-, Methoden- oder Klassendefinitionen zu springen, kann ebenfalls für Xtextsprachen implementiert werden. Xtext stellt hierfür eine API bereit. In Verbindung mit der \"Expression language\" Xbase und dem JvmModelInferrer kann so auf IDE-Ebene eine nahtlose Integration der Sprache in das Java Typsystem geschaffen werden. Einerseits lässt sich so ein Hyperlink zu einer Methode einer Java-Klasse implementieren, andererseits ist es auch möglich in Javaklassen die abgeleiteten Javatypen zu verwenden. Wird der Hyperlink im Javaeditor angeklickt, öffnet dann sogar der Xtext-Spracheditor die entsprechende Datei und markiert den Referenzierten Teil.\n\nWie im Abschnitt über die Compiler-Konzepte beschrieben, können Fehler, Warnungen und Informationen im Quelltext gefunden werden. Diese werden automatisch im Editor von Eclipse angezeigt.\n\nXtexteditoren unterstützten automatisch das Zusammenklappen von Quelltextblöcken.\n\nXtext stellt eine Klasse bereit, um das Autovervollständigungs-Feature von Eclipse zu unterstützen. So lässt sich implementieren, welche Inhalte vorgeschlagen werden. In Kombination mit den angezeigten Fehlern und Warnungen können in gewissem Maße sogar Verbesserungsvorschläge generiert werden. Diese können dann den als fehlerhaft markierten Bereich eines Dokumentes durch den Vorschlag ersetzen um Fehler zu entfernen.\n\nAus Eclipse ist der s.g. Outline View bekannt. Dieser zeigt z. B. alle Felder und Methoden einer Java-Klasse an. Ein solcher Outline View kann auch für eine Xtextsprache implementiert werden. Dieser ist dann auch mit der geöffneten Datei verknüpft, sodass der Courser mit einem Klick auf ein Element im Outline zu der entsprechenden Stelle im Text springt.\n\nRename Refactoring wird von Xtexteditoren unterstützt.\n\nDSLs sind formale Sprachen und müssen daher ausführbar gemacht werden. Dies geschieht auf zwei unterschiedlichen Wegen:\n\n\n"}
{"id": "3684353", "url": "https://de.wikipedia.org/wiki?curid=3684353", "title": "Sony Ericsson Xperia X1", "text": "Sony Ericsson Xperia X1\n\nDas Xperia X1 ist das erste Modell in Sony Ericssons Xperia-Baureihe. Das Xperia X1 wurde erstmals auf dem Mobile World Congress 2008 vorgestellt.\n\nDas X1 ist ein ARC-Slider mit Windows Mobile 6.1 Professional als Betriebssystem. Es ist Sony Ericssons erstes Smartphone mit Windows Mobile.\n\nDas X1 besitzt ein 3\" großen Touchscreen und eine vollwertige QWERTZ-Tastatur. Der WVGA-Display besitzt 65.536 Farben und ist in der Lage die Anzeige, bei geöffneter Tastatur, automatisch auf das Querformat umzustellen. Außerdem ist auf der Rückseite eine Kamera mit 3,2 Megapixeln angebracht, die Bilder mit einer Auflösung von 2048 × 1536 Pixeln und Videos mit 30 Bildern pro Sekunde und einer Auflösung von bis zu 640 × 480 Pixeln aufnehmen kann. Zusätzlich ist an der Vorderseite des Telefons eine kleine QCIF-Kamera für Videogespräche angebracht. Das X1 hat 512 MB internen Speicher, von dem noch etwa 400 MB für eigene Daten genutzt werden können. Der Speicher ist durch eine microSD-Speicherkarte auf max. 32 GB erweiterbar.\n\nDas Gerät besitzt einen Qualcomm ARM 11 MSM7200A 528 MHz Dual Core CPU (die US-Version einen MSM7201A), mit 528 MHz Taktfrequenz. Der virtuelle Speicher beträgt 384 MB (256 MB Arbeitsspeicher + 128 MB Grafikspeicher).\n\n\nDas X1 wird mit Opera Mobile als Web-Browser und Office Mobile ausgeliefert. Es unterstützt Push-Mail, RSS-Web-Feeds und Handschrifterkennung.\n\nEs wurde im Auftrag von Sony Ericsson von HTC unter dem Namen Kovsky produziert, die mit dem HTC Touch Diamond bzw. HTC Touch Pro ein hardwareseitig sehr ähnliches Mobiltelefon auf dem Markt hatten.\n\n\nDas X1 wurde in zwei Farben verkauft\n\n\nGesprächszeit:\n\nStandby:\n\nDas XPERIA X1 unterstützt alle gängigen Übertragungsprotokolle, welche ein problemloses Streamen von Audio- und Videodaten ermöglichen.\n\nVerbindungsmöglichkeiten:\n\n"}
{"id": "3684358", "url": "https://de.wikipedia.org/wiki?curid=3684358", "title": "Sony Xperia", "text": "Sony Xperia\n\nDie Marke Xperia steht für eine Produktserie von Smartphones und Tablets von Sony (beziehungsweise Sony Mobile nach der Auflösung von Sony Ericsson).\n\nDie Xperia-Produktserie wurde am 10. Februar 2008 in Barcelona vorgestellt, um dem Konvergieren der Anwendungsbereiche Web, Multimedia und Applikationen auf hochwertigen mobilen Endgeräten entgegenzukommen. Der Zugriff auf alle wichtigen Funktionen soll durch eine von Sony Ericsson neu entwickelte Benutzeroberfläche, dem „Xperia panel interface“, besonders einfach sein. Mit der Aussage von Rikko Sakaguchi (Head of Portfolio and Propositions, Sony Ericsson): \"„Our vision for the XPERIA™ X1 is to deliver a seamless blend of mobile Web communication and multimedia entertainment within a distinctive design“\" (deutsch: \"„Unser Ziel für das XPERIA™ X1 ist es, eine nahtlose Verschmelzung von mobiler Netzkommunikation und Multimedia-Unterhaltung in einem unverwechselbaren Design zu liefern.“\") positionierte Sony Ericsson das erste Gerät der Serie klar im gleichen Segment wie das iPhone. Seitdem Sony seine Smartphones und Tablets nicht mehr mit Windows Mobile, sondern mit Android ausliefert, wird das sogenannte Sony UI, eine für Android konzipierte Benutzeroberfläche, zu Android hinzugefügt. Diese beinhaltet vorinstallierte Apps, ein eigenes Design und verschiedene Funktionen, wie zum Beispiel eine ins System integrierte Unterstützung des DualShock 4 Controllers oder einen Energiesparmodus mit dem Namen „Stamina Mode“.\n\nNeben zahlreichen Smartphones und Tablet-PCs gibt es seit 2017 eine Besonderheit mit einem integrierten digitalen Projektor: Der Sony Xperia Touch verfügt über eine Infrarotkamera, mit der die Lage eines Fingers auf der Projektionsfläche ermittelt werden kann. Das Gerät soll damit ähnlich wie ein Tablet-PC bedient werden können.\n\nDas \"Sony Ericsson Xperia X1\" ist das erste Modell der Xperia-Serie. Das Gerät ist ein Smartphone mit einer herausziehbaren QWERTZ-Tastatur und verfügt über einen 3-Zoll-WVGA-Touchscreen mit einer Auflösung von 480 × 800 Pixeln. Als Betriebssystem wird Windows Mobile 6.1 Professional eingesetzt, auf dem das \"Xperia panel interface\" aufbaut. Das X1 ist das erste Mobiltelefon von Sony Ericsson mit einem Windows-Betriebssystem. Das X1 soll eines der ersten Mobiltelefone auf dem europäischen Markt sein, das neben WLAN und HSDPA auch HSUPA zur Datenübertragung unterstützt. Weitere Besonderheiten des Produkts sind ein A-GPS-Empfänger sowie ein dedizierter Grafikchip. Es wurde im Auftrag von Sony Ericsson von HTC unter dem Namen \"Kovsky\" produziert, das mit dem HTC Touch Diamond bzw. HTC Touch Pro ein hardwareseitig sehr ähnliches Mobiltelefon auf dem Markt hatte.\n\nDas Sony Ericsson Xperia X2 ist das zweite Modell der Xperia-Serie. Das Gerät trat die Nachfolge des X1 an und ist mit Windows Mobile 6.5 ausgestattet. Nach offiziellen Angaben von Sony Ericsson wurde das X2 kurz nach dem Marktstart mit Version 6.5.1 und später 6.5.3 des Windows-Mobile-Betriebssystems aktualisiert. Die Installation von Windows Phone 7 ist aus technischen Gründen nicht möglich. Das X2 hat weitestgehend die gleichen Hardwarespezifikationen wie das X1, was den Prozessor, Grafikchip, RAM und Flash-Speicher angeht, bietet aber auch einige technische Verbesserungen. Dazu gehören die 8-Megapixel-Kamera mit Autofokus sowie LED-Blitz, Stereo-Lautsprecher und ein mit 3,2 Zoll etwas größeres Display.\nDer offizielle Marktstart war im Januar 2010 in den USA, März 2010 in Frankreich und Großbritannien und Ende Mai 2010 im restlichen Europa.\n\nDas Sony Ericsson X5 („X5“ bzw. „X5i“), auch „Sony Ericsson Xperia Pureness“ genannt, ist ein im September 2009 vorgestelltes Mobiltelefon, das im Gegensatz zu anderen Xperia-Smartphones keinen Touchscreen hat und außerdem ein proprietäres Betriebssystem von Sony Ericsson verwendet. Sein TFT-Display galt damals als neuartig.\n\nDas Sony Ericsson Xperia X8 (Codename Shakira) verfügt über EGPRS, EDGE, UMTS, HSDPA, WLAN und A-GPS. Als Betriebssystem kommt Android 1.6 zum Einsatz (eine Aktualisierung auf Version 2.1 ist möglich). Sony Ericsson hat bekanntgegeben, dass es für das Xperia X8 keine weiteren Software-Aktualisierungen geben werde. Es ist mit einem kapazitiven Touchscreen und einer 3,2-Megapixel-Kamera mit Fix-Fokus ausgestattet. Der Prozessor läuft mit einer Taktfrequenz von 600 MHz. Das Mobiltelefon hat eine 3,5-mm-Klinkenbuchse zum Anschluss herkömmlicher Kopfhörer und Headsets. Hierbei handelt es sich um einen erweiterten Anschluss, an den auch Kopfhörer mit Kabelfernbedienung angeschlossen werden können; passende Kopfhörer mit Fernbedienung für den proprietären Stecker sind nur von Sony Ericsson erhältlich. Apps und Aktualisierungen für die bereits installierten Softwarepakete können über den Android Market bezogen oder als apk-Datei auf das Gerät übertragen und so installiert werden. Der verbaute Akku (Modellbezeichnung E15i bzw. E15a) ermöglicht eine Musikwiedergabedauer von ca. 23:40 Stunden.\n\nDas Sony Ericsson Xperia X10 (Codename \"Rachael\") ist das erste Smartphone der Xperia-Reihe, das auf dem Mobil-Betriebssystem Android basiert. Das Android-Betriebssystem ist vom Hersteller mit einer eigenen Benutzeroberfläche ausgestattet, die sich User-Experience-Plattform (UX) nennt.\n\nIm Herbst 2010 gab es eine Aktualisierung von Android 1.6 auf 2.1.\nSony Ericsson kündigte am 25. März für Juli/August 2011 die Version 2.3.3 an.\nAm 29. Juli 2011 wurden einige Länder damit versorgt. (Europa, Asien, …), andere Länder sollten bis zum 5. August 2011 folgen. Eine Aktualisierung auf Android 4.0 erhielt das Gerät nicht mehr.\n\nIm Februar 2010 wurde das Sony Ericsson Xperia X10 mini (Codename \"Robyn\") und Sony Ericsson Xperia X10 mini pro (Codename \"Mimmi\") angekündigt. Diese Modelle fallen sehr klein aus. Der Unterschied zwischen \"mini\" und \"mini pro\" liegt darin, dass die \"pro\"-Version eine vollwertige QWERTZ-Tastatur bietet und der Akkumulator austauschbar ist. Die beiden \"Mini\"-Versionen haben einen 2,55 Zoll kleinen Touchscreen (kapazitiv), einen 600-MHz-Qualcomm MSM7227-Prozessor, eine 5-MP-Kamera, die bereits im „großen“ X10 verwendete Oberfläche (UX) und Timescape. Beide Smartphones wurden mit dem Betriebssystem Android 1.6 ausgeliefert. Am 30. Oktober 2010 veröffentlichte Sony Ericsson eine Aktualisierung auf Android 2.1.\n\nFür das Quadband-Smartphone Xperia arc (auch als Xperia X12 oder Anzu bezeichnet) gibt es Android 4.0.4 als Aktualisierung (Auslieferung 2.3.2). Die aktuelle Firmware-Version ist 4.1.B.1.13. Es besitzt ein UKW-Radio und unterstützt WLAN 802.11b/g/n sowie A-GPS und Bluetooth. An der Oberseite befindet sich eine Mini-HDMI-Schnittstelle, am oberen Ende der linken Außenseite ein 3,5-mm-Klinkenbuchse und an der rechten ein Micro-USB zum Aufladen des Akkumulators und für Datenübertragungen. Es hat einen TFT-LC-Bildschirm, der laut Hersteller durch die BRAVIA-Engine Farben lebensechter darstellen soll.\n\nDas Sony Ericsson Xperia arc S wird mit Android 2.3.4 („Gingerbread“) ausgeliefert. Eine Aktualisierung auf Android 4.0.4 („Ice Cream Sandwich“) ist verfügbar. Bis auf den Prozessor und die Unterstützung von HSDPA mit 14,4 Mbit/s (statt 7,2 Mbit/s) ist die Hardware identisch mit dem Vorgängermodell Xperia arc.\n\nDas Xperia Play hat die Besonderheit, dass es eine physische Eingabefläche, ähnlich einem PlayStation-Controller, hat. Damit kann man speziell angepasste Android-Spiele steuern. Auf dem Xperia Play sind bereits einige solcher Spiele installiert, beispielsweise FIFA 10 oder Die Sims 3.\n\nFür die beiden Quadband-Smartphones steht Android 2.3.4 (Auslieferung beim Neo: 2.3.2 / Update auf 4.0 für beide verfügbar) zur Verfügung. Die Spezifikationen sind fast identisch; das Xperia pro hat im Gegensatz zum Xperia neo eine vollwertige Tastatur und ist daher größer und schwerer. Sie unterstützen Wi-Fi, A-GPS und UKW-Radio. Außerdem brachte Sony Ericsson eine abgeänderte Version des Xperia Neo heraus. Diese heißt Xperia Neo V und unterscheidet sich nur durch eine 5-Megapixel-Kamera.\n\nIm Mai 2011 wurde das Sony Ericsson Xperia mini und Sony Ericsson Xperia mini pro (Codename mango) angekündigt. Diese Modelle haben ungefähr Scheckkartenformat und sind damit das „weltweit kleinste Smartphone mit HD-Videoaufnahme“ (bezogen auf das Produkt aus Länge × Breite, im Juni 2011). Trotzdem hat die Pro-Version eine physische Tastatur und eine zweite Kamera auf der Vorderseite. Wie auch bei den Highend-Smartphones \"Xperia Arc\" und \"Xperia Play\" kommt die \"Mobile Bravia Engine\" (Bildqualität) zum Einsatz. Die Kamera beider Modelle hat eine Auflösung von 5 Megapixeln. Es wird Wi-Fi, A-GPS und UKW-Radio empfangen. Beide Smartphones wurden beim Verkaufsstart im dritten Quartal 2011 mit dem Betriebssystem Android in der Version 2.3.3 ausgeliefert. Anfang November 2011 wurden beide Modelle mit neuer Firmware 4.0.2. A.0.58 auf Android 2.3.4 aktualisiert. Ende 2012 folgte Android 4.0.4, das ausschließlich über das Programm \"PC Companion\" installiert werden konnte. Für die Durchführung der Aktualisierung war also ein Computer erforderlich.\n\nDas Sony Ericsson Xperia ray erschien mit September 2011 mit dem Betriebssystem Android 2.3.3, später wurde 4.0.4 nachgeliefert. Im Herbst 2012 gab Sony bekannt, keine weiteren Versionen mehr für das Xperia ray bereitzustellen.\n\nDas Quadband-Smartphone Xperia active mit Android 2.3.3 (Update auf 4.0.4 verfügbar) ist speziell für den sportlichen Einsatz ausgerichtet. Es ist gemäß IP67 zertifiziert, so dass es wasserfest, staubgeschützt ist, und verfügt über einen Touchscreen, der sich auch mit nassen Fingern bedienen lässt.\n\nDas zweite Smartphone von Sony, seit der Übernahme von Ericsson, ist das Sony Xperia ion. Es war von der Bildschirmgröße das seinerzeit größte Smartphone von Sony und unterstützt als Besonderheit den LTE-Standard. Es wurde in den USA über AT&T vertrieben. Sony verteilte Android 4.1 für das Ion am 25. Juni 2013. Für Europa gibt es eine Variante, die auf HSPA basiert. Die Modellbezeichnung lautet „LT28h“ bzw. „LT28i“\n\nDas Sony Xperia S ist das erste Smartphone von Sony Mobile mit Mehrkernprozessor, und gleichzeitig das erste Smartphone nach der Auflösung des Joint Ventures Sony Ericsson. Die Modellbezeichnung ist „LT26i“. Die wassergeschützte Outdoor-Variante ist das Xperia acro S mit der Modellbezeichnung „LT26w“. Im Test schneidet das Sony Xperia S sowohl beim Design, der Sprachqualität, der Bildschirmdarstellung als auch mit seiner 12-Megapixel-Kamera gut ab. Die Akkulaufzeit ist dagegen weniger gut. Das Xperia S ist mit einem Micro-Simkarten-Slot ausgestattet – normale SIM-Karten können nicht mehr verwendet werden. Dass das Smartphone noch mit der Android-Version 2.3.7 Gingerbread anstatt des seit November 2011 existierenden aktuellen Android-Betriebssystems 4.0 Ice Cream Sandwich (ICS) ausgeliefert wird, werteten Experten als Enttäuschung. Am 21. Juni 2012 begann Sony mit der Auslieferung von Android 4.0.4 für das Xperia S. Am 29. Mai 2013 wurde für das Sony Xperia S die Version Android 4.1 verteilt. Sony bestätigte in verschiedenen Internetportalen, dass das Xperia SL im deutschen Raum nicht in den Handel kommen wird.\n\nDas Sony Xperia P ist das zweite Sony-Smartphone der Sony-Xperia-NXT-Serie. Das Gehäuse besteht als Besonderheit aus einem Stück Aluminium. Außerdem hat es eine neuartige Bildschirm-Matrix, die sich „WhiteMagic“ nennt. Es trägt die Modellbezeichnung „LT22i“.\n\nDas Sony Xperia U ist das dritte Modell der Sony-Xperia-NXT-Serie. Das Besondere ist, dass es die Farbe des transparenten Streifens ändern kann. Es trägt die Modellbezeichnung „ST25i“ bzw. „ST25a“. Die beiden Modelle unterscheiden sich hauptsächlich nur in der Nutzung der Frequenzbänder im UMTS-/HSPA-Bereich.\n\nDas Xperia sola („MT27i“) ist das vierte Smartphone von Sony und unterscheidet sich im Design deutlich von den Modellen der NXT-Serie. Außerdem wurde eine andere Display-Technik verbaut, die es u. a. erlaubt, Effekte hervorzurufen, indem man seinen Finger nahe an den Bildschirm hält, ohne diesen zu berühren. Sony nennt dies „floating touch“.\n\nDas Xperia go wurde am 30. Mai 2012 angekündigt. Es ist dem Xperia sola sehr ähnlich, aber rundlicher und mit anderer Rückseite. Außerdem wird es von Sony Mobile als eines der „Outdoor“-Smartphones vertrieben, wie auch das Xperia active. Somit ist es nach IP 67 angefertigt. In den USA trägt es den Namen „Xperia Advanced“ Die Modellbezeichnung lautet „ST27i“ bzw. „ST27a“.\nDas Sony Xperia acro S ist dem in Japan bereits erhältlichen Xperia acro HD (Release: März 2012) sehr ähnlich, sowohl im Design als auch in seinen Dimensionen. Sony vermarktet es, wie das Xperia go, als „Outdoor“-Smartphone, wonach es eine Zertifizierung gemäß IP55 und IP57 trägt. Es trägt die Modellbezeichnung „LT26w“. Außerdem kam es im dritten Quartal 2012 mit installiertem Android 4.0 („Ice Cream Sandwich“) auf den Markt.\n\nDas Xperia neo L ist der Nachfolger des Xperia neo unter der Marke Sonys. Es erschien ausschließlich im südostasiatischen Raum und trägt die Bezeichnung „MT25i“.\n\nDas Sony Xperia tipo, das im August 2012 erschienen ist, ist ein Einsteiger-Smartphone der Xperia-Reihe von Sony. Eine Besonderheit ist: Es existiert auch als Dual-SIM-Variante unter dem Namen Xperia tipo dual.\n\nDas Sony Xperia miro, trägt die Modellbezeichnung „ST23i“, das im Design dem Xperia sola ähnelt und vom japanischen Elektronikkonzern als sogenanntes „fun social smartphone“ angekündigt wird, verfügt über eine volle Facebook-Integration und speziell darauf abgestimmte Funktionen, eine 5-Megapixel-Kamera auf der Rückseite des Geräts sowie eine Kamera auf der Vorderseite für Telefonate mit Video. Auf den Markt kommt das Smartphone voraussichtlich ab dem dritten Quartal 2012.\n\nDas Sony Xperia SL ist eine überarbeitete Version des Xperia S und bietet im Gegensatz zum Xperia S lediglich einen 1,7-GHz-Dual-Core-Prozessor und einen Bildschirm mit hoher Auflösung (342 ppi). Es trägt die Modellbezeichnung LT26ii. Das Xperia SL wird laut Sony nicht in Deutschland erscheinen.\n\nDas Xperia J wurde von Sony auf der IFA 2012 zusammen mit dem Xperia T und dem Xperia V vorgestellt. Es besitzt einen 4-Zoll-Bildschirm und eine 5-Megapixel-Kamera. Es trägt die Modellbezeichnung „ST26i“ bzw. „ST26a“.\n\nDas Xperia V (\"LT25i\") wurde von Sony auf der IFA 2012 zusammen mit dem Xperia T und dem Xperia J vorgestellt. Es war (nach dem Xperia ion) das zweite LTE-fähige Smartphone von Sony mit IP55/57-Zertifizierung. In Japan wurde das Xperia V unter dem Namen „Xperia AX“ zunächst vom japanischen Mobilfunkbetreiber NTT DoCoMo vertrieben.\n\nDas Xperia T (LT30p) wurde am 29. August 2012 von Sony zusammen mit dem Xperia J und dem Xperia V auf der IFA 2012 als neues Flaggschiff der Xperia-Reihe vorgestellt. Es besitzt ein 4,55 Zoll großes LC-Display mit einer Auflösung von 1280 × 720 Pixeln und eine 13-Megapixel-Kamera. Das Smartphone wurde zunächst mit dem Betriebssystem Android 4.0 „Ice Cream Sandwich“ ausgeliefert. In den USA erschien außerdem, durch AT&T vertrieben, eine Variante mit LTE-Funktechnik („Xperia TL“). Es trug die Modellbezeichnung LT30a bzw. LT30at. Nach einem Auftritt im James-Bond-Film wurde das Xperia T auch als Sonderedition „Bond Phone“ angeboten.\n\nDas Xperia TX ist eine veränderte Version des Xperia T, genauer genommen die internationale Variante des in Japan erhältlichen Xperia GX. Es besitzt einen 4,55-Zoll-Bildschirm. Die Besonderheit an dem Modell ist der auswechselbare Akku. Es trägt die Modellbezeichnung LT29i. Das Xperia TX ist nie in Deutschland erschienen.\n\nDas Xperia T2 Ultra ist anders als Xperia T und TX ein Mittelklasse-Smartphone der Xperia-Serie von Sony. Es wurde am 14. Januar 2014 zusammen mit dem Xperia E1 vorgestellt. Die Merkmale des T2 Ultra sind das 6 Zoll große HD Display sowie die geringe Tiefe von nur 7,65 Millimetern.\nEs existiert in 4 Varianten (D5303, D5306, XM50h, XM50t) als Xperia T2 Ultra mit einem SIM-Kartenschacht und in einer weiteren Variante (D5322) als Xperia T2 Ultra Dual mit 2 SIM-Kartenschächten.\n\nDas Xperia T3 ist ein Mittelklasse-Smartphone der Xperia-Serie von Sony. Es wurde am 3. Juni 2014 als Nachfolger des Xperia T2 Ultra vorgestellt. Es besitzt einen Rahmen aus rostfreiem Edelstahl und ist nochmal dünner aber auch kleiner und besitzt einen kleineren Bildschirm als das Xperia T2 Ultra. So kommt es auf eine Tiefe von 7 Millimetern, ist rund 15 Millimeter kürzer, 7 Millimeter schmäler und besitzt ein 0,7 Zoll kleineres Display.\nEs existiert in 3 Varianten (D5102, D5103, D5106) als Xperia T3 mit einem SIM-Kartenschacht.\n\nDas Xperia E wurde am 5. Dezember 2012 offiziell angekündigt. Es hat das gleiche Design wie das Xperia J, nur hat es einen 3,5-Zoll-Bildschirm. Es gibt ebenfalls eine Dual-Sim-Variante des Xperia E mit dem Namen \"Xperia E dual\". Die Typenbezeichnung lautet beim Single-SIM-Modell C1505 bzw. C1504 und bei der Dual-Sim-Variante C1605 bzw. C1604.\n\nDas Xperia E1 ist ein Einsteiger-Smartphone der Xperia-Serie von Sony. Es wurde am 14. Januar 2014 zusammen mit dem Xperia T2 Ultra als Nachfolger des Xperia E vorgestellt. Sein Äußeres ist ähnlich dem Sony-typischen Omnibalance-Design, es ist jedoch abgerundeter und sein Powerbutton ist in Schwarz anstatt in Silber. Es besitzt ein um 0,5 Zoll auf 4 Zoll größeres Display und Lautsprecher, die laut Hersteller eine Lautstärke von 100 dB erreichen. Des Weiteren besitzt es an der oberen Kante eine Walkman-Taste, die als Schnellzugriff für die Walkman-App dient.\nEs existiert in 2 Varianten (D2004, D2005) als Xperia E1 mit einem SIM-Kartenschacht und in weiteren 3 Varianten (D2104, D2105, D2114) als Xperia E1 Dual mit 2 SIM-Kartenschächten.\n\nDas Xperia E3 ist ein Einsteiger-Smartphone der Xperia-Serie von Sony. Es wurde am 3. September 2014 im Rahmen der IFA in Berlin als Nachfolger des Xperia E1 vorgestellt. Es bietet das Sony-typische Omnibalance-Design mit großen Seitenrändern, jedoch ohne Schutz vor Staub und Wasser. Für dieses Design gewann es am 31. März 2015 einen Red Dot Design Award.\n\nDas Xperia E4 ist ein Einsteiger-Smartphone der Xperia-Serie von Sony. Es wurde am 10. Februar 2015 im Rahmen des GSMA Mobile World Congress als Nachfolger des Xperia E3 vorgestellt. Sein Äußeres bietet im Vergleich zum Vorgänger viel dünnere Seitenränder und ähnelt mit seiner abgerundeten Rückseite dem Design des Xperia E1.\nEs existiert in 2 Varianten (E2104, E2105) als Xperia E4 mit einem SIM-Kartenschacht und in weiteren 3 Varianten (E2114, E2115, E2124) als Xperia E4 Dual mit 2 SIM-Kartenschächten. Außerdem wurde es in Deutschland nie verkauft.\n\nAm 24. Februar 2015 stellte Sony das Xperia E4g als LTE-Variante des E4 vor. Es besitzt im Vergleich zum Xperia E4 wieder etwas dickere Seitenränder über und unter dem Display und ein um 0,3 Zoll auf 4,7 Zoll kleineres Display. Weitere Unterschiede sind ein kleineres, dickeres Gehäuse, die Unterstützung von NFC, ein schnellerer Prozessor und eine UVP, die 30 € über der des Xperia E4 liegt, also bei 129 €.\nEs existiert in 3 Varianten (E2003, E2006, E2053) als Xperia E4 mit einem SIM-Kartenschacht und in weiteren 2 Varianten (E2033, E2043) als Xperia E4 Dual mit 2 SIM-Kartenschächten.\n\nDas Xperia Z wurde von Sony am 8. Januar 2013 auf der CES vorgestellt. Es besitzt ein 5″-Zoll-Optifine-Display mit Full-HD-Auflösung, das Schwarz noch farbechter anzeigen soll, unterstützt von der Mobile BRAVIA Engine 2, die auch im Xperia V verwendet wird. Außerdem besitzt es ein weitgehend symmetrisches Design, eine 13-Megapixel-Kamera mit HDR, verwirklicht durch den „Exmor RS for mobile“-Sensor. Es ist außerdem gemäß IP57 vor Wasser und Staub geschützt. Die Vorder- und die Rückseite bestehen aus Glas. Es gibt das Xperia Z in einer HSPA+-Version (Modell C6602) und in einer LTE-Version (C6603).\n\nZusammen mit dem Xperia Z kündigte Sony auch das \"Xperia ZL\" an. Dieses hat die gleiche Ausstattung wie das Xperia Z, aber eine Rückseite aus Kunststoff und keine IP57-Zertifizierung. Die Frontkamera befindet sich auf der unteren rechten Kante des Geräts. Es hat einen kleinen Lichtstreifen, der wie beim Xperia U die Farbe ändern kann. Es wurde zunächst mit Android 4.1 ausgeliefert.\nAm 18. März 2013 hat Sony das Xperia SP zusammen mit dem Xperia L angekündigt. Das in der oberen Mittelklasse anzuordnende Smartphone besitzt an der Unterseite einen transparenten Streifen, der bei Benachrichtigungen und bei Ansteuerung durch einige vorinstallierte Anwendungen farbig leuchten kann. Zudem ist das Xperia SP mit einem Aluminium-Rahmen rund um das Gehäuse ausgestattet. Ansonsten hat das Xperia SP eine 8-Megapixel-Kamera auf Exmor-RS-Basis, einen 1,7-GHz-Dual-Core-Snapdragon-S4-Pro-Prozessor und einen 4,6-Zoll-Kapazitiv-Touchscreen mit einer Auflösung von 1280 × 720 Pixeln (HD). Als Besonderheit ist die Unterstützung von LTE (4G) integriert. MHL ist ebenfalls verfügbar.\n\nDas Xperia L (C2105) ist ein weiteres Smartphone, das Sony am 18. März 2013 angekündigt hat. Es sieht ähnlich aus wie die Xperia-Arc-Serie, zu dem auch das Xperia J gehört. Wie dieses besitzt es an der Unterseite eine Benachrichtigungs-LED. Ansonsten hat es ein 4,3-Zoll-Display und eine 8-Megapixel-HDR-Kamera auf Exmor-RS-for-mobile-Basis.\nDas Xperia M ist ein Mittelklasse-Smartphone der Xperia-Serie von Sony (Modellnummer C1904/C1905), das im Juni 2013 angekündigt wurde. Das Äußere dieses Modells ist an das Omnibalance-Design des Xperia Z und an das Sony-typische Arc-Design angelehnt. Es hat eine große Benachrichtigungs-LED am unteren Rand und unterstützt NFC und Bluetooth . Es gibt auch eine Dual-Sim-Variante des Xperia M („Xperia M dual“, Modellnummer C2004/C2005).\n\nDas Xperia M2 ist ein Mittelklasse-Smartphone der Xperia-Serie von Sony. Es wurde am 24. Februar 2014 im Rahmen des GSMA Mobile World Congress zusammen mit dem Xperia Z2 und dem Xperia Z2 Tablet als Nachfolger des Xperia M vorgestellt. Es besitzt das Sony-typische \"OmniBalance\"-Design, das dem des Xperia Z2 stark ähnelt. Wie auch in diesem wird der Akku nun fest verbaut und die Rückseite ist nicht mehr abnehmbar. Die Unterschiede im Vergleich zum Vorgänger betreffen hauptsächlich das veränderte Design, der schnellere Prozessor und das Display, das zwar etwas höher auflöst, aber im Ergebnis eine niedrigere Pixeldichte aufgrund des stärker gewachsenen Displays zur Folge hat. Das Modell unterstützt die Mobilfunkstandards NFC und Bluetooth Low Energy.\nEs existiert in 4 Varianten (D2303, D2305, D2306, D2316) als Xperia M2 mit einem SIM-Kartenschacht und in einer weiteren Variante (D2302) als Xperia M2 Dual mit 2 SIM-Kartenschächten.\n\nAm 19. September 2014 präsentierte Sony das Xperia M2 Aqua. Es ist IP68-zertifiziert, also stärker vor Staub und Wasser geschützt als das M2. Die Farboption Violett wurde durch Kupfer ersetzt. \n\nDas Xperia M4 Aqua ist ein Mittelklasse-Smartphone der Xperia-Serie von Sony. Es wurde am 2. März 2015 im Rahmen des MWC als Nachfolger des Xperia M2 Aqua vorgestellt. Das Äußere dieses Modells ist stark an das Sony-typische Omnibalance-Design des Xperia Z3 angelehnt. Es zeichnet sich außerdem durch seinen Schutz nach IP68 vor Staub und Wasser aus.\nEs existiert in 3 Varianten (E2303, E2306, E2353) als Xperia M4 Aqua mit einem SIM-Kartenschacht und in weiteren 3 Varianten (E2312, E2333, E2363) als Xperia M4 Aqua Dual mit 2 SIM-Kartenschächten.\n\nDas Xperia M5 ist ein Mittelklasse-Smartphone der Xperia-Serie von Sony. Es wurde am 3. August 2015 als Nachfolger des Xperia M4 Aqua vorgestellt. Es bietet das Sony-typische Omnibalance-Design mit Schutz nach IP65/68 vor Staub und Wasser und den neuen Sony IMX 230 Kamerasensor mit Hybrid Autofokus.\nEs existiert in 3 Varianten (E5603, E5606, E5653) als Xperia M5 mit einem SIM-Kartenschacht und in weiteren 3 Varianten (E5633, E5643, E5663) als Xperia M5 Dual mit 2 SIM-Kartenschächten.\n\nDas Gerät wurde für die guten Kameras bei dem Preis gelobt.\n\nDas Xperia Z Ultra (Codename \"Togari\") wurde von Sony am 25. Juni 2013 vorgestellt. Es besitzt einen 6,4″-Bildschirm mit Full-HD-Auflösung, das mit der Triluminos-Technik Farben natürlicher anzeigen soll, unterstützt von dem Bildprozessor \"X-Reality for mobile\". Es war damit zu seiner Zeit eines der größten Phablets. Das Display kann mit einem beliebigen leitfähigen Gegenstand (z. B. einem ganz normalen Bleistift) bedient werden. Es ist außerdem gemäß IP58 vor Wasser und Staub geschützt. Die Vorderseite und Rückseite bestehen aus Glas. Die Modellnummer lautet C6833 für die \"World\"-Variante mit LTE.\n\nDas Xperia Z1 (Codename \"Honami\") wurde von Sony am 4. September 2013 auf der IFA vorgestellt. Es besitzt einen 5-Zoll-Bildschirm mit Full-HD-Auflösung und eine 20,7-Megapixel-Kamera und ist gemäß IP58 vor Staub und Wasser geschützt. Die Vorderseite und Rückseite bestehen aus Glas. Die Modellnummer lautet C6903.\n\nDas Xperia Z1 Compact (in Japan als Z1f bereits Ende Dezember 2013 erschienen) wurde von Sony am 6. Januar 2014 auf der CES in Las Vegas vorgestellt.\n\nEs besitzt ein 4,3-Zoll-Display mit einer Auflösung von 1280 × 720 Pixeln, eine 20,7-Megapixel-Kamera mit EXMOR-Sensor, die in gleicher Form auch im Xperia Z1 zum Einsatz kommt. Nutzer berichten von einem Konstruktionsfehler, welcher die aufgenommen Bilder nachteilig beeinflusst: Durch einen Spalt zwischen der Umrandung der Kameralinse und Kunststoffrückseite des Smartphones scheint der LED-Blitz hindurch. Dadurch landet ein Lichtschein im Farbton der von unten angeleuchteten Rückwand des Z1 Compact vor der Linse und anschließend auf dem fertigen Foto. Dieser Effekt ist vor allem auf bei Dunkelheit aufgenommenen Bildern gut erkennbar.\n\nEs ist ebenso wie das Xperia Z1 gemäß IP58 vor Staub und Wasser geschützt. Die Vorderseite besteht aus Glas, die Rückseite aus Kunststoff. Die Modellnummer lautet D5503.\n\nDas Xperia Z2 (Codename \"Sirius\") wurde von Sony am 24. Februar 2014 auf dem MWC in Barcelona vorgestellt. Es besitzt einen 5,2 Zoll (= 132 mm) großen Bildschirm mit Full-HD-Auflösung und eine 20,7-Megapixel-Kamera, die neben 4K-Videos auch solche mit erhöhter Aufnahmefrequenz ermöglicht. Das Z2 ist gemäß IP58 vor Staub und Wasser geschützt. Die Vorderseite und Rückseite bestehen aus Glas. Die Modellnummer lautet D6503 (Europäische Version mit LTE). Der offizielle Markteinführungspreis in Deutschland lag bei 679 Euro.\n\nDas Xperia Z3 wurde von Sony im September 2014 auf der IFA in Berlin vorgestellt. Es ähnelt mit dem ebenfalls 5,2 Zoll großen Bildschirm mit Full-HD-Auflösung und der 20,7-Megapixel-Kamera stark seinem Vorgänger, dem Xperia Z2. Auch der Snapdragon-801-Prozessor ist derselbe, jedoch mit 2,5 GHz leicht höher getaktet. Bei beiden Geräten gibt es 3 GB an Arbeitsspeicher. Der Akku ist um 100 mAh auf 3100 mAh geschrumpft. Lediglich der Staub- und Wasserschutz nach IP68 gilt statt nur in einem halben Meter Tiefe auch in eineinhalb Meter im Wasser. Die Vorderseite und Rückseite bestehen wieder aus Glas, auch die Seiten sind aus Aluminium, dafür aber etwas runder. Das Gerät ist bei gleich großem Bildschirm etwas kürzer, weniger breit, dünner und daher auch leichter geworden. Zudem gibt es mit schwarz, weiß, Mint und Kupfer etwas andere Farbvarianten als beim Z2 (Schwarz, Weiß und Violett).\n\nDas Xperia Z3 Compact wurde zusammen mit dem Xperia Z3 auf der IFA im September 2014 in Berlin vorgestellt. Analog zum Z1 Compact verwendet das kleinere Gerät fast die gleiche Hardware wie das größere, nur mit einem kleineren Bildschirm und dementsprechend auch einem kleineren Gehäuse. Das Z3 Compact hat einen 4,6-Zoll großen Bildschirm, der mit 1280 × 720 Pixeln auflöst. Der verwendete Snapdragon-801-SoC ist der gleiche wie im Z3. Obwohl der Bildschirm im Vergleich zum Z1 Compact etwas größer ist, ist das Z3 Compact genauso hoch und breit und sogar etwas dünner und leichter.\n\nDas Gerät wurde dafür gelobt, bei einem kompakteren Gehäuse dennoch Oberklasse-Hardware zu bieten.\n\nDas Xperia Z4 wurde am 20. April 2015 als Nachfolger des Xperia Z3 vorgestellt. Das Z3+ welches am 26. Mai 2015 für Deutschland bestätigt wurde, war eine angepasste Version des Z4 und Z4v. Das Z4v wiederum war U.S. exklusiv (Verizon) Der Hauptunterschied lag in der Display Auflösung (1440p (Z4v) vs 1080p (Z3+ / Z4)), und der genutzten Android Software. Das Z3+ / Z4 / Z4v unterscheidet sich hauptsächlich durch den verbauten Snapdragon 810-SoC zu seinem Vorgänger, welcher einen neuen 64-Bit-Achtkern-Prozessor und eine verbesserte Grafikeinheit mit dem Namen „Adreno 430“ beinhaltet, und die 5-Megapixel-Frontkamera welche mit Weitwinkelobjektiv ausgestattet ist. Weitere leichte optische Änderungen sind der stärker glänzende Rahmen und die Stereo-Lautsprecher, die nun am Rand der Vorderseite platziert sind. Die Abmessungen haben sich leicht um 0,1 mm bis 0,4 mm verringert, wodurch das Xperia Z3+ / Z4 bei gleicher Bildschirmgröße im Vergleich zum Xperia Z3 kompakter geworden ist, und auch das Gewicht wurde um 8 Gramm verringert.\n\nDas Xperia Z5 ist ein Flaggschiff-Smartphone der Xperia-Serie von Sony. Es wurde am 2. September 2015 zusammen mit dem Xperia Z5 Compact und dem Xperia Z5 Premium im Rahmen der IFA als Nachfolger des Xperia Z3+ vorgestellt. Die Neuerungen im Vergleich zum Vorgänger kommen eher im Detail zum Vorschein. So zum Beispiel wurde der Rahmen neu gestaltet und auf der Rückseite kommt nun mattes Glas zum Einsatz. Außerdem wird nun der neue Kamerasensor vom Typ Sony IMX 230, welcher schon im Xperia M5 verbaut ist, verwendet. Dieser löst nun mit 23 Megapixeln auf. Bei den Vorgängern vom Xperia Z1 bis zum Xperia Z3+ wird ein 20,7-Megapixel-Sensor verwendet und beim Xperia M5 einer mit 21,5 Megapixeln. Weiterhin setzt es auf einen Hybrid-Autofokus, welcher mit 0,03 Sekunden schneller als das Blinzeln eines menschlichen Auges ist. Laut ersten Berichten wurden die Überhitzungsprobleme des Vorgängers in Verbindung mit dem Qualcomm Snapdragon 810 SoC behoben. Wie seine Vorgänger ist das Xperia Z5 wasser- und staubdicht, nämlich nach IP65/68-Zertifizierung.\nEs existiert in 2 Varianten (E6603, E6653) als Xperia Z5 mit einem SIM-Kartenschacht und in weiteren 2 Varianten (E6633, E6683) als Xperia Z5 Dual mit 2 SIM-Kartenschächten.\n\nDas Xperia Z5 Compact ist ein Flaggschiff-Smartphone der Xperia-Serie von Sony. Es wurde am 2. September 2015 zusammen mit dem Xperia Z5 und dem Xperia Z5 Premium im Rahmen der IFA als Nachfolger des Xperia Z3 Compact vorgestellt. Zu den Neuerungen zählen der neue Snapdragon 810 SoC und der neue Kamerasensor vom Typ Sony IMX 230, welcher nun mit 23 Megapixeln auflöst. Weiterhin setzt es auf einen Hybrid-Autofokus, welcher mit 0,03 Sekunden schneller als das Blinzeln eines menschlichen Auges ist. Auf der Vorderseite wird ein 5,1-Megapixel-Exmor-R-Objektiv verbaut, welches 88°-Weitwinkelaufnahmen ermöglicht. Das erneuerte Design ähnelt nun dem Xperia Z5, der Rahmen ist jedoch aus Plastik und besitzt keine Nylon-Ecken zum Schutz vor Sturzfolgen. Hier wurde der Rahmen überarbeitet und die Rückseite besteht nun aus mattem Glas. Der interne Speicher wurde von 16 auf 32 GB verdoppelt und die Maße wurden leicht vergrößert, wodurch es nun auch 9 Gramm schwerer als das Xperia Z3 Compact ist. Die neuen Farbvarianten sind bunt und ähneln wieder denen des Xperia Z1 Compact. In dem an der Seite angebrachten Einschaltknopf ist auch ein Fingerabdrucksensor zum Entsperren des Mobiltelefons integriert.\nEs existiert in 2 Varianten (E5803, E5823) als Xperia Z5 Compact mit einem SIM-Kartenschacht.\n\nDas Xperia Z5 Premium ist ein Flaggschiff-Smartphone der Xperia-Serie von Sony. Es wurde am 2. September 2015 zusammen mit dem Xperia Z5 und dem Xperia Z5 Compact im Rahmen der IFA vorgestellt. Es ist das weltweit erste Smartphone mit 4K-Display und besitzt eine sehr hohe Pixeldichte von 806 ppi. Einen weiteren Rekord hält das Z5 Premium zusammen mit dem Xperia Z5 und Z5 Compact mit seiner Kamera, welche dank Hybrid-Autofokus in nur 0,03 Sekunden fokussiert, was schneller als das Blinzeln eines menschlichen Auges ist. Es besitzt das geradlinige, Sony-typische Omni-Balance-Design. Seine Rückseite besteht aus Glas und spiegelt sehr stark, was Sony als Spiegeleffekt vermarktet. Zu seinen technischen Spezifikationen gehören der Qualcomm Snapdragon 810 SoC inklusive Adreno 430 Grafik, 3 GB Arbeitsspeicher, 32 GB interner Speicher, ein großer 3700 mAh Akku sowie ein 23-Megapixel-Exmor-RS-Kameramodul mit dem neuen Kamerasensor vom Typ Sony IMX 230.\nEs existiert in einer Variante (E6853) als Xperia Z5 Premium mit einem SIM-Kartenschacht und in weiteren 2 Varianten (E6833 and E6883) als Xperia Z5 Premium Dual mit 2 SIM-Kartenschächten.\n\nDas Xperia X und X Performance wurden auf dem MWC 2016 präsentiert, das Xperia X Compact wurde hingegen einige Monate später auf der IFA 2016 präsentiert. Sony führte mit diesen Smartphones die X-Reihe als Nachfolger der Z-Reihe ein. Das Xperia X Compact wurde zeitgleich mit dem Xperia XZ vorgestellt, trotzdem besteht mehr Ähnlichkeit mit dem Xperia X.\n\nDas Sony Xperia XA ist ein Android-Smartphone von Sony. Das Gerät, Teil der neuen Sony Xperia X-Familie, wurde am 22. Februar 2016 im Rahmen des Mobile World Congress 2016 in Barcelona zusammen mit dem Sony Xperia X und der Sony Xperia X Performance präsentiert.\nDas Sony Xperia XA2 ist ein Smartphone der Mittelklasse, das auf der CES 2018 von Sony als Teil der Xperia- Familie präsentiert wurde.\nEbenfalls vorgestellt wurde das XA2 Ultra, welches sich vom kleineren Bruder XA2 durch den 6-Zoll-Bildschirm, die Dual-Front-Kamera (16 MP + 8 MP, beide mit 120 ° Weitwinkel) und dem 3580 mAh-Akku unterscheidet.\nDas Xperia XZ wurde auf der IFA 2016 mit dem Xperia X Compact präsentiert, das XZs und XZ Premium wurde einige Monate später auf dem MWC 2017 vorgestellt.\n\nDas Xperia XZ Premium sticht mit seinem 4K-Display hervor und seiner spiegelnden Rückseite. Zudem sind das XZs und XZ Premium die ersten Smartphones, deren Kamera Super-Slow-Motion mit 960 FPS unterstützt.\nDas Xperia XZ1 und XZ1 Compact wurden auf der IFA 2017 vorgestellt. Die Geräte unterstützen wie ihre Vorgänger Super-Slow-Motion.\nDas Xperia XZ2 und XZ2 Compact wurden auf dem MWC 2018 vorgestellt. Sony nahm das erste Mal nach Jahren wieder tiefgreifendere Designänderungen vor. Die Displayränder wurden verkleinert, weswegen der Fingerabdrucksensor zum ersten Mal auf die Rückseite angebracht wurde. Die Besonderheit des XZ2 ist das sogenannte \"Dynamic Vibration System\". Der Bass wird durch den Vibrationsmotor verstärkt, dadurch kann man den Bass \"spüren\". Es unterstützt auch als erstes Sony-Smartphone drahtloses Laden.\nZwei Monate später präsentierte Sony das Xperia XZ2 Premium als sein drittes Smartphone mit 4K-Display nach dem XZ Premium und dem Z5 Premium. Dieses besitzt nicht das 18:9-Format der anderen beiden Geräte, stattdessen verwendet es das übliche 16:9-Seitenverhältnis und verfügt über eine Diagonale von 5,8 \". Es zeichnet sich weiterhin aus durch ein Dual-Kamera-System auf der Rückseite, bestehend aus dem 19-Megapixel Sensor des XZ2 und XZ2 Compact und einem 12-Megapixel Schwarz/Weiß-Sensor für verbesserte Qualität bei schlechten Lichtverhältnissen. Ansonsten ist das Design an das des XZ2 angelehnt.\n\nNeben Smartphones vertreibt Sony seit dem 29. August 2012 auch Tablets unter der Marke von „Sony Mobile“.\n\nDas Sony Xperia Tablet S wurde erstmals auf der IFA 2012 in Berlin vorgestellt. Es hat einen 9,4-Zoll-HD-Bildschirm, einen Nvidia-Tegra-3-Prozessor sowie eine 8-Megapixel-Kamera. Außerdem ist es spritzwassergeschützt und lässt sich auch mit nassen Fingern benutzen. Es ist außerdem PlayStation-zertifiziert. Als weitere Besonderheiten lässt es sich in einem Gastmodus betreiben und bietet die Unterstützung zahlreicher „Sony Entertainment“-Dienste wie „Music“ und „Video Unlimited“.\n\nAnfang Oktober 2012 wurde bekannt, dass diverse Baureihen des Xperia Tablet S einen Produktionsfehler aufweisen, aufgrund dessen Feuchtigkeit in das Gerät eindringen und dessen Einsatzfähigkeit beeinträchtigen kann. Sony Japan hat den Verkauf der betreffenden Serien daraufhin eingestellt. Laut Sony handelt es sich hierbei um eine kleine Anzahl an Geräten. Sony bietet außerdem Besitzern der jeweiligen Modelle eine kostenlose Reparatur an.\n\nAuf der CES 2013 hat Sony erstmals das Xperia Tablet Z vorgestellt. Es hat das gleiche „OmniBalance“-Design wie das Xperia Z, außerdem einen 10,1-Zoll-Full-HD-Bildschirm, ein Qualcomm-Snapdragon-S4-Pro-SoC und 2 GB RAM. Zudem ist es gemäß IP5X und IP55/57 vor dem Eindringen von Staub und Wasser (30 min bei 1 m Tiefe) geschützt.\n\nDas Xperia Z2 Tablet (Codename \"Castor\") wurde von Sony am 24. Februar 2014 auf dem GSMA Mobile World Congress vorgestellt. Es hat das gleiche „OmniBalance“-Design wie schon das Xperia Tablet Z und ist ebenfalls gemäß IP5X und IP55/57 vor dem Eindringen von Staub und Wasser (30 min bei 1 m Tiefe) geschützt. Das Z2 Tablet gibt es mit 16 GByte und 32 GByte Flash-Speicher sowie mit 16 GByte und LTE. Die Modellbezeichnung lautet SGP521 für die LTE-Variante.\n\nDas Xperia Z3 Tablet Compact ist das erste 8-Zoll-Tablet von Sony. Es wurde zusammen mit dem Z3 und dem Z3 Compact im Rahmen der IFA 2014 in Berlin vorgestellt. Dementsprechend gleicht die Hardware den Smartphones der Z3-Generation. Wie auch beim Z2 Tablet gibt es eine 16-GB-WLAN-Variante (SGP611), eine 32-GB-WLAN-Variante (SGP612) und zwei 16-GB-LTE-Varianten (SGP621 & SGP641). Ausschließlich die 16-GB-Variante gibt es neben der schwarzen auch in einer weißen Version.\n\nDas Xperia Z4 Tablet wurde von Sony am 2. März 2015 zusammen mit dem Xperia M4 Aqua auf dem GSMA Mobile World Congress vorgestellt. Es hat das gleiche „OmniBalance“-Design wie das Xperia Tablet Z2 Tablet und ist ebenfalls vor dem Eindringen von Staub und Wasser geschützt. Der Standard wurde jedoch von IP 55/58 auf IP 65/68 erhöht, wodurch das Xperia Z4 Tablet bis zu 1,5 m 30 Minuten lang unter Wasser benutzt werden kann. Das Z4 Tablet gibt es mit 32 GB internem Speicher und wahlweise LTE in den Farben Schwarz und Weiß.\n\nSony bietet seinen japanischen Kunden ebenfalls eine Vielzahl an japanischen exklusiven Modellen an. Manche werden als internationale Version auf die anderen Märkte gebracht, manche speziell für den japanischen Markt angepasst. Die japanischen Modelle haben besondere Funktionen wie bargeldlose Bezahlung und andere Dienste. Oft tragen diese Modelle ganz andere Modellbezeichnungen als die globalen Varianten. Diese werden an die japanischen Anbieter wie NTT DoCoMo oder au verteilt und durch diese angeboten.\n\nDas Xperia acro HD ist das japanische Äquivalent zum Sony Xperia acro S. Es gibt unterschiedliche Modellbezeichnungen mit unterschiedlichen Anpassungen. Das Gerät wird sowohl von NTT DoCoMo als auch von au vertrieben.\n\nDas Sony Xperia GX ist die japanische Version des Xperia TX. Es wurde erstmals auf dem „Sony product blog“ am 9. Mai 2012 angekündigt.\n\nDas Xperia SX („SO-05D“) wurde ebenfalls am 9. Mai 2012 zusammen mit dem Xperia GX angekündigt. Auffälligstes Merkmal ist sein Design, welches das Telefon in vier Blöcke aufteilt.\n"}
{"id": "3686283", "url": "https://de.wikipedia.org/wiki?curid=3686283", "title": "ThumbsPlus", "text": "ThumbsPlus\n\nPlus (von \"\") ist eine Software zur Bilderverwaltung unter dem Betriebssystem Microsoft Windows. Das Programm liest Bilder und ähnliche Daten aus ausgewählten Verzeichnissen oder ganzen Laufwerken und Datenträgern ein, erzeugt Miniaturen und speichert Bilddaten, die beispielsweise um Stichwörter ergänzt werden können, in Datenbanken (Access/MSSQL, SQLite, MySQL, Sybase, DB2, PostgreSQL oder Firebird) ab. Diverse Versionen erlauben Mehrfachuser-Netzwerkzugriff auf zentral geführte Datenbanken. Auch die wichtigsten Grundfunktionen zur Bildverarbeitung stehen zur Verfügung und können automatisiert angewandt werden.\n\nDas Programm unterstützt neben den gängigen Bilddatenformaten (Pixelformat) wie TIFF, GIF, JPG, BMP und PNG auch Vektorgrafiken wie WPG und CDR, Videoformate wie MPG, MOV, MP4, WMV usw., weitere Metadateien sowie Fonts und viele seltenere Formate anderer Betriebssysteme sowie die meisten Rohdatenformate, diese allerdings nur in der Professional-Version. Letztere ermöglicht auch das Einbinden von Photoshop-Plug-ins.\nDOC-, DOCX-, XLS-, XLSX-, WPD-, PDF- Dateien und viele mehr können ebenfalls in Form der ersten Seite der Datei in diesem Bild-, Grafik-, Video- und Dokument- Verwaltungsprogramm angezeigt werden.\n\nThumbsPlus liest bzw. schreibt sowohl Exif- als auch IPTC-Daten. Insbesondere die IPTC-Felder einzelner Bilder als auch großen Mengen gleichzeitig können mit ThumbsPlus komfortabel editiert werden.\n\nDas Programm hat besondere Stärken in der Ähnlichkeitserkennung von gleichartigen Bildern. Es ist möglich, per Batch sehr große Bilddatenbestände hinsichtlich prozentual einstellbarer Ähnlichkeit zu analysieren. Diese Fähigkeit wird u. a. auch von zahlreichen Ermittlungsbehörden eingesetzt. Für die Verfolgung von Kinderpornografie stellt der Hersteller der deutschen Ausgabe den Behörden kostenlos eine Polizeiversion zur Verfügung.\n\nThumbsPlus 8 wurde für 2007 angekündigt, eine Beta-Version erschien jedoch erst im Januar 2009. Version 9 wurde auf Unicode umgestellt, um internationale Zeichenvorräte zu bieten. Infolgedessen werden ältere Windows-Versionen (95/98/ME) jedoch nicht mehr unterstützt. Viele Komponenten wurden für die 64-Bit-Windows überarbeitet, um eine schnellere Bildverarbeitung und mehr Speichereinsatz zu ermöglichen.\n\nEine deutschsprachige Version der Software wurde bis zur Version 7 angeboten; folgende Versionen sind ausschließlich in Englisch.\n\n"}
{"id": "3686384", "url": "https://de.wikipedia.org/wiki?curid=3686384", "title": "CDBurnerXP", "text": "CDBurnerXP\n\nCDBurnerXP ist ein kostenloses Brennprogramm. Es liegt in der vierten Version vor und läuft unter Windows ab Version 2000. Das Programm wurde ursprünglich von Stefan und Fredrik Haglund entwickelt. Seit dem Jahr 2007 ist die Canneverbe Limited für die Weiterentwicklung verantwortlich.\n\nCDBurnerXP verarbeitet sowohl CD-R- und CD-RW-Medien als auch DVD±R, DVD±RW, DVD-RAM sowie Blu-ray Disc (BD-R) und HD DVD inklusive Double-Layer-Datenträger. Man kann ISO-Abbilder erstellen und diese brennen.\n\nBeim Erstellen von Audio-CDs können auch komprimierte Audiodateien, wie z. B. Ogg Vorbis, direkt verwendet werden und müssen nicht zuvor manuell dekomprimiert werden. In den Dateien vorhandene Informationen über Titel, Interpret etc. können zum Erstellen eines Covers für die CD verwendet werden.\n\nDas Programm umgeht einen eventuell vorhandenen Kopierschutz nicht.\n\nAb Version 4 benötigt CDBurnerXP das .NET-Framework. Der Quelltext ist nicht frei verfügbar, da proprietäre Programmbibliotheken verwendet werden.\n"}
{"id": "3688234", "url": "https://de.wikipedia.org/wiki?curid=3688234", "title": "HAL (Software)", "text": "HAL (Software)\n\nHAL (kurz für \"Hardware Abstraction Layer\") ist eine freie Software, die es Anwendungen ermöglicht, Informationen über verfügbare Hardware abzurufen und mit ihr zu kommunizieren. Mit HAL können Anwendungen auf das Anschließen und Entfernen von Hardware reagieren (Plug and Play). HAL arbeitet als Daemon und benutzt D-Bus, um Informationen an Anwendungssoftware weiterzugeben. HAL verfügt über eine eigene Datenbank, die detaillierte Beschreibungen von Hardwarekomponenten enthält. So kann Anwendungssoftware beispielsweise in die Lage versetzt werden, eine Digitalkamera als solche anzusprechen, auch wenn sie sich am Universal Serial Bus nur als Datenspeicher zu erkennen gibt.\n\nDie erste Version dieser Software wurde im September 2003 veröffentlicht.\n\nDer deutsche Begriff dafür wäre \"Hardwareabstraktionsschicht\". Im Mai 2008 bekannte der Verfasser der Spezifikationen indessen, dass es sich \"nicht\" um eine Abstraktionsschicht handelt.\n\nHAL wird inzwischen wieder aus vielen Linux-Distributionen entfernt, da es „ein großes, monolithisches unwartbares Durcheinander geworden ist“. Basierend auf den Erfahrungen des HAL-Projekts war ursprünglich DeviceKit als Ersatz geplant, inzwischen wurde dessen Funktionalität in eine Sammlung von Einzelkomponenten wie udev, UPower, RFkill und andere aufgeteilt.\n\n"}
{"id": "3689305", "url": "https://de.wikipedia.org/wiki?curid=3689305", "title": "Photomatix", "text": "Photomatix\n\nPhotomatix ist eine Software zum Erstellen von HDR-Bildern. Sie wurde von \"HDRsoft\" für die Betriebssysteme Mac OS X und Windows (32- und 64-Bit-Version) entwickelt.\n\nSeit Anfang 2011 wird das Produkt in zwei Versionen angeboten. Neben der bis dahin vertriebenen \"Normalversion\", die sich nun Photomatix PRO 5 nennt, wird eine funktionsreduzierte Software mit der Bezeichnung Photomatix Essentials 3.x (früher unter der Bezeichnung \"Light\") zu einem deutlich reduzierten Preis angeboten. Die Essentials-Version ist seit 8. Januar 2012 auch auf Deutsch erhältlich.\n\nEine zeitlich unbefristete und komplett funktionsfähige Testversion steht auf der Website zur Verfügung. Es wird allerdings ein Wasserzeichen in die Bilder eingebracht, solange sie nicht lizenziert ist.\n\nPhotomatix bis Version 3.x verwendete im Gegensatz zu Software wie HDR Shop oder Photosphere einen Tone-Mapping-Operator, der für naturgetreue Ergebnisse schwer konfigurierbar ist. Es gibt Kritik, dass dies den Benutzer dazu verleite, Tone Mapping als künstlerisches Effektmittel zu missbrauchen.\n\nAb Version 4 ist es möglich, \"Photomatix\" die Anzahl der zu nutzenden Prozessorkerne zuzuordnen. Zudem kann nach dem Einlesen der Vorlagebilder über „Preset Thumbnails“ unter folgenden 12 unterschiedlichen HDR-Erstellungsmodi gewählt werden:\nZusätzlich können eigene Voreinstellungen (Presets) erstellt werden.\n\nMit der Version 4.2 wurde im April 2012 die Auswahl der voreingestellten Operatoren deutlich erhöht wurde und der Import neuer RAW-Formate erheblich erweitert (z. B. Canon S100, G1 X, Nikon D4, D800, Sony NEX-7, Panasonic GX1).\n\nVersion 5 erschien (bisher nur in englischer Sprache) am 20. November 2013. Die deutsche Version ist am 3. März 2014 veröffentlicht worden.\n\n\n"}
{"id": "3691292", "url": "https://de.wikipedia.org/wiki?curid=3691292", "title": "PackageKit", "text": "PackageKit\n\nPackageKit ist eine freie Software, die eine allgemeine Schnittstelle für unterschiedliche Paketverwaltungen zur Verfügung stellt. Es verwendet PolicyKit zur Überprüfung von Berechtigungen.\n\n2007 begann Richard Hughes PackageKit zu entwickeln und stellte es auf seinem Blog vor. Nun wird es von einem kleinen Team weiterentwickelt. Fedora 9 und Foresight Linux 1.4.1 nutzen PackageKit bereits standardmäßig. Als erste dpkg-basierte Distribution setzt Kubuntu ab Version 9.04 („Jaunty Jackalope“) statt auf Adept auf PackageKit.\n\nDer Daemon codice_1 stellt eine D-Bus-Schnittstelle für die Paketverwaltung zur Verfügung. Programme können auch über die Bibliothek codice_2 mit PackageKit kommunizieren.\n\nViele verbreitete Paketverwaltungen können als Backends genutzt werden. Es werden Frontends für GNOME (GNOME Software) und KDE (Apper) angeboten.\n\nAb Version 0.4.0 enthält PackageKit eine Bash-Erweiterung ähnlich dem Debian-Programm codice_3. Wenn ein Befehl im System nicht gefunden wurde, aber nachinstalliert werden kann, wird der Benutzer gefragt, ob er dieses installieren möchte. Zudem werden Tippfehler erkannt.\n"}
{"id": "3694449", "url": "https://de.wikipedia.org/wiki?curid=3694449", "title": "Fluxbuntu", "text": "Fluxbuntu\n\nFluxbuntu war ein inoffizielles Derivat der Linux-Distribution Ubuntu.\n\nFluxbuntu wurde speziell für ältere Rechner entworfen. Um diesem Anspruch gerecht zu werden, wurden einige Änderungen im Vergleich zu den offiziellen Ubuntu-Derivaten, wie Kubuntu oder Xubuntu, vorgenommen. Die wichtigste ist die Verwendung des sparsamen Fenstermanagers Fluxbox mit dem ROX-Filer als Dateimanager. Das Fluxbox-Menü ist dynamisch mit den via Advanced Packaging Tool installierten Programmen verknüpft. Des Weiteren wurden einige der mitgelieferten Programme durch sparsamere Alternativen ersetzt. Als Textverarbeitung kommt AbiWord (anstatt OpenOffice.org), als Browser Kazehakase (anstatt Mozilla Firefox) zum Einsatz.\n\nDie letzte stabile Version von Fluxbuntu basiert auf Ubuntu 7.10 „Gutsy Gibbon“. Des Weiteren existiert eine Testversion, die auf Ubuntu 8.10 basiert, sowie eine experimentelle Version die Ubuntu 9.04 zur Grundlage hat. Alle drei Versionen werden bereits seit Jahren nicht mehr gepflegt.\n\n\n"}
{"id": "3694611", "url": "https://de.wikipedia.org/wiki?curid=3694611", "title": "Oekaki", "text": "Oekaki\n\nOekaki (jap. , \"Oekaki\") ist ein japanischer Ausdruck um die Tätigkeit des Zeichnens zu beschreiben, er bedeutet auf Deutsch in etwa \"Skizze\" oder \"Gekritzel\".\n\nOekaki-Internetforen erlauben es den Benutzern mit den auf dem Server gespeicherten Anwendungen Zeichnungen zu erstellen und im angebundenen Forum zu veröffentlichen. Die Benutzer laden dabei nicht bereits fertige Zeichnungen hoch, sondern erstellen diese \"live\" in ihrem Webbrowser mithilfe einer Maus, einem Grafiktablett oder mittels eines Touchscreen. Es gibt jedoch auch Oekaki, die den Upload fertiger Zeichnungen erlauben, dies ist jedoch mit gewissen Einschränkungen versehen wie beispielsweise eine bestimmte Anzahl Uploads pro Woche.\n\nDie unterliegende Technik kann dabei ein Java-Applet oder ActiveX sein. Einige Systeme bieten dabei einen Funktionsumfang mit dem sich Zeichnungen auf professionellem Niveau anfertigen lassen. Die jeweilige Bildgröße beschränkt sich dabei jedoch in Höhe und Breite jeweils nur auf einige hundert Pixel. \n\nSofern diese Funktion vom jeweiligen System unterstützt wird, können die Benutzer des entsprechenden Forums der Entstehung der Zeichnung als Animation zusehen. Einige bieten ebenfalls eine Fortsetzen-Funktion an, die es dem Künstler erlaubt die Arbeit an seiner Zeichnung zu unterbrechen und zu einem späteren Zeitpunkt an derselben Stelle fortzufahren. Darüber hinaus bieten diverse Systeme ebenfalls die Benutzung von Ebenen an.\n\nOekaki ist in der Anime-Subkultur sehr beliebt, da es ohne den Einsatz von Bildbearbeitungssoftware die Anfertigung und den Austausch von Fanart erlaubt.\n\n\nDarüber hinaus gibt es noch andere Applets wie etwa \"PictureBBS\" und \"BBSPainter\" diese sind jedoch nicht weit verbreitet. \"Lascaux Sketch\", ein Applet welches sich unter 2draw.net finden lässt, wird nicht öffentlich verbreitet aber stellt wahrscheinlich den mächtigsten Funktionsumfang aller derzeit zur Verfügung stehenden Systeme bereit.\n\n\n"}
{"id": "3696011", "url": "https://de.wikipedia.org/wiki?curid=3696011", "title": "EJay", "text": "EJay\n\neJay ist der Titel einer Musiksoftware-Produktreihe, unter welcher primär einfach zu bedienende Sequenzer für Microsoft Windows vertrieben werden, die bereits eine Sammlung an Samples mit sich bringen . Neben dieser Produktlinie wurde außerdem noch andere Software produziert, die u. a. zur Audiorestauration dient. Die Vermarktung begann im Jahre 1997 als das erste Produkt „Dance eJay“ erschien.\n\nIm Mai 2009 wurde auf der offiziellen Facebook-Seite von eJay verkündet, dass der bisherige Besitzer \"Empire Interactive Europe Limited\" die Rechte an „eine andere Firma“ verloren habe und die Zukunft von eJay ungewiss sei. Am 1. August 2009 wurde die Internetpräsenz von eJay, als auch die von Empire Interactive entfernt.\n\nSeit Ende 2010 vertreibt das Unternehmen HBV einige der eJay-Produkte mit dem Zusatztitel „Reloaded“. Die offizielle URL \"www.ejay.com\" wurde von dem Unternehmen ebenfalls reaktiviert.\n\nMit dem Programm war es möglich, eigene Musik zu komponieren und zu mischen sowie zu verwalten, hauptsächlich je nach Version, Musikrichtungen wie Techno, Rave, Dance, Hip-Hop, aber auch Rock- und Popmusik. Sie sollte hauptsächlich die Jugendkultur ansprechen.\n\nBenutzer dieser Software konnten ihre eigenen Tracks auf der Website \"www.ejay-uk.com\" veröffentlichen und sich darüber hinaus mit anderen Anwendern austauschen.\n\nDas Programm bestand aus einer bestimmten Anzahl von Tonspuren, auf denen schon fertiggestellte Samples eingesetzt werden können. Die Samples ähnelten einzelnen Bausteinen, die per Mausklick je nach Kategorie entweder einen kurzen Rhythmus, ein Bassriff, Effekte, eine Melodie oder irgendwelchen anderen kurzen Instrumentalisierungen oder Sounds abspielten.\n\nDie Software gilt als besonders benutzerfreundlich und leicht bedienbar und wird deshalb öfters Anfängern und Laien empfohlen.\n\nDie Samples sind lizenzfrei und werden im Sample-Archiv im Internet kostenlos zur Verfügung gestellt. Wöchentlich werden 20 neue Sounds hinzugefügt und können in die Software eingebunden werden. Die Version \"Dance eJay\" enthält bis zu 5000 lizenzfreie Samples, die im so genannten \"Wave-Editor\" bearbeitet und verändert werden können.\n\nEigene Samples im \"WAV-Format\" lassen sich importieren oder mit der eingebauten \"eJay Recording Box\" aufzeichnen.\n\nDie Musik kann auf Rechner exportiert und auf eine CD kopiert werden.\n\n"}
{"id": "3702045", "url": "https://de.wikipedia.org/wiki?curid=3702045", "title": "EPASS-HELENA", "text": "EPASS-HELENA\n\nEPASS-HELENA ist eine Planungs- und Beratungssoftware zur energetischen Bewertung von Gebäuden für Energieausweise, EnEV-Nachweise, Energieberatungen, Wirtschaftlichkeitsberechnungen, für Wohn- und Nichtwohngebäuden gem. Energieeinsparverordnung (EnEV), DIN V 18599, DIN 4108-6, DIN 4701-10/12, EN 832 und EN ISO 6946 für verbrauchs- und bedarfsorientierte Berechnungen.\n\nDie Software wurde im Jahre 2001 in Kooperation mit der Universität Kassel, Univ.-Prof. Dr.-Ing. Gerd Hauser, dem Fraunhofer-Institut für Bauphysik Holzkirchen, Saint-Gobain ISOVER G+H AG und dem ebz süd entwickelt und trat damit die Nachfolge der bereits seit Anfang der 1980er Jahre von Professor Gerd Hauser entwickelten Software EPASS (Version 1 bis 3) an, mit der bis dahin Berechnungen gem. Wärmeschutzverordnung (WsVO) möglich waren.\n\nDas Programm gilt als Referenz-Implementierung und wurde schon bei der Konzeption der EnEV 2002 zu deren Validierung verwendet. Die Software ist im akademischen Umfeld und an Universitäten weit verbreitet. Für die Nutzung im Bereich Forschung- und Lehre, so wie für Studenten sind kostenlose Lizenzen erhältlich.\n\nDer Hersteller ist an Forschungsprojekten zur Fortschreibung und Weiterentwicklung der zugrunde liegenden Normen beteiligt:\n\nDie Software ermöglicht mehrzonige Berechnung gem. EN 832 und DIN V 18599.\n\nBis zum Inkrafttreten der EnEV 2002 waren Berechnungen des baulichen Wärmeschutzes grundsätzlich noch auf einem Komplexitätsniveau angesiedelt, welches Handrechnungen möglich machte. Auch waren einfache Kalkulationen mit Excel Tabellen bis dahin sehr beliebt. Parallel zu EPASS-HELENA wurde ebenfalls von der Universität Kassel eine freie, excel-basierte Lösung entwickelt, die zwar weite Verbreitung fand, sich aber nicht durchsetzen konnte. Der komplexere Ansatz der EnEV ab 2002 unter Berücksichtigung der haustechnischen Installationen in Gebäuden nach DIN 4701 machte derartige Ansätze in der Regel unwirtschaftlich. Weiter kam hinzu, dass zunehmend freie Energieberatungen durchgeführt und öffentlich gefördert wurden (beispielsweise durch das Bundesamt für Wirtschaft und Ausfuhrkontrolle, BAFA) oder die KfW Bankengruppe (KfW) und immer öfter auch eine komplexe bauwirtschaftliche Analyse und der Vergleich einer Vielzahl möglicher Sanierungsvarianten für ein Gebäude nötig wurden. Das Programm integriert mit dem Druckmodul der Deutschen Energie-Agentur (dena) die Möglichkeit zum Erstellen des öffentlich-rechtlichen Energieausweises für Gebäude.\n\nDie Software ist Vorreiter auf dem Gebiet der energetischen Gebäudeplanung. Mittlerweile gibt es eine Vielzahl weiterer Hersteller, die ähnliche Werkzeuge mit vergleichbarem oder teilweise sogar noch größerem Funktionsumfang anbieten oder in andere Planungssysteme (wie zum Beispiel TGA-Software) integrieren. Eine Untersuchung zum Vergleich verschiedener Software-Lösungen im Auftrag des Bundesamtes für Bauwesen und Raumordnung (BBR) wird gegenwärtig vom IAIB, Professor Klaus Fehlauer, durchgeführt.\n\n\n"}
{"id": "3702473", "url": "https://de.wikipedia.org/wiki?curid=3702473", "title": "Btrfs", "text": "Btrfs\n\nDas Btrfs (\"B-tree FS\"; : []) ist ein Copy-On-Write-Dateisystem, das von der Oracle Corporation seit 2007 als freie Software unter der GNU General Public License (GPL) für das Betriebssystem Linux und seit 2016, mit der Bezeichnung WinBtrfs, plattformübergreifend für Windows (ab Windows 7) sowie ReactOS entwickelt wird.\n\nBtrfs wird seit einiger Zeit als Nachfolger des bislang im Linux-Umfeld vorherrschenden ext4-Dateisystem gehandelt, da dieses nur einen Teil der Beschränkungen von ext3 wie Dateigröße und Gesamtdateisystemgröße aufgehoben hat. So setzte Andrew Morton, einer der prominentesten Linux-Kernel-Entwickler, in 2008 \"auf längere Sicht auf Btrfs\". Stand 2018 hat Btrfs diese Nachfolge jedoch noch nicht angetreten.\n\nBtrfs weist zahlreiche Gemeinsamkeiten mit ZFS auf und wird deswegen als Linux-Analogon zu ZFS beschrieben. ZFS wurde zwar schon sieben Jahre früher vom mittlerweile selben Hersteller (Sun Microsystems, aufgegangen in Oracle) als ultimatives Dateisystem entworfen, war wegen seines Lizenzstatus jedoch für die Verwendung mit Linux ungeeignet. Beide haben integriertes RAID, Volume-Management, prüfsummenbasierten Schutz vor Datenübertragungsfehlern und nutzen Copy-On-Write, ein Verfahren, bei dem eine Kopie erst dann „real“ angefertigt wird, sobald sie von einem der Beteiligten verändert wird. Solange alle Beteiligten ihre Kopie nicht verändert haben, genügt es, das Original ein einziges Mal zu speichern. Die Kopie erfolgt also zunächst „virtuell“ und wird erst bei einer ersten Benutzung verzögert angelegt. Das in das Dateisystem integrierte RAID-Subsystem bietet gegenüber klassischen Hardware- oder Software-RAID-Implementierungen den Vorteil, dass zwischen belegten und freien Datenblöcken unterschieden werden kann und somit bei der Rekonstruktion eines gespiegelten RAID-Volumens nur belegter Plattenplatz gespiegelt werden muss. Hieraus resultiert im Schadensfall, besonders bei wenig gefüllten Dateisystemen, eine enorme Zeitersparnis. Die RAID Funktionalität wird zudem, im Gegensatz zu klassischen RAID-Verfahren, mit Hilfe von größeren\nDatenblöcken organisiert. Es erfolgt dann beispielsweise im RAID1 keine Spiegelung der Datenträger, sondern es wird sichergestellt, dass jeder Datenbereich auf mindestens zwei Datenträgern abgelegt wird. So wird es möglich einen RAID 1 aus einer ungeraden Anzahl von Datenträgern unterschiedlicher Kapazität, unter voller Ausnutzung derer Kapazität, zu bilden.\n\nWeiterhin baut Btrfs mit der B-Baum-Struktur auf einem zentralen Konzept aus XFS auf. Es ist somit auch mit dem nicht mehr weiterentwickelten Reiser4 verwandt, zu dem es als adäquate Alternative gesehen wird.\n\nBtrfs liefert das Programm codice_1 mit, mit dem bestehende ext3- und ext4- Dateisysteme in ein Btrfs-Dateisystem konvertiert werden können. Die Konvertierung ist reversibel.\n\nBtrfs soll vor allem auch Funktionen bieten, die es vom derzeitigen Linux-Standard ext3/ext4, aber auch von anderen Dateisystemen wie XFS oder JFS abheben, hierunter fallen:\n\nDie Kernstruktur von Btrfs – die Copy-on-write-B-Baum-Struktur – wurde ursprünglich von dem IBM-Forscher Ohad Rodeh im Rahmen einer Präsentation bei der USENIX 2007 vorgeschlagen. Rodeh schlug auch das Hinzufügen von Referenzierungszählern für Speicherblöcke und bestimmte Lockerungen der Balancing-Algorithmen normaler B-Bäume vor, die die B-Bäume für Hochleistungsspeicherlösungen mit Copy-On-Write-Schnappschüssen tauglich machen und dabei gute Nebenläufigkeit bewahren.\n\nChris Mason, damals ein ReiserFS-Entwickler bei SUSE, wurde noch im selben Jahr von Oracle eingestellt und begann dort seine Arbeit an einem neuen Dateisystem, das fast ausschließlich solche B-Bäume verwendet – nicht nur für Meta- und Nutzdateien, sondern auch rekursiv zur Verfolgung der Speicherzuteilung der Bäume selber. Damit können sämtliche Operationen durch dieselben Routinen abgewickelt werden.\n\nAm 9. Januar 2009 wurde Btrfs erstmals in den Linux-Kernel 2.6.29 aufgenommen. In einigen Linux-Distributionen steht das Dateisystem bereits offiziell bei der Installation zur Auswahl.\n\nFür den experimentellen Einsatz wurde das Dateisystem erstmals unter OpenSUSE 11.3 unterstützt sowie unter Oracle Linux Release 2 und es existieren Pakete für die Distributionen Gentoo, Fedora, RHEL, Arch Linux, Debian und Ubuntu.\n\nBtrfs ist das Standard-Dateisystem von MeeGo und sollte seit mehreren Versionen auch in Fedora als Standard-Dateisystem verwendet werden. Dies wurde jedoch immer wieder verworfen, weil einige Tools um das Dateisystem zu administrieren immer noch nicht ausgereift sind.\n\nSUSE Linux Enterprise Server 12 (SLES 12) verwendet Btrfs als Standard-Dateisystem. openSUSE setzt ab Version 13.2 auf Btrfs.\n\nAls optionales Dateisystem wird Btrfs von nahezu allen Distributionen unterstützt (Stand: ).\n\n2017 gab Red Hat bekannt, Btrfs nicht mehr zu unterstützen und langfristig zu entfernen.\n\nDer Dateisystemtreiber WinBtrfs ermöglicht die Verwendung des Dateisystems unter Windows und ReactOS.\n<br>\n\nRed Hat beauftragte im zweiten Quartal 2010 Edward Shishkin, einen der ursprünglichen Reiser4-Entwickler, mit einem Codereview. Shishkins Schluss war, dass das Design fehlerhaft ist, da dem ursprünglichen Algorithmus in Kernpunkten nicht gefolgt wird. Die Designfehler führen dazu, dass in speziellen Fällen der Plattenplatz ausgehen kann, obwohl genügend Platz vorhanden ist. Die Btrfs-Entwickler widersprachen der Behauptung, dass es sich um einen Designfehler handele. Sie bezeichneten es vielmehr als Implementierungsfehler der bereits behoben sei. Im August 2017 kündigte Red Hat an, langfristig die Unterstützung von Btrfs in RHEL einzustellen. Anlass dafür seien Probleme des Dateisystems im Zusammenspiel mit Docker, sowie Anwenderbeschwerden, die nach Aussage von Mitentwicklern häufig aufwändige manuelle Korrekturen im Dateisystem erforderten. Die aktuelle Version von RHEL verwendet allerdings den Linux Kernel 3.10, welcher von den btrfs Entwicklern nicht empfohlen wird und einen Stand von btrfs von 2013 widerspiegelt. Ein Grund könnte auch sein, dass RedHat Konkurrenz im eigenen Haus vermeiden möchte, welcher durch den Zukauf von Permabit entstanden wäre. Um an Features wie Daten-Deduplizierung zu gelangen, müssen Kunden von Red Hat so zu einer kommerziellen, kostenpflichtigen Erweiterung greifen.\n\n\n\n"}
{"id": "3702827", "url": "https://de.wikipedia.org/wiki?curid=3702827", "title": "Macintosh Application Environment", "text": "Macintosh Application Environment\n\nDas Macintosh Application Environment (MAE) war ein Programmpaket, welches 1994 von Apple Computer, Inc. herausgebracht wurde, um auf ausgewählten Unix-Rechnern Macintosh-Programme laufen zu lassen.\n\nMAE verwendete das X Window System in der Version X11 R5, um einen vollständig simulierten Macintosh-Rechner mit 68k-Prozessor (genau: Motorola 68040LC) zur Ausführung von Mac-OS-Programmen zur Verfügung zu stellen. Version 2.0 beruhte auf System 7.1, die letzte Version, MAE 3.0, war kompatibel zu System 7.5.3. Der Emulator lief in einem einzigen X11-Fenster, d. h., es gab keine Möglichkeit, einzelne Mac-OS-Fenster in einzelnen X11-Fenstern darzustellen („“).\n\nZum Datenaustausch zwischen Emulator (MAE) und Wirts-System (X11) standen (virtuelles) Netzwerk, gemeinsam genutzte Dateisysteme und die Zwischenablage zur Auswahl. Druckerausgabe (auf Postscript-Drucker), Tonausgabe und Grafikausgabe über Quicktime wurden unterstützt. Der Zugriff auf Disketten und CD-ROMs war vom emulierten Mac OS aus möglich.\n\nLaut „MAE 3.0 “ verwendet der zugrundeliegende 68k-Emulator dynamische Kompilierung.\n\nAm 14. Mai 1998 wurde das Programm eingestellt.\n\nMAE 3.0 benötigte entweder:\n\n\n"}
{"id": "3707758", "url": "https://de.wikipedia.org/wiki?curid=3707758", "title": "RadioTux", "text": "RadioTux\n\nRadioTux ist ein 2001 gegründetes Podcast-Projekt mit den thematischen Schwerpunkten Linux, Open Source und Netzkultur. Es produziert in regelmäßigen Abständen verschiedene Formate und hat sich zu einem gefragten Medienpartner für OpenSource Events entwickelt.\n\nMonatlich erscheint eine Magazinsendung von RadioTux. Diese beschäftigt sich entweder mit einem speziellen Thema oder bietet eine Vielzahl von verschiedenen Beiträgen, die durch Musik getrennt werden. Spezialsendungen gab es bisher unter anderem vom Zarafa Summer Camp 2010.\n\nAufgrund von Personal- und Ressourcenmangel wurden viele Formate Ende 2011 eingestellt oder in eigenständige Projekte ausgelagert, um sich mehr auf die monatliche Magazinsendung zu konzentrieren.\nDiese wöchentliche Talksendung aus dem Studio des HoRadS, welche auch live übertragen wurden, beschäftigte sich mit Neuigkeiten rund um Linux, OpenSource und Netzkultur. Als Trenner zwischen den verschiedenen thematischen Blöcken diente freie Musik.\n\nZu den Gästen zählten bisher unter anderem der Vizepräsident des KDE e.V. Frank Karlitschek oder Dirk Deimeke, Vorsitzender des ubuntu Deutschland e.V.\n\nDas Format wurde zwar bei RadioTux eingestellt, wird aber unter dem Namen \"Binärgewitter\" weitergeführt.\n\nRadioTux war auf nahezu allen großen Messen und Konferenzen in Deutschland vertreten. So zählten zum Beispiel die CEBIT, die Chemnitzer Linux-Tage oder der LinuxTag in Berlin zu festen Größen im Sendungskalender. Als Interviewpartner durfte das Projekt neben den Verantwortlichen hinter KDE und Gnome und etlichen Linux-Distributionen auch die Stars der Linux Szene wie den Ubuntu Gründer Mark Shuttleworth begrüßen.\n\nRadioTux diente als Plattform für die wöchentlichen Nachrichtensendungen von den Communitys von Ubuntu, Fedora und openSUSE.\n\nDer Techview Podcast beschäftigt sich mit verschiedenen Technologiethemen rund um GNU/Linux, neue Hard- und Software, netzpolitischen Themen und BeOS bzw. Haiku (Betriebssystem). Auch dieses Format wird seit 2012 von Leszek Leszner eigenständig weitergeführt.\n\nDer P3Cast, ehemals Playing Penguin Podcast, beschäftigt sich mit Themen aus der Linux-Spiele-Szene. Dabei wird kein Unterschied zwischen freien und kommerziellen Spielen gemacht. Auch dieses Format wird seit 2012 eigenständig weitergeführt.\n\nAlle entstandenen Beiträge werden auf verschiedenen Plattformen wie iTunes oder via Web-Feed als Podcasts veröffentlicht. Hierbei stehen verschiedene Varianten der Beiträge zur Verfügung. So werden Sendungen im MP3-Format sowie im freien Ogg/Vorbis-Format angeboten. Des Weiteren können Hörer zwischen Versionen mit als auch ohne (freier) Musik wählen.\n\nMittlerweile stehen mehrere hunderte Podcasts im Archiv zur Verfügung.\n\n\n\n"}
{"id": "3714279", "url": "https://de.wikipedia.org/wiki?curid=3714279", "title": "PowerDirector", "text": "PowerDirector\n\nPowerDirector ist eine kommerzielle Videoschnittsoftware und Authoring-Software, die unter dem Betriebssystem Windows läuft. Ausgerichtet ist die Software auf Import/Aufnahme und Korrektur von Videos und Fotos, sowie Videobearbeitung, Authoring und dem Brennen von DVDs und Blu-ray Discs. High-Definition Formate (wie AVCHD, Blu-ray Discs) werden in der Ultra-Version durchgängig unterstützt.\n\nMit folgenden Features lassen sich Videos mit PowerDirector bearbeiten:\n\nAnwender können mit PowerDirector eigene Bild-im-Bild-Objekte und DVD-Menüvorlagen erstellen. Diese können auf Director Zone geladen und Benutzern anderer CyberLink Anwendungen zur Verfügung gestellt werden. Von dort lassen sich ebenfalls Vorlagen Anderer herunterladen und bewerten.\n\n\n"}
{"id": "3714883", "url": "https://de.wikipedia.org/wiki?curid=3714883", "title": "FastTrack Schedule", "text": "FastTrack Schedule\n\nFastTrack Schedule ist eine Projektmanagementsoftware. Sie wurde von AEC Software entwickelt und wird im deutschsprachigen Raum von der ComputerWorks GmbH vertrieben. Das Programm unterstützt Projektmanager bei der Planung und Durchführung von Projekten. Die derzeit aktuelle Version ist FastTrack Schedule 10, es wird sowohl Windows als auch macOS unterstützt.\n\n1985 begann AEC Software mit der Entwicklung der Projektmanagementsoftware \"FastTrack Schedule\". Seither wurde sie kontinuierlich weiterentwickelt.\n\nDie Projektmanagementsoftware läuft unter Mac OS X und Windows, liest und schreibt MS-Project-Dateien und ermöglicht den Datenaustausch mit gängigen Lösungen und Formaten wie Mindjets MindManager, iCal, iCalender, MS Excel und HTML. Dies ist besonders für Rechnerumgebungen mit gemischten Windows/MAC-Systemen von Vorteil.\n\nDas Programm ermöglicht dem Anwender Vorgangsbalken frei mit der Maus einzutragen und den Einsatz von Bildern. Außerdem ermöglicht es den Einsatz von frei definierbaren Textfeldern und farbigen Vorgangsbalken und Legenden.\nDie eigene Makrosprache „FastSteps“ erlaubt mehrere Menübefehle automatisch auszuführen. So kann zum Beispiel ein Kostenreport mit einem Mausklick generiert und ausgedruckt werden.\n\nMit der Konsolidierungsfunktion können Teilprojekte in einen Masterplan konsolidiert werden. Änderungen in den Teilprojektplänen werden über eine Update-Funktion automatisch in den Masterplan übernommen.\n"}
{"id": "3716967", "url": "https://de.wikipedia.org/wiki?curid=3716967", "title": "Kontakte (App)", "text": "Kontakte (App)\n\nKontakte bzw. Adressbuch (vor OS X Mountain Lion) ist ein Programm von Apple zur Verwaltung von Telefonnummern, Adressen und anderen Kontaktdaten. Es ist Bestandteil der zwei Betriebssysteme OS X und iOS.\n\nBereits bei NeXTStep war und seit Erscheinen der ersten Version von Mac OS X ist das Programm fester Bestandteil des Betriebssystems – zunächst als \"Adressbuch\" (Address Book.app) und ab OS X 10.8 als \"Kontakte\" (Contacts.app), wie unter iOS.\n\nNeben einer Integration in weitere Apple-Produkte (z. B. Serienbriefe in Pages und natürlich E-Mail-Adressen in Mail) ist auch eine Synchronisation (Abgleichen von Kontaktdaten) mit verschiedenen Servern wie Microsoft Exchange und Google Kontakte möglich, mit der Apple-Cloud iCloud kann eine Push-Synchronisierung mit den Versionen der App für iOS ermöglicht werden. Durch die Unterstützung des vCard-Systems können auch gängige Mobiltelefone und PDAs mit \"Kontakte\"-Dateien beliefert werden. Importiert werden auch LDIF- und CSV-Dateien. Um mit anderen Programmen zu kommunizieren, verwendet die App eine C- sowie Objective-C-API und unterstützt AppleScript. \n\nEinträge können in Gruppen zusammengefasst werden, manuell oder mittels Spotlight-Unterstützung auch in „intelligente“ Gruppen. Seit der Einführung von Apple Maps lassen sich Adressen auf einer Karte anzeigen. Telefonnummern werden bei Eingabe automatisch formatiert. Duplikate in der Datenbank werden automatisch erkannt. \n\nDie Druckfunktion des Programms bietet die Möglichkeit, automatisch generierte digitale Druckvorlagen für Adressaufkleber, Briefumschläge, Versandlisten oder Taschen-Adressbücher an Drucker auszugeben. \n\nDurch die tiefe Verankerung im System wird auch anderen Programmen Zugriff auf die Adressen – nach vorheriger Zustimmung des Benutzers – gewährt: So kann das Adressbuch von Mail verwendete Empfängeradressen speichern, URLs in Adressbuch-Karten erscheinen in Apple Safaris „Adressbuch“-Lesezeichen und Kontakte in Nachrichten können Adressbuch-Karten zugeordnet werden. Außerdem können in \"Kontakte\" gespeicherte Geburtstage in Kalender eingeblendet werden.\n\nDie Apple-Adressbuch-Version 4.1, Teil von Mac OS X 10.5, wurde stark kritisiert, weil die in den Vorgängerversionen vorhandene Bluetooth-SMS- und Anruf-Funktionalität entfernt worden war.\n\n"}
{"id": "3720336", "url": "https://de.wikipedia.org/wiki?curid=3720336", "title": "GnuWin32", "text": "GnuWin32\n\nDas GnuWin32-Projekt stellt Windows-Portierungen von GNU- und Open-Source-Programmen zur Verfügung. Diese werden als kompilierte Programme, Quellcode-Patches, und Quelltext veröffentlicht.\n\nEinige der Programme, die in GnuWin32 enthalten sind:\n\n\n"}
{"id": "3720779", "url": "https://de.wikipedia.org/wiki?curid=3720779", "title": "KDE aided design", "text": "KDE aided design\n\nKDE aided design kurz KAD ist ein CAD-Programm für zweidimensionale Zeichnungen. Es basiert auf der Community-Edition von QCad und besitzt eine angepasste Oberfläche für KDE. Die aktuelle Version 0.8.2 ist für Linux, Unix und BSD verfügbar. Sie verwendet die KDE-Bibliotheken in Version 3. An einer Portierung auf KDE4 wird zurzeit gearbeitet.\n\nKAD bietet über 40 Konstruktions- und mehr als 20 Modifikations-Werkzeuge für Punkte, Linien, Bögen, Kreise, Ellipsen, Splines, Polylinienzüge, Texte, Bemaßungen und Schraffuren. Weiter kann das Programm mit Layern und Blöcken (Gruppierungen) umgehen. Maßstabgetreues Drucken, Objekt-Fang, Symbolbibliothek und Mess-Werkzeuge gehören ebenfalls zum Funktionsumfang.\n\nAls Dateiformat verwendet KAD das DXF. Die zu verwendende Maßeinheit kann zwischen dem metrischen und dem britischen Einheitensystem umgeschaltet werden.\n\n"}
{"id": "3720787", "url": "https://de.wikipedia.org/wiki?curid=3720787", "title": "Schwedischer Schachcomputerverein", "text": "Schwedischer Schachcomputerverein\n\nDer Schwedische Schachcomputerverein (schwedisch: Svenska schackdatorföreningen, Abkürzung: SSDF) ist eine unabhängige nichtkommerzielle Vereinigung, die sich mit Computerschach und Software im Zusammenhang mit Schach befasst. Er wurde im August 1984 gegründet. Ziel des Vereins ist, die Spielstärke von Computerschachprogrammen vergleichend zu ermitteln. Dazu werden von vielen Freiwilligen Schachpartien zwischen verschiedenen Schachprogrammen auf Computern durchgeführt und auf Basis der Resultate Elo-Zahlen ermittelt. Als Bedenkzeit gelten generell die im Turnierschach üblichen 120 Minuten für die ersten 40 Züge.\n\nDie so ermittelte Rangliste (SSDF-Liste) spiegelt die relative Spielstärke der Programme wider und wird in unregelmäßigen Abständen veröffentlicht. Koordinator und Ansprechpartner ist Lars Sandin. Die SSDF-Rangliste stellt eine gängige Weltrangliste für die Spielstärke von Schachcomputern und Computerschachprogrammen dar. Deutschen Lesern wurde sie vor allem durch die Fachzeitschrift \"Computerschach und Spiele (CSS)\" bekannt, in der sie seit 1986 in jeweils aktueller Fassung immer wieder veröffentlicht wurde.\n\nAußer der Übersichtsliste (siehe Weblinks), die das „\"Rating\"“ (deutsch: die Leistung) der besten zehn Programme als Elo-Zahl anzeigt, existiert auch eine ausführlichere Tabelle (engl.: \"Full list\"), die deutlich mehr Programme auflistet und weitere statistische Informationen liefert. Dazu gehört zunächst die Angabe der Unsicherheit (Toleranz), des möglichen Schwankungsbereichs der Elo-Angabe (beispielsweise +34 und −32 Elo-Punkte), wobei diese Grenzen den Bereich der doppelten Standardabweichung (2-σ-Bereich) beschreiben, womit sich ein Vertrauensbereich von etwas mehr als 95 Prozent ergibt. Ferner wird für jedes Programm die Anzahl der absolvierten Partien (engl.: \"Games\"), der Prozentsatz der Gewinnpartien (engl.: \"Won\") und der Durchschnitt der Elo-Zahlen der Gegner (engl.: \"Average opponents\") angegeben. Ferner gibt es noch die „Langversion“ der SSDF-Liste, die als ASCII-Textdatei zum Herunterladen angeboten wird und die hunderte aktuelle und historische Programme aufführt (siehe Weblinks).\n\nNachdem 2015 das Programm Stockfish (Version Stockfish 6 MP x64 2GB Q6600 2,4 GHz) mit einer Elo-Zahl von 3334 an der Spitze lag, und es 2016 durch Komodo (Version 9.1 MP x64 2GB Q6600 2,4 GHz) mit einer Elo-Zahl von 3361 Punkten abgelöst wurde, führt aktuell (Februar 2018) wieder Stockfish (8 MP x64 16GB 1800X 3,6 GHz) nun mit 3436 Punkten die Liste an.\n\nDie folgende Liste illustriert die Steigerung der Spielstärke der künstlichen Intelligenz im Schach seit dem Jahr 1984 anhand der Elo-Besten der SSDF-Rangliste zum jeweiligen Jahresanfang. Zu beachten ist, dass die Referenz dieser Liste, also der absolute Maßstab für die Spielstärke, über die Jahre nicht konstant blieb. Dies liegt an dem schwierigen Vergleich zwischen der Spielstärke von Computern und der Spielstärke menschlicher Schachspieler, da Computer nur selten unter offiziellen Turnierbedingungen gegen Menschen antreten. „Umgekehrt, da Menschen nur selten unter offiziellen Turnierbedingungen gegen die Computer antreten“, würden die Schachcomputer sagen, wenn sie sprechen könnten. Tatsächlich äußerten sich Schachprogrammierer, die Entwickler der Schachcomputerprogramme, entsprechend. Sie bedauerten damit, dass ihre Schachprogramme nur selten zu offiziellen (Menschen-)Turnieren zugelassen wurden. Man könnte auch sagen, dass menschliche Schachspieler immer seltener den Mut aufbrachten, in offiziellen Turnieren gegen Schachcomputer anzutreten. So war die Skalierung der maschinellen Spielstärke anhand der menschlichen nur lückenhaft möglich. Inzwischen hat sich dies aber praktisch erledigt und der Wunsch, die Spielstärke von Schachcomputern anhand von Menschen zu messen, ist obsolet geworden. Grund ist, dass die maschinelle Spielstärke der menschlichen inzwischen weit überlegen ist. Mit anderen Worten: Kein Mensch kann der künstlichen Schach-Intelligenz das Wasser reichen. Es wäre wie ein Wettkampf im Gewichtheben gegen einen Gabelstapler. Selbst den besten menschlichen Schachspielern, wie dem Schachweltmeister, werden Elo-Zahlen von kleiner als 2900 zugeordnet, während die stärksten Schachcomputerprogramme Werte jenseits der 3400 aufweisen.\n\nIm Versuch, die Vergleichbarkeit der Spielstärke dennoch möglichst gut zu erzielen, wurde die Bewertungszahl immer wieder neu angepasst und die Liste „renormiert“. Details zur Renormierung können im unten angegebenen Beleg \"„PLY/SSDF – the story“\" nachgelesen werden. Aus einer solchen \"Renormierung\" resultiert beispielsweise auch der vermeintliche Abfall der Spielstärke des Spitzenreiters im Jahr 1991. Die ermittelten Zahlen sind daher nicht ohne weiteres miteinander vergleichbar. Auch können sie nicht unmittelbar mit denen menschlicher Schachspieler gleichgesetzt werden, da sie ausschließlich durch Partien zwischen Computern ermittelt wurden.\n\n\n"}
{"id": "3722763", "url": "https://de.wikipedia.org/wiki?curid=3722763", "title": "Star Wars: The Clone Wars (Film)", "text": "Star Wars: The Clone Wars (Film)\n\nStar Wars: The Clone Wars (Arbeitstitel: \"Clone Wars 3D\") ist ein US-amerikanischer Computeranimationsfilm, der im Star-Wars-Universum von George Lucas spielt und am 14. August 2008 in die deutschen Kinos kam. Starttermin in den USA war der 15. August 2008.\n\nDie Handlung spielt zwischen \"\" und \"\". Im Unterschied zur Serie \"\" wurde statt einer 2D-Animation eine aufwändigere 3D-Animation verwendet. Der Film stellt den Pilotfilm zur dar, die am 3. Oktober 2008 in den USA auf Cartoon Network startete und am 23. November 2008 in Deutschland auf ProSieben Premiere feierte.\n\nIn der Galaxis toben die Klonkriege. Im äußeren Rand entwickelt sich der Krieg zu Gunsten der Separatisten. Diese entführen Rotta the Hutt, den Sohn des mächtigen Gangsterbosses Jabba der Hutt. Hiermit wollen sie erreichen, dass dieser die Galaktische Republik um Hilfe bittet, denn wenn diesen die Rettung seines Kindes nicht gelingt, so könnte sich Jabba den Separatisten zuwenden. Der Jedi-Meister Yoda entscheidet sich, Obi-Wan Kenobi und Anakin Skywalker auf diese Mission zu schicken.\n\nKenobi und Skywalker sind mit ihrer Armee von Klonkriegern momentan auf dem Planeten Christophsis stationiert, wo sie gegen den separatistischen General Whorm Loathsom und seine Droidenarmee kämpfen. Die Lage ist ernst, denn die Klonkrieger sind weit in der Unterzahl. Nach einigen Kämpfen stößt Hilfe zu den Klonen, als der junge, weibliche Padawan Ahsoka Tano mit zur Verstärkung eintrifft. Es stellt sich heraus, dass das junge Mädchen nicht wie erwartet Obi-Wan, sondern Anakin zugeteilt worden ist. Anakin, der eigentlich keinen Schüler will, reagiert zuerst eher negativ auf Ahsoka. Er schleicht jedoch mit ihr hinter die feindlichen Linien, um den Schildgenerator, das Rückgrat der Separatisten des Planeten, auszuschalten, was schließlich auch gelingt. Die Separatisten werden besiegt.\n\nDie drei Jedi werden nun von Meister Yoda mit dem Auftrag der Rettung von Jabbas Sohn, Rotta, betraut. Anakin und seine neue Schülerin, deren Verhältnis zu ihm sich mittlerweile gebessert hat, sollen ihn aus einem verlassenen Kloster retten. Er wird dort von Separatisten festgehalten, was die Republik jedoch nicht weiß. Obi-Wan soll nach Tatooine zu Jabba gehen, um dort die Verhandlungen aufzunehmen.\n\nAnakin und Ahsoka greifen nun auf Teth mit einer Armee Klonkrieger das stark befestigte, auf einem sehr steilen Berg liegende Kloster an. Sie werden zur Landung auf dem Waldboden gezwungen und müssen so mit Seilen und Kampfläufern den Berg erklimmen. Dabei werden sie bereits von Kampfdroiden beschossen. Nachdem sie oben angekommen sind und die Besatzung ausgelöscht haben, betreten sie das dunkle Innere des Klosters. Hier werden sie von einem Droiden begrüßt, der sich als Verwalter des Hauses vorstellt. Er erzählt ihnen von den Gefängnissen des Klosters. Ahsoka und Anakin trennen sich von den Klonkriegern und steigen in die besagten Kerker. Hier finden sie den kleinen, kranken Huttling und nehmen ihn mit. Auf Teth treffen aber weitere Droidentruppen unter dem Kommando der Sith Schülerin Asajj Ventress ein, welche die Klone davon abhalten wollen, den Huttling aus der Festung zu retten. Nach einigen Kämpfen mit Droiden und Ventress können Ahsoka und Anakin mit Rotta vom Berg flüchten. Währenddessen kommt der Anführer der Separatisten, Count Dooku, ebenfalls bei Jabba an und zeigt ihm Aufnahmen von Anakin und Ahsoka, die es so aussehen lassen, als ob sie Rotta entführt hätten. Ventress, die bereits vor dem Eintreffen der neuen Droiden im Kloster anwesend gewesen war, hatte Dooku diese zuvor geschickt. Er bietet dem Hutten an, sich den Separatisten anzuschließen, damit sie seinen Sohn zurückbringen.\n\nAuf Teth sind Anakin und Ahsoka gerade an einem Frachtschiff angekommen, mit dem sie nach Tatooine fliegen wollen. Aus diesem entsteigt plötzlich der angebliche Verwalter-Droide, der sich als Separatist entpuppt. Anakin und sein neuer Schüler können den Droiden und die Kampfdroiden des Schiffes eliminieren. Sie übernehmen schließlich die Kontrolle über das alte Schiff. Über Teth tobt aber eine Raumschlacht zwischen Jägern der Republik und den ferngesteuerten Raumschiffen der Separatisten. Die beiden wollen in einem republikanischen Kreuzer landen, als die Droidenjäger, die Anakin verfolgen, mitten in den Hangar reinfliegen und der Hangar explodiert. Daraufhin müssen Anakin und Ahsoka mit dem Frachter allein nach Tatooine fliegen, um dort Rotta zu seinem Vater zurückzubringen. Als die drei aus dem Hyperraum austreten, wird das Schiff jedoch von Dookus Leibwächterdroiden abgeschossen und muss mitten in der Wüste, nahe dem Palast von Jabba the Hutt, notlanden. Die beiden beschließen, zu Fuß zum Palast zu gehen.\n\nIm Kloster von Teth trifft zur selben Zeit Obi-Wan Kenobi auf Asajj Ventress, woraufhin die beiden sich mit dem Laserschwert duellieren. Nach einem ausgeglichenen Kampf muss Ventress jedoch die Flucht ergreifen.\n\nSpäter beschließen Anakin und Ahsoka auf Tatooine, sich aufzuteilen, da Anakin spürt, dass eine dunkle Präsenz immer näher kommt. Anakin geht alleine zum Palast und Ahsoka nimmt mit Rotta und R2-D2 einen anderen Weg. Anakin wird auf seiner Route durch die Wüste unerwartet von Count Dooku angegriffen. Dieser glaubt, Anakin habe Rotta bei sich, da er den Rucksack, im welchen die beiden Jedi den Hutt transportiert hatten, auf dem Rücken trägt. Die beiden duellieren sich, doch als Anakin Darth Tyranus seine List offenbart, zeigt dieser ihm Aufnahmen davon, wie seine Magna-Wachen die junge Ahsoka attackieren. Anakin flieht daraufhin vor Count Dooku zum Palast.\n\nObi-Wan hat mittlerweile sein Duell mit Ventress beendet und kehrt wieder auf seinen Raumkreuzer zurück.\n\nAuf Coruscant, der Hauptwelt der Republik, trägt sich derweil anderes zu. Senatorin Amidala, Anakins heimliche Ehefrau, hat von der Mission der Jedi erfahren und versucht, den ortsansässigen Hutten Ziro, Jabbas Onkel, auf die Seite der Republik zu ziehen. Ziro lehnt aber entschieden ab. Als sich die Senatorin von dem Droiden, der sie hinausbegleitet, losreißt, kann sie ein Gespräch des Hutten mit einem Hologramm Graf Dookus belauschen, wodurch sie herausfindet, dass Ziro Dooku dabei geholfen hat, Rotta zu entführen. Padmé wird von Ziros Leibwachen ergriffen und verhaftet. Es gelingt ihr jedoch durch Zufall, kurz C-3PO, ihren Protokolldroiden, zu kontaktieren. Dieser trifft wenig später mit mehreren Klonen ein, die Ziro überwältigen. Dieser wird nun gezwungen, Jabba zu kontaktieren und ihm seine Intrige zu gestehen.\n\nAuf Tatooine ist Anakin mit dem Düsenschlitten Dookus unterwegs zu Jabbas Palast. Als er ankommt, ruft ihn Ahsoka, doch Anakin hört sie nicht und fährt weiter. Er geht zu Jabba und bedroht ihn, damit ihm dieser sagt, was mit Ahsoka geschehen ist. Kurz darauf stößt Ahsoka zu ihnen und übergibt Jabba seinen Sohn. Trotzdem will Jabba sie umbringen lassen. Kurz bevor zwischen Jabbas Schergen und den beiden Jedi ein Kampf ausbricht, trifft Ziros Geständnis ein. Jabba lässt die beiden Jedi ziehen und die Republik die von ihm kontrollierten Hyperraumstraßen benutzen. Anakin und Ahsoka werden schließlich von Tatooine abgeholt.\n\nDie Handlung des Films setzt kurz nach \"\" ein und spielt damit inmitten des namensgebenden Klonkriegs. \nDer Film stellt den Pilotfilm zur Fernsehserie \"\" dar und besteht aus vier Folgen. Die Handlung um die Entführung von Jabbas Sohn hätte eigentlich als Pilot-Dreiteiler im TV laufen sollen, diese aber wurden zu einem Film zusammengesetzt. Dabei nahm man auch eine Folge die später produziert wurde, aber deren Handlung vor der Entführung spielte. Die Folge in der das erste Treffen zwischen Anakin und Ahsoka auf Christophsis gezeigt werden sollte, wurde der erste Teil des Filmes und die vier Folgen wurden für die Kinoleinwand aufbereitet. Da Star-Wars-Schöpfer George Lucas meinte, dass die Serie zu gut für das Fernsehen sei, feierte der Film seine Premiere im Kino. Die vier Episoden hießen \"The New Padawan\", \"Castle of Deception\", \"Castle of Doom\" und \"Castle of Salvation\".\nIn der Serie kommen, anders als im Pilotfilm, viele andere Figuren aus dem erweiterten Star-Wars-Universum als Hauptcharaktere in den einzelnen Folgen vor. So gibt es zum Beispiel eine Folge, in der Yoda auf dem Heimatplaneten der Toydarianer eine Allianz vereinbaren will, während Kit Fisto in einer anderen Episode versucht, das Versteck von General Grievous zu orten.\n\nDie Effekte werden von Lucasfilm Animation in Singapur sowie von Industrial Light & Magic produziert. George Lucas ist für die Produktion als Ganzes verantwortlich, das Drehbuch schrieb Henry Gilroy, zusammen mit Steven Melching und Scott Murphy. Es wurde zugesichert, dass sich die Geschichte an die bisherigen Erzählungen aus Comics und Romanen (dem Expanded Universe) orientieren wird, somit soll die Kontinuität zwischen den einzelnen Werken bewahrt werden. Gilroy erhielt jedoch die Anweisung, dass eine gut erzählte Geschichte keinesfalls aufgrund der Kontinuität fallen gelassen werden soll.\n\nFür die deutsche Version wurden, im Gegensatz zur Originalversion, die gleichen Synchronsprecher wie in den normalen Kinofilmen verpflichtet. Neuauftritte feierten lediglich einige wenige Synchronsprecher, zum Beispiel Josephine Schmidt als Ahsoka Tano. Die Synchronsprecher waren im Einzelnen:\nDer Film wurde neben Spielzeugfiguren und Lego-Bausätzen auch mit Stickeralben, Büchern, Comics und beinahe täglich im Internet erscheinenden Videos und Trailern beworben. Die Figuren und Bausätze wurden am 26. Juli 2008 veröffentlicht. In einigen Toys’R’Us-Filialen gab es auch Mitternachtsveranstaltungen, bei denen die Produkte ab Mitternacht gekauft werden konnten. In Deutschland konnte man sich mit kostümierten Mitgliedern der 501st Legion fotografieren lassen und eine gratis Actionfigur von General Grievous sowie ein Poster mit den Lego Sets zum Film erhalten.\n\nToys’R’Us und Galeria Kaufhof hatten die Erstverkaufsrechte für die Lego-Bausätze. Normale Spielwarenhändler wurden erst im September mit nur drei der insgesamt acht Bausätze die zum Film erschienen beliefert, da fünf der Sets exklusiv für die beiden oben genannten Händler sind und nur dort öffentlich verkauft werden.\nDa die Bausätze der Filme im Vergleich mit anderen Lego Produkten sehr teuer waren, waren die Absatzzahlen auch sehr gering. Kurz vor Weihnachten wurden die Sets billiger angeboten.\n\nInsgesamt erhielt The Clone Wars nahezu ausschließlich negative Kritiken und wurde von einigen Kritikern förmlich verrissen. Der Film wurde außerdem für die Goldene Himbeere in der Kategorie „Schlechtestes Prequel, Fortsetzung oder Rip-off“ nominiert.\n\nDie Deutsche Film- und Medienbewertung FBW in Wiesbaden verlieh dem Film das Prädikat wertvoll.\n\n\n"}
{"id": "3726691", "url": "https://de.wikipedia.org/wiki?curid=3726691", "title": "Pgrep", "text": "Pgrep\n\npgrep ist ein Kommandozeilenprogramm, das ursprünglich für das Solaris 7 Betriebssystem entwickelt wurde. Seither wurde jedoch auch eine Version für GNU/Linux und OpenBSD geschrieben. Das Programm sucht nach allen benannten Prozessen, die als Reguläre Ausdrücke angegeben werden können, und gibt standardmäßig ihre Prozess-ID zurück. Zu den Alternativen gehören pidof, das ursprünglich für Linux entwickelt wurde, und ps.\n\"pgrep\" an sich ist als vereinfachte Schreibweise einer komplexeren Verkettung von Programmaufrufen, um die Eingabe zu vereinfachen. Es wurde aber auch um eigene Funktionalität erweitert, um komplexere Abfragen für Prozesse ausführen zu können.\n\nGrundfunktionalität\n\nErweiterte Funktionalität\n\n"}
{"id": "3728301", "url": "https://de.wikipedia.org/wiki?curid=3728301", "title": "Computerschach und Spiele", "text": "Computerschach und Spiele\n\nComputerschach und Spiele (Abkürzung: CSS) war eine Fachzeitschrift, die sich mit Computerschachprogrammen, Schachcomputern und im weitesten Sinne mit Computerschach befasste. Ab dem Jahr 2005 besteht CSS seitdem in veränderter Form als Online-Magazin „CSS Online“.\n\nDie Zeitschrift wurde von Frederic Friedel (später Mitbegründer der Firma \"ChessBase\" in Hamburg) und Dieter Steinwender zunächst als Club-„Zeitschrift des ersten Computerschachclubs der Bundesrepublik“ gegründet und erschien erstmals Anfang 1983 unter dem Titel „Computerschach International“. Die Erscheinungsweise war seither zweimonatlich. Anfang 1984 wurde der Name in „Computerschach & Spiele“ geändert, bevor er Mitte 1986 mit Heft 4/1986 seine endgültige Form „Computerschach und Spiele“ erhielt. Seit 1986 wurde in CSS auch die SSDF-Rangliste veröffentlicht, die auf der Basis der Elo-Zahlen seit Jahrzehnten die maßgebliche Referenz für die relative Spielstärke von Computern darstellt.\n\nDie letzte Papierausgabe erschien schließlich Ende 2004. Zum folgenden Jahreswechsel wurde das Magazin auf eine Online-Version umgestellt. Die Zahl der Online-Ausgaben ging seither kontinuierlich zurück. Die bislang letzten Ausgaben waren zwei Halbjahresausgaben 2008.\n\nDie Jahrgänge der Zeitschrift 1983 bis 2000 wurden vor einigen Jahren vollständig digitalisiert und sind als DVD \"Chronik des Computerschachs\" erhältlich.\n\nWie die traditionsreiche Heftserie seit 1983, deren hohe Qualität auch durch eine Reihe von internationalen Mitarbeitern wie beispielsweise Ken Thompson, David Levy, John Nunn und Christian Donninger über die vergangenen Jahrzehnte profitierte, befasst sich das Online-Magazin mit allen Aspekten des Computerschachs.\n\nWichtige Themen sind Schachcomputer, Computer-Schachprogramme, Computer-Schachturniere, insbesondere die Mikrocomputer-Schachweltmeisterschaft (WMCCC) und die Schachcomputer-Weltmeisterschaft (WCCC), Vergleich Mensch gegen Computer, möglichst objektive Einschätzung der Spielstärken, Erläuterungen zu Programmstrukturen, Stärken und Schwächen in den verschiedenen Partiephasen, wie Eröffnung, Mittelspiel und Endspiel, Eröffnungs- und Endspieldatenbanken, Hilfsprogramme, beispielsweise zur Auswertung von Turnieren, und so weiter. CSS veröffentlicht auch eine Rangliste (siehe Weblinks), die ähnlich wie die erwähnte SSDF-Liste die Spielstärke von Schachprogrammen angibt.\n\nÜber das Schach hinaus, gilt das Interesse des Magazins – wie bereits im Titel zum Ausdruck kommt – auch anderen Brettspielen wie Backgammon, Dame, Go und Mühle.\n\n\n"}
{"id": "3729099", "url": "https://de.wikipedia.org/wiki?curid=3729099", "title": "Fraps (Software)", "text": "Fraps (Software)\n\nFraps (zusammengesetzt aus Frames per second) ist ein Screencast- und Echtzeitvideoaufnahmeprogramm für DirectX- und OpenGL-Anwendungen. Es wird häufig zur Bestimmung der Computerleistung bei Spielen als auch zur Aufnahme von Computerspielszenen verwendet. Das Programm ist bei Amateur-Machinima-Filmern sehr beliebt. Fraps ist nur als Kaufversion durch Digitale Distribution erhältlich. \n\nAm 19. Mai 2003 wurde mit Version 1.9d die letzte Fassung als Freeware veröffentlicht. Die aktuelle Demoversion platziert ein nicht entfernbares \"Fraps\"-Zeichen als digitales Wasserzeichen an die obere Ecke des Videos und die Aufnahmezeit ist auf 30 Sekunden beschränkt. In der Vollversion werden Videos ohne das Fraps-Logo aufgezeichnet. Darüber hinaus beträgt die maximale Auflösung 7680 × 4800 und die Bildwiederholrate 120 Bilder pro Sekunde. Bei aktivierter Aufnahme wird die Bildwiederholrate von Fraps an die des aufzunehmenden Programms angepasst (meistens 30 fps). Obwohl 30 fps der Durchschnitt sind, können bereits leistungsschwache Computer bzw. Computer mit geringen Datenübertragungsraten sowie umfangreiche Programme die Bildwiederholrate drastisch verringern und zu ruckelnden Aufnahmen führen.\n\nDie Aufnahme von einzelnen Screenshots wird zumeist vom Betriebssystem oder der grafischen Benutzeroberfläche als Funktion zur Verfügung gestellt. Wenn dies bei einem Programm nicht funktioniert, weil es grafische Programmierschnittstellen wie DirectX oder OpenGL benutzt, können mit Fraps Screenshots gemacht werden.\n\n"}
{"id": "3734182", "url": "https://de.wikipedia.org/wiki?curid=3734182", "title": "Address Space Layout Randomization", "text": "Address Space Layout Randomization\n\nASLR bezieht sich auf den EBP und Libraries sowie das Heap-, das Text-, das Data- und das BSS-Segment, wobei letztere nicht bei allen ASLR-Lösungen randomisiert werden.\n\nDie ASLR-Technik fand erstmals in dem Betriebssystem OpenBSD Verwendung und wurde mit Erscheinen von Windows Vista auch von Microsoft eingeführt, außerdem wurde sie von Apple in Mac OS X Leopard eingeführt. Allerdings unterscheidet sich die Implementierung in Vista und Mac OS X Leopard voneinander. Während in Vista ASLR im gesamten System implementiert ist, wird bei Mac OS X Leopard nur die Bibliothek geschützt. Mit der Einführung von Mac OS X Lion wurde ASLR komplett implementiert.\n\nDer offizielle Linux-Kernel bietet ab der Version 2.6.12 (Juni 2005) eine unvollständige ASLR-Implementierung. Mit PaX konnte jedoch schon seit 2001 eine weiterreichende Unterstützung verwendet werden. Seit Kernelversion 3.14 gibt es eine vollständige Implementierung von ASLR. Ab Version 4.8 kollidiert die (KASLR) nicht mehr mit der Hibernate-Funktion.\n\nMit iOS 4.3 fand die ASLR-Technik erstmals auch auf einem Betriebssystem für Mobilgeräte Anwendung. Die Implementierung in dem mobilen Webbrowser Apple Safari sollte Anwendern Sicherheitsvorteile bringen, führte aber zunächst zu einer Sicherheitslücke. Dem Hacker Charlie Miller gelang es nur drei Tage nach Erscheinen der Firmware, diese über das ASLR-Einfallstor zu hacken.\nAndroid führte in der Version 4.0 \"(Ice Cream Sandwich)\" ebenfalls eine unvollständige ASLR-Implementierung ein. Ab Version 4.1 \"(Jelly Bean)\" verfügt Android über eine vollständige Implementierung.\n\nASLR lässt sich durch sogenanntes \"\" umgehen. Dabei wird der Schadcode über hunderte Megabyte im Speicher dupliziert (großflächiges „Sprayen“). Dadurch steigt die Wahrscheinlichkeit, dass trotzdem (irgendwann) ein Bibliotheksaufruf Schadcode ausführt.\n"}
{"id": "3745960", "url": "https://de.wikipedia.org/wiki?curid=3745960", "title": "WinG", "text": "WinG\n\nWinG (sprich: \"Win Gee\") ist eine Grafik-Programmierschnittstelle für Windows 3.1, die bis Windows 98 Second Edition unterstützt wurde und anschließend komplett in die Graphics Device Interface (GDI) übernommen wurde. Grund für ihre Entwicklung waren Unzulänglichkeiten in der GDI.\n\nDie GDI wurde mit dem Gedanken entwickelt, Grafik unabhängig vom tatsächlich verwendeten Ausgabegerät darstellen zu können. So ist es beispielsweise möglich, mit demselben Quelltext eine Grafik zu drucken oder auf den Bildschirm auszugeben. Mittels entsprechender GDI-Treiber werden die Grafikdaten umgewandelt und in einer gerätespezifischen Umgebung, dem sogenannten \"Device Context\" (DC), in einem Bildspeicher (Framebuffer) gespeichert. Von dort aus können sie dann direkt von der jeweiligen Hardware angesprochen werden. Für Animationen ist dieser Ansatz jedoch nicht geeignet, da die Bilddaten sehr häufig aktualisiert werden müssen und die Umwandlung in einen geräteabhängigen Kontext zeitaufwendig ist. Hinzu kommt, dass es nicht möglich ist, Informationen aus dem Kontext wieder auszulesen, da die Daten dort in einem Hardware-abhängigen Format vorliegen und das Auslesen keinen Sinn hätte.\n\nDiese Problematik wird von WinG mit Hilfe von device-independent Bitmaps (DIB), also geräteunabhängigen Bildern umgangen, die in einem \"WinGDC\" abgelegt werden. Diese besitzen zwar nicht die Möglichkeit, direkt auf andere Ausgabegeräte umgeleitet zu werden, aber die zeitaufwendige Konvertierung in ein natives Format ist nicht notwendig, und die Pixel können wieder ausgelesen werden.\n\nWinG wurde maßgeblich dafür entwickelt, Grafikanwendungen für Windows populärer zu machen. DOS besaß den großen Vorteil, dass man ohne Probleme direkt auf den Grafikspeicher zugreifen konnte, was unter Windows ohne WinG nicht möglich war. Mit BitBlt() und StretchBlt() führte die API einige Blitting-Operationen ein, die die Anzeige von Bildern und Grafiken erleichtern sollte. Zusammen mit den GDI-Aufrufen, zu denen WinG vollständig kompatibel ist, war es erstmals auch unter Windows möglich, komplexere Grafik-Algorithmen wie Double Buffering, \"Dirty Rectangles\", \"Overdraw\", und \"Fast Scrolling\" zu realisieren.\n\nNach der Installation führt WinG einen grafischen Performanz-Test durch, in dem es eine Reihe von Funktionsaufrufe an die Grafikkarte durchführt und deren Zeit misst. Auf dem Bildschirm ist für einige Zeit ein Muster mit verschlungenen roten Linien zu sehen, die sich hin- und herbewegen. Wenn der Test abgeschlossen ist, werden die schnellsten Funktionen, die keine Artefakte verursachen, gespeichert, so dass der Test nur einmal ausgeführt werden muss.\n\nMit dem wachsenden Erfolg von DirectDraw wurde WinG zunehmend unbedeutender und wurde mit dem Release von DirectX 6.0 vollständig in die GDI-Bibliothek eingegliedert. Als indirekte Nachfolger können GDI+ und die Windows Graphics Foundation gesehen werden.\n"}
{"id": "3746178", "url": "https://de.wikipedia.org/wiki?curid=3746178", "title": "Rasteroperation", "text": "Rasteroperation\n\nEine Rasteroperation (kurz: \"ROP\") verknüpft mit booleschen Operatoren, die zu schreibende Bilddaten mit den bereits im Framebuffer vorhandenen Werten. Man unterscheidet zwischen binären ROPs, die lediglich Quelle und Ziel miteinander verknüpfen und seltener verwendeten ternären Operatoren, die zusätzlich einen Pinsel (\"Brush\") verwenden. Im Gegensatz zu Blending werden bei Rasteroperationen ganzzahlige Farbwerte miteinander verknüpft, da logische Verknüpfungen im Allgemeinen auf Gleitkommazahlen nicht angewandt werden können.\n\nIm einfachsten Fall werden die Pixel des zu schreibenden Bildes ohne Rücksicht auf den Inhalt des Framebuffers geschrieben. Die Quelle wird also mit einer logischen Eins verknüpft, das Ziel mit einer logischen Null. Da die Quelle unverändert übernommen wird, wird die durchgeführte Operation als \"Kopie\" bezeichnet.\n\nEin häufiges Anwendungsgebiet von ROPs sind Maskierungsfunktionen: Das Ausgangsbild und ein Schwarz-Weiß-Bild (die Maske) werden mit einem UND-Operator verknüpft. Diese Verknüpfung mit einem schwarzen Bildpunkt in der Maske führt immer zu einem schwarzen Bildpunkt, ein weißer Bildpunkt lässt die Ausgangsdaten unverändert. Mit dieser Methode ist es möglich, Transparenz-Effekte zu erzeugen, d. h. nur Teile des Bildes zu rendern.\n\nEine andere Einsatzmöglichkeit von Rasteroperationen ist das Invertieren einer Grafik. Dazu werden die Bilddaten mit einer XOR-Verknüpfung, angewandt auf eine weiße Maske, verbunden. Da jedes Bit in der Maske gesetzt ist, wird jedes Bit im Bild invertiert, das Resultat ist also das Negativ des Originals.\n\nDie boolesche Rasteroperation wird stets gleich an \"allen\" Bits aller Pixel ausgeführt. Daher ergibt sich bei der o.a. XOR-Verknüpfung die Komplementärfarbe, sofern es sich nicht um Palettenindizes handelt. Bei Palettenbildern ergibt sich hingegen nur das Komplement des Palettenindex'. Nur durch die komplementäre Verteilung von Farbwerten in der Palette ergibt sich dann das erwartete visuelle Ergebnis. In der Praxis belegt man dazu die beiden „Enden“ der Palette mit festen, häufig benutzten Farbwerten, etwa den Index 0 mit Schwarz und den Index 255 mit Weiß. Unter MS-Windows sind bei einer Bildschirmanzeige mit 256 Farben 20 komplementäre Paletteneinträge reserviert, 10 „unten“ und 10 „oben“.\n\nBei \"binären ROPs\" gibt es insgesamt 2 = 16 Kombinationen, Quell- und Zieldaten zu verknüpfen. Quelldatenbits werden üblicherweise mit P (für Pattern) und Zieldatenbits mit D (für Destination) symbolisiert.\nDa sowohl P als auch D logisch Null oder Eins sein kann, ergeben sich 2 = 4 Kombinationen des Zusammentreffens von Bits. Eine 4 Zeilen lange (= 2 Adressbits) und 1 Bit breite Lookup-Tabelle übersetzt dann in das Ergebnisbit. Davon gibt es 16 mögliche.\nDie Implementierung via Lookup-Tabelle ist für Grafikprozessoren günstig. Bei Verwendung eines Universalprozessors wird die Lookup-Tabelle durch eine (hier schnellere) logische Verknüpfung ersetzt, die gleich mehrere Bits verarbeitet. Die Angabe der logischen Verknüpfung findet man abgekürzt in umgekehrter polnischer Notation etwa so vor: DPx bedeutet D exklusiv-oder-verknüpft mit P.\n\nBei \"ternären ROPs\" gibt es insgesamt 2 = 256 Kombinationen, das B-Bit (für Brush) kommt hinzu. Die Lookup-Tabelle ist entsprechend 8 Zeilen (= 3 Adressbits) lang.\n\nBinäre Rasteroperationen sind ideal für Bildschirme mit geringer Farbtiefe, etwa schwarz-weiße, einfach zu implementieren und dabei schnell. Sie leiden nicht an Sättigungseffekten wie arithmetische (Alpha-Blending) und sind für CAD-Bearbeitung in mehreren Lagen (typischerweise Platinenentwurf) sehr günstig. Hingegen sind sie für Bildbearbeitung in hoher Farbtiefe (etwa Fotos) kaum geeignet. Durch die Verwendung farbtiefer Systeme sind binäre Rasteroperationen aus der Mode gekommen. In Vektorgrafiken werden binäre ROPs nur in den Windows-Vektordateien WMF und EMF unterstützt und bereiten Probleme bei der Konvertierung zu solchen, die das nicht unterstützen.\n\nAlpha-Blending als eine wichtige moderne Operation löst mehr und mehr die binären Rasteroperationen ab und ist in nahezu allen modernen Vektorgrafikformaten verfügbar, etwa PDF und SVG. Alpha-Blending kann nicht durch binäre Rasteroperationen nachgebildet werden, abgesehen vom „harten“ Ausstanzen, was einem 1-Bit-Alpha-Kanal entspricht.\n"}
{"id": "3748177", "url": "https://de.wikipedia.org/wiki?curid=3748177", "title": "Presto (Film)", "text": "Presto (Film)\n\nPresto ist ein US-amerikanischer Animations-/Kurzfilm aus dem Jahr 2008, der im Kino als Vorfilm vor \"WALL·E – Der Letzte räumt die Erde auf\" gezeigt wurde.\n\nAm 27. Juni 2008 lief \"Presto\" zusammen mit seinem Hauptfilm in den US-amerikanischen Kinos an. Der Kinostart war in Deutschland und Österreich am 25. September 2008, in der Schweiz am 2. Oktober 2008.\n\nEin kleiner Hase namens \"Alec Azam\" (ein Wortspiel mit dem Zauberwort \"Alakasam\") sitzt hungrig in seinem Käfig in einer Bühnengarderobe und versucht eine in seiner Nähe liegende Mohrrübe zu ergattern. Da erscheint der Zauberer \"Presto DiGiotagione\" (ein Wortspiel mit „prestidigitation“ – zu deutsch „Fingerfertigkeit“ bzw. „Taschenspielertrick“).\n\nNach einem Blick auf seine Uhr nimmt er seine beiden Zauberhüte (die miteinander verbunden sind; was in den einen hinein gesteckt wird kommt aus dem anderen wieder heraus) sowie den Hasen und eilt auf die Bühne um seine Vorstellung zu beginnen. Der Hase wird dabei am Bühnenrand zurückgelassen. Der hungrige Hase erbittet dringend die Mohrrübe, die ihm der Zauberer jedoch verweigert. Daraufhin sabotiert der Hase die Vorführung des Zauberers, ohne ihn dabei gänzlich bloßzustellen. Das Publikum hält die Tricks für echt und applaudiert. Als der Hase dem Zauberer am Schluss mittels des Zauberhutes das Leben rettet, tobt das Publikum vor Begeisterung. Im Hintergrund erkennt man Waldorf und Statler aus der Muppet Show auf einem der Balkone. Der Zauberer erkennt seinen Fehler, gibt dem Hasen seine Möhre und lässt ihn zukünftig als gleichberechtigten Partner mit auftreten.\n\n\n"}
{"id": "3750670", "url": "https://de.wikipedia.org/wiki?curid=3750670", "title": "Tunebite", "text": "Tunebite\n\nTunebite ist eine Mediensoftware der in Karlsruhe beheimateten RapidSolution und war ursprünglich als Audiorekorder konzipiert, der sich inzwischen zu einem gegenüber den ersten Versionen der Software umfangreicheren Rekorder und Medienkonverter für Audio (Musik, Hörbuch etc.), Video (Filme, Clips etc.), Internetstreams usw. entwickelt hat.\n\nHeute kann Tunebite als Bestandteil von \"Audials One\" (wie auch der \"Radiotracker\" und \"Mediaraptor\") oder als eigenständige Software betrieben werden.\n\nSeit 2004 können Anwender von Tunebite durch das Abspielen von kopiergeschützten Inhalten und durch die gleichzeitige Wiederaufnahme auf urheberrechtlich einwandfreie Art und Weise Privatkopien für den Eigenbedarf aufnehmen. Seit 2007 bearbeitet Tunebite auch Videodateien.\n\nMit Audials Tunebite kann laut Hersteller Musik aus dem Internet, z. B. von Seiten wie Simfy oder Spotify aufgenommen werden. Die Software ist offenbar in der Lage, Musikstücke aus dem Stream zu erkennen, auszuschneiden und als MP3 zu speichern. Neben dem Mitschnitt von Musik können mit Hilfe der Software außerdem Videos und Filme von beliebigen Internetseiten aufgenommen und in verschiedenen Dateiformaten für sämtliche Geräte gespeichert werden.\n\nMit Tunebite können Audio- und Videodateien in eine Vielzahl an Dateiformaten umgewandelt werden. Unterstützt werden im Folgenden die Wichtigsten der 100 unterstützten Formate aufgezählt: 3GP, 3ivx, AAC, AC3, AIFF, ASV, AU, AVI, DIVX, DV, FLAC, FLV, H.264, M4B, M4P, M4V, MKV, MOV, MP2, MP3, MP4, MPEG, MPEG4, MPG, OGG, WAV, WebM, WMA, WMA Pro, WMV und XVID.\n\nTunebite hat bereits einige Auszeichnungen von deutschen und internationalen Fachzeitschriften erhalten. Nennenswert sind:\n\n\n"}
{"id": "3761797", "url": "https://de.wikipedia.org/wiki?curid=3761797", "title": "Das Berliner Regierungsviertel Teil 1: 1932–1938", "text": "Das Berliner Regierungsviertel Teil 1: 1932–1938\n\nDas Berliner Regierungsviertel ist der erste Teil einer Trilogie, die sich mit dem historischen Berliner Regierungsviertel bis zu seiner Zerstörung am Ende des Zweiten Weltkrieges befasst. Der Film ist komplett 3D-animiert und wurde in Südafrika produziert. Erschienen ist er in Deutschland am 29. September 2005. Die Veröffentlichung fand in Frankfurt (Oder) und in Berlin statt.\n\nDiese Dokumentation ist der erste Teil einer dreidimensionalen computeranimierten Filmtrilogie, die einen detaillierten Überblick über die Architektur und die Baumaßnahmen im Berliner Regierungsviertel in der Zeit zwischen 1932 und 1945 gibt. So werden durch geschickte Übergänge, bauliche Veränderungen aufgezeigt und räumliche Zusammenhänge vermittelt, wie es dem bloßen überlieferten Fotostudium nicht möglich wäre.\n\nEs werden im Film folgende Straßenzüge gezeigt: \n\nDes Weiteren wird auf folgende Gebäude in den drei Straßen/Plätzen besonders eingegangen: \n\n\nDie Musik zum Film wurde von Lorraine Shannon und Robert Schöder komponiert und aufgenommen. Beide sind Mitglieder von SAMRO, der South African Music Rights Organisation. Die Musik ist vordergründig Klaviermusik, welche von zahlreichen Streichern begleitet wird.\n\n\n\n\n\n"}
{"id": "3770132", "url": "https://de.wikipedia.org/wiki?curid=3770132", "title": "Axigen", "text": "Axigen\n\nAXIGEN ist ein kommerzieller Mailserver mit Groupware-Erweiterungen. Er bietet SMTP/ESMTP, IMAP, POP3, Webmail und Mailinglisten. AXIGEN ist eine Alternative zu Microsoft Exchange.\n\nAXIGEN sieht sich als Mailserver, der die Anforderung von KMU bis hin zum ISP-Einsatz abdeckt. AXIGEN kommt ohne zusätzliche Software aus und wird in einem Gesamtpaket mit wizard-gesteuerter Installation bereitgestellt.\n\nAXIGEN bietet ein mehrsprachiges Ajax-Webmail-Interface. Die Groupware- und Organizer-Funktionen (Kalender, Aufgaben, Notizen und Journal) sind sowohl über das Webmail-Interface als auch über einen speziellen Outlook Connector nutzbar. Für mobile Endgeräte wird eine eigene Weboberfläche nach dem XHTML-Standard angeboten. Zudem wurde die ActiveSync-Schnittstelle von Microsoft lizenziert, um Push E-Mail zu ermöglichen.\n\nAXIGEN unterstützt zahlreiche Plattformen, wie Linux, BSD, Solaris und Windows.\n\nAXIGEN ist in vier Editionen erhältlich: \n\nDie Version X/10 bringt eine verbesserte Weboberfläche für User und Administration, IPv6 Support, Multidomain SSL Support mit SNI und wird ausschließlich als 64bit Version ausgeliefert.\n\nDie Entwicklung des AXIGEN Mail Server begann 2003. Die erste Version wurde 2005 veröffentlicht. Seit 2006 ist das Produkt auch in Deutschland erhältlich. Nach Angaben von GeCad wird das Produkt weltweit in über 10.000 Firmen mit 6 Millionen Mailboxen eingesetzt.\n\n"}
{"id": "3770911", "url": "https://de.wikipedia.org/wiki?curid=3770911", "title": "Vyatta Open-Firmware-Router", "text": "Vyatta Open-Firmware-Router\n\nDie Vyatta Open-Router-Firmware (kurz: \"Vyatta-OFR\" oder \"OFR\") war eine auf die Funktionen Router/Firewall/Gateway-Sicherheit spezialisierte Linux-Distribution auf Debian-Basis.\n\n\"Vyatta OFR\" ist wie auch andere Routerdistributionen als eigenständiges Server-Betriebssystem zu verstehen. Es wird zunächst per Live-Boot-CD auf einem PC gestartet und kann später über die Konsolenschnittstelle auf z. B. Festplatte fest installiert werden. Nach wenigen Grundeinstellungen beginnt die Installation, welche die Partitionierung der Festplatte vornimmt und die Dateien überspielt. Zunächst ist eine relativ schlanke Konsolenversion verfügbar und das Netzwerk ist nicht konfiguriert. Um den Router ohne Tastatur und Monitor betreiben zu können, muss mindestens eine Netzwerkschnittstelle konfiguriert werden.\n\nDie Hauptaufgabe von Vyatta-OFR ist, die wesentlichen Merkmale eines Routers abzubilden. Durch die flexible Nutzung eines Debian als Betriebssystembasis können nahezu alle Features der x86-Architektur genutzt werden.\n\nDie Konfiguration der Routing-Engine erfolgt im Wesentlichen im Configure-Edit-Mode, in dem alle Änderungen in eine Config-Datei geschrieben werden. Die Besonderheit ist dabei, dass die Änderungen noch nicht sofort aktiv werden, sondern die neue Konfiguration erst mit einem dedizierten „Commit“-Befehl aktiviert wird, die zu commitende Konfiguration also vor dem Aktivieren noch mal kontrolliert werden kann. Anschließend muss ähnlich wie bei Cisco zusätzlich noch die Konfiguration auf dem permanenten Datenspeicher (z. B. Festplatte) geschrieben werden, damit die Konfiguration rebootfest ist. Neben dem Konfigurations-Modus ist das zugrundeliegende Debian-Linux voll zugreifbar. Zugriffe auf das System erfolgen über eine serielle Console, über den Konsolenbildschirm, über SSH, oder die ebenfalls enthaltene WebGUI.\n\nHinter OFR steht die amerikanische Vyatta Inc. aus dem kalifornischen Belmont und eine Community aus freiwilligen Entwicklern und Helfern. Das Geschäftsmodell von Vyatta besteht primär in der Bereitstellung von Support und von Schulungen zum Vyatta-OFR. Zusätzlich werden aktuell zwei Hardware-Router angeboten und es ist möglich Dell-Rechner über Vyatta als Router konfiguriert zu bestellen.\nMit Version 6.0 stieg Vyatta auf ein Core Modell um, wobei Vyatta Core der vorherigen Community Edition entspricht, Zusatzfunktionen können in Paketen zugekauft werden.\n\nVyatta Core ist unter die Lizenz GPL gestellt.\n\nDie Firma Vyatta Inc. wurde im Nov. 2012 durch Brocade Communications Systems übernommen.\n\nUm den Kern von Vyatta auf einer freien Basis auch ohne Mithilfe von Brocade weiterzuentwickeln fand sich eine Community zusammen, welche den Fork namens \"VyOS\" erschuf. Version 1.0, Codename Hydrogen, wurde am 22. Dez. 2013 veröffentlicht.\n\nDie Version 4.0 beinhaltet folgende Hauptfunktionen:\n\n\nIn Version 5 sind einige Neuerungen in die Distribution integriert worden:\n\n\n\n"}
{"id": "3773217", "url": "https://de.wikipedia.org/wiki?curid=3773217", "title": "Rubyripper", "text": "Rubyripper\n\nRubyripper ist ein freier CD-Ripper zum digitalen Auslesen von Audio-CDs („Rippen“ oder spezifischer DAE) für unixoide Betriebssysteme (getestet auf Linux, macOS und FreeBSD). Oberstes Ziel der Software sind möglichst fehlerfreie Ausleseergebnisse, um identische Kopien erstellen zu können, wobei die proprietäre Windows-Software Exact Audio Copy als Vorbild dient. Es ist das erste Programm seiner Art aus der Linux- und Freie-Software-Welt, das diesen Fokus hat und einen zuverlässigen Mechanismus zur Umgehung des bei vielen CD-Laufwerken anzutreffenden Lesepuffers hat.\n\nFür die Linux-Distribution Mageia ist Rubyripper in den offiziellen Paketquellen verfügbar. Für Debian und Arch Linux sind Community-Repositories oder Paketquellen von Drittanbietern verfügbar.\n\nDie ausgelesenen Daten können in RIFF WAVE und über entsprechende Encoder in den Formaten FLAC, Vorbis und MP3 (über LAME) oder auch über benutzerdefinierte andere Programme ausgegeben werden (auch mehrere gleichzeitig).\nCDs können auch in ein Abbild ausgelesen werden, also an einem Stück in einer großen Audio-Datei mit einem dazugehörigen Cuesheet, das die entsprechenden Metadaten aufnimmt.\nAußerdem können auch Titelinformationen von freedb übernommen (per cd-discid) und die Ausgabedateien nach einem Namensschema benannt und in Verzeichnisse eingeordnet werden. Des Weiteren können die erzeugten Dateien auch gleich mit Replay-Gain-Informationen versehen werden.\n\nDie Anwendung ist in der Programmiersprache Ruby geschrieben (daher der Name) mit einer GTK+-2-basierten grafischen Benutzeroberfläche, während jedoch auch eine Kommandozeilenschnittstelle und eine Programmierschnittstelle für andere Frontends zur Verfügung steht. Zum Lesen der CDs baut es wie praktisch alle anderen Linux-Ripper auch auf cdparanoia auf, setzt dieses jedoch auf eine ausgeklügelte Weise ein, die auch die Umgehung des Lesepuffers gewährleistet (was cdparanoia von sich aus lange Zeit nicht konnte): Um sichere Ergebnisse zu erhalten werden alle Daten mehrfach gelesen und anschließend miteinander verglichen, um problematische Stellen zu finden, die dann noch öfter gelesen werden, bis mindestens zwei (konfigurierbar) übereinstimmende Ergebnisse geliefert werden. Dabei werden die Daten nicht häppchenweise mehrfach gelesen, sondern läuft jeder Lesevorgang komplett durch, sodass praktisch jeder Laufwerks-Lesepuffer überlaufen muss und somit garantiert zweimal gelesen und nicht einfach der Inhalt des Lesepuffers zweimal ausgegeben wird. Problematische Stellen werden in einer ausführlichen Logdatei protokolliert.\n\nSeinen Anfang nahm die Geschichte des Rubyripper als Python-Programm im Oktober 2005. Für die Version 0.1 vom 29. Januar 2006 wurde das Programm in Ruby komplett neu geschrieben und erhielt seinen heutigen Namen.\nMit der Veröffentlichung von Version 10.2 des zugrundeliegenden cdparanoia am 11. September 2008 ist dieses selber in der Lage den Lesepuffer der Laufwerke zu umgehen.\n\nAm 10. März 2014 erklärte Hauptentwickler Bouke Woudstra, sich nicht mehr an Rubyripper beteiligen zu wollen, da er in Zeiten von Musikstreamingdiensten keinen Sinn mehr darin sähe, CDs zu kopieren. Das Projekt ist seitdem verwaist, da Woudstra zum Schluss der einzige noch verbliebene Entwickler war.\n\n"}
{"id": "3775821", "url": "https://de.wikipedia.org/wiki?curid=3775821", "title": "Universal Audio Architecture", "text": "Universal Audio Architecture\n\nDie Universelle Audio-Architektur (Universal Audio Architecture – UAA) geht auf eine Initiative von Microsoft aus dem Jahr 2002 zurück. Es wurde versucht, einen Standardisierungsansatz zu etablieren für die Hardware- und Treiber-Klassen-Architekturen der Audiogeräte in modernen Microsoft Windows-Betriebssystemen. Drei verschiedene Audio-Geräte-Klassen wurde standardmäßig unterstützt: USB, IEEE 1394 (auch als Apple FireWire oder Sony iLink bezeichnet) und Intel High Definition Audio, welches PCI und PCI-Express unterstützt.\n"}
{"id": "3778062", "url": "https://de.wikipedia.org/wiki?curid=3778062", "title": "Huygens (Supercomputer)", "text": "Huygens (Supercomputer)\n\nHuygens ist ein Supercomputer des Unternehmens SARA. Er wurde am 13. Juni 2007 durch den Bürgermeister Job Cohen im Science Park in Amsterdam in Betrieb genommen. Der Computer ist nach dem niederländischen Astronomen und Naturwissenschaftler Christiaan Huygens und dem niederländischen Dichter Constantijn Huygens benannt.\n\nHuygens besteht derzeit aus 120 1,9 GHz schnellen IBM Power 5+ Prozessoren mit jeweils 16 Kernen und erreicht eine Leistung von 14,6 TeraFlops. In seiner endgültigen Ausbaustufe soll er 60 TeraFlops leisten. Aktuell steht er damit auf Platz 315 der TOP500 (stand: Juni 2008).\n\nSeine Speicherkapazität beträgt derzeit 480 TB, bestehend aus einzelnen 300 GB Festplatten. Als Betriebssystem kommt SuSE Linux Version 9 for POWER5+ zum Einsatz.\n\n"}
{"id": "3780116", "url": "https://de.wikipedia.org/wiki?curid=3780116", "title": "Datapoint 2200", "text": "Datapoint 2200\n\nDas Datapoint 2200 war ein massenproduziertes, programmierbares Computerterminal, welches von Computer Terminal Corporation (CTC, später Datapoint Corporation) im Juni 1970 der Öffentlichkeit vorgestellt wurde (Auslieferung der ersten Geräte 1971). Es wurde von CTC als ein vielseitiges, kostengünstiges Terminal entworfen, welches durch das Laden verschiedener Terminal-Emulationen von Band an eine Vielzahl von Großrechnern angeschlossen werden konnte. Allerdings erkannten einige Geschäftskunden (z. B. Pillsbury Company), dass dieses sogenannte „programmierbare Terminal“ Aufgaben eines einfachen Computers übernehmen konnte, und benutzten daraufhin ihre Geräte als eigenständige Computersysteme. Bedeutsam ist die Tatsache, dass die Multi-Chip-CPU (Prozessor) des Terminals zum Vorfahren der x86-Prozessor-Architektur wurde, auf dem der ursprüngliche IBM PC und dessen Abkömmlinge basieren.\n\nDas Datapoint 2200 hatte eine eingebaute Tastatur, einen eingebauten Monochrom-Monitor (grün) mit 12 Zeilen zu 80 Spalten und zwei 47 Zeichen pro Zoll Kassettenlaufwerke mit jeweils 130 kB Kapazität. Die Größe von 24 × 47 × 50 cm und die Form (eine Kiste mit vorstehender Tastatur) entsprachen in etwa der IBM Selectric Kugelkopfschreibmaschine. Anfangs war das Terminal mit einem Diablo 2,5 MB 2315-Wechselrahmen-Festplattenlaufwerk zusammen mit Modems, verschiedenen Arten serieller Schnittstellen, paralleler Schnittstelle, Drucker und einem Lochkartenleser erhältlich. Später standen auch ein 8-Zoll-Diskettenlaufwerk und weitere, größere Festplatten zur Verfügung. Ein Industriestandard 7/9-Spur-(wählbar)-Magnetbandlaufwerk war ab 1975 erhältlich. Ende 1977 präsentierte Datapoint das ARCNET zur lokalen Vernetzung. Der ursprüngliche Typ 1 wurde mit 2 kB seriellem Schieberegister als Hauptspeicher ausgeliefert, der auf 8 kB erweitert werden konnte. Der Typ 2 verwendete dichtere 1 kBit RAM-Chips, so dass die Grundkonfiguration 4 kB Speicher entsprach und auf bis zu 16 kB erweiterbar war. Der Einstiegspreis lag bei rund 5.000 US-Dollar (entspricht 29.000 USD im Jahr 2014), und ein vollständiges 16 kB Typ 2 2200 hatte einen Listenpreis von knapp über 14.000 USD. Dem Modell 2200 folgten 5500, 1100, 6600, 3800, 8800 usw.\n\nAbgesehen davon, einer der ersten Personal Computer gewesen zu sein, hat das \"Datapoint 2200\" eine weitere Verbindung zur Computer-Geschichte. Das ursprüngliche Design verlangte nach einem Ein-Chip-8-Bit-Mikroprozessor als CPU, anstatt einem Prozessor aus diskreten TTL Modulen wie sie zu jener Zeit üblich waren. Im Jahr 1969 beauftragte CTC zwei Unternehmen, Intel und Texas Instruments (TI), damit, den Chip zu bauen. TI sah sich nicht in der Lage, einen zuverlässigen Chip zu fertigen, und trat vom Vertrag zurück. Intel wiederum konnte CTCs Frist nicht einhalten, worauf beide Unternehmen ihren Vertrag neu verhandelten, wodurch CTC sein Geld und Intel die Rechte am schließlich fertiggestellten Prozessor behielt.\n\nCTC fertigte das Datapoint 2200 mit ca. 100 TTL-Komponenten (SSI/MSI Chips) statt eines Mikroprozessors, während Intels Single-Chip-Design schließlich im April 1972 unter dem Namen Intel 8008 veröffentlicht wurde. Dennoch kommt der 8008-CPU wegweisende Bedeutung zu. Sie war der Urahn von Intels 8-Bit-CPU-Linie, dem die auf Assembler-Ebene kompatiblen 16-Bit-CPUs folgten — die ersten Mitglieder der x86-Familie, als der der Befehlssatz später bekannt wurde. Der 8008 selbst wurde in den ersten Mikrocomputern wie SCELBI, MCM/70 und Micral verwendet.\n\nDer ursprüngliche Befehlssatz wurde von Victor Arme und Harry Pyle entwickelt. Das letztlich verwendete TTL-Design wurde von Gary Asbell durchgeführt. Das Produktdesign (das Äußere inklusive Logo des Unternehmens) stammt von Jack Frassanito.\n\nHaupteinheit\n\n\nZusatzgeräte\n\nDie Benutzer des 2200 und der nachfolgenden Terminals konnten letztlich aus mehreren Zusatzgeräte wählen. Darunter waren\n\n"}
{"id": "3781654", "url": "https://de.wikipedia.org/wiki?curid=3781654", "title": "LXDE", "text": "LXDE\n\nLXDE ist eine freie Desktop-Umgebung für Unix und andere POSIX-konforme Plattformen, wie Linux oder BSD. Der Name LXDE steht für „Lightweight X11 Desktop Environment“.\n\nDas Projekt wurde im Jahr 2006 begonnen, um eine Desktop-Umgebung zu schaffen, welche schnell und energiesparend arbeitet. Sie eignet sich daher gut für den Einsatz auf Netbooks oder auf älteren PCs, bei denen die heute üblichen Leistungsanforderungen nicht erbracht werden können. Im Gegensatz zu anderen Desktop-Umgebungen wie KDE oder Gnome funktionieren die einzelnen Komponenten in LXDE mit nur wenigen Abhängigkeiten voneinander und können so einfach nach und nach auf andere Systeme übertragen werden.\n\nLXDE basiert wie Gnome 2, Xfce und ROX auf dem GTK+2-Toolkit und ist in der Programmiersprache C geschrieben. Der Quelltext wird unter der freien Lizenz GPL und teilweise unter der LGPL veröffentlicht. Es werden Computerarchitekturen wie Intel, MIPS und ARM unterstützt.\nEs läuft auch unter den Betriebssystemen OpenSolaris und BSD. LXDE ist zu den Standards von freedesktop.org konform und kann daher auch mit GNOME- und KDE-Programmen umgehen – und umgekehrt. LXDE ist unter anderem die Standard Desktop-Umgebung bei der Live-Distribution Knoppix und Raspbian, welche alle auf Debian GNU/Linux beruhen. LXDE unterstützt derzeit über 35 Sprachen. Es kann im Aussehen beim Grafikthema, Schriftstil und bei Icons angepasst werden und bietet einen Microsoft Windows ähnlichen Dienstprogramm-Starter. Es ist energiesparender als KDE, Gnome und Xfce und benötigt keine 3D Beschleunigung. Die Hardwareanforderungen entsprechen denen eines alten Windows 98 PCs, eine Pentium II CPU ist ausreichend.\n\nDas Projekt wurde 2005 von dem Taiwaner Hong Jen Yee (PCMan) gestartet und brachte 2006 seine erste Veröffentlichung hervor.\nNachdem LXDE in Mandriva und Fedora aufgenommen wurde und auch in Debian als Standard-Desktop zur Verfügung steht, gibt es Projekte wie U-Lite (ehemals Ubuntulite) und Linux Mint LXDE. Im Februar 2009 wurde das Projekt Lubuntu ins Leben gerufen, welches auf Ubuntu basiert. Im September desselben Jahres entstand die erste Testversion. Auf dem Ubuntu Developer Summit Mitte Mai 2011 wurde bekanntgegeben, dass Lubuntu ein offizieller Abkömmling von Ubuntu wird.\nMit der Veröffentlichung von openSUSE 11.3 ist LXDE auch dort als offiziell unterstützte und wählbare Desktopumgebung verfügbar. In der Folgezeit wurde LXDE in einer Vielzahl weiterer Linux-Distributionen integriert.\n\nEnde Juli 2011 wurde das GUI-Toolkit GTK+ 3 eingeführt, das zum Nachfolger von GTK+ 2 erklärt wurde, zugleich aber nicht abwärtskompatibel zu GTK+ 2 ist. Monatelang vorher war bereits auf die Einführung von Gtk+ 3 hingearbeitet worden. Für den Chefentwickler des LXDE-Projekts, Hong Jen Yee, stellte sich damit die Frage, welches Toolkit das LXDE-Team in Zukunft verwenden wolle, da LXDE bis dahin auf dem Widget-Set GTK+ 2 basierte, welches fortan nicht mehr weiterentwickelt werden sollte. Yee stellte in einem Blog-Posting fest, dass GTK+ 3 „speicherintensiver und langsamer“ als GTK+ 2 sei, was seinem Projekt-Konzept von einer „schlanken, ressourcenschonenden“ Desktop-Umgebung widersprach.\n\nEr kündigte eine Qt-Version von LXDE an, deren Fertigstellung für Anfang Juli 2013 geplant war. Jerôme Leclanche, ein wichtiger Entwickler der Desktopumgebung Razor-qt, und Yee beschlossen, die Qt-Version von LXDE und Razor-qt zusammenzuführen. Der bisherige LXDE-Code, welcher auf GTK+ 2 basierte, solle parallel dazu noch eine Weile gewartet werden, so der Beschluss. Diese Aufgabe wurde an einen extra dafür ausersehenen Maintainer delegiert.\n\nIm Mai 2014 erschien eine erste Version (0.7.0) des auf Qt basierenden LXDE, nun „LXQt“ genannt. Sie wurde in Zusammenarbeit mit dem Maui Project entwickelt. Neben dem Quelltext werden Pakete unter anderem für Debian, Fedora, openSUSE, Arch Linux und Ubuntu angeboten.\n\nVersion 0.8.0 brachte volle Kompatibilität mit Qt5. Dazu wurden neue Bibliotheken der KDE Frameworks integriert, um bisherige selbst programmierte Bibliotheken mit weniger Funktionen zu ersetzen. Das neue \"lxqt-admin\" enthält Admintools, beispielsweise zum Konfigurieren von Datum und Zeit sowie zum Verwalten von Benutzern und Gruppen. Der Bildschirm wird nun mit \"lxqt-config-monitor\" anstelle von \"lxqt-config-randr\" angepasst. Der Dateimanager \"PCmanFm-Qt\" sowie das \"LxQt-Panel\" wurden verbessert.\n\nLXDE hat folgenden minimalen Hardwarebedarf:\n\nWie ROX oder Xfce ist auch LXDE modular aufgebaut, die folgenden Komponenten bietet LXDE an:\n\nBei einigen Komponenten von LXDE ist deren Weiterentwicklung unklar, beziehungsweise sind diese im Begriff, wieder aufgegeben zu werden:\n\n\nUnfertig verworfen wurde das Netzwerkmanagementwerkzeug LXNM.\n\nSystemdienste, die eine LXDE-spezifische Rolle spielen, sind:\n\nBei den folgenden Distributionen ist LXDE in den offiziellen Paketquellen oder als Standard-Benutzeroberfläche enthalten:\n\n"}
{"id": "3791682", "url": "https://de.wikipedia.org/wiki?curid=3791682", "title": "Z-Ordnung", "text": "Z-Ordnung\n\nZ-Ordnung (, auch \"Z-Reihenfolge\") ist die Ordnung überlappender zweidimensionaler Objekte, wie Fenstern in grafischen Benutzeroberflächen oder Formen in Vektorgrafikprogrammen. Eine der Eigenschaften typischer Benutzeroberflächen ist, dass Fenster einander überlappen können. Die Z-Ordnung legt dann fest, welches über dem anderen erscheint.\n\nDer Begriff „Z-Ordnung“ bezieht sich auf die Reihenfolge von Objekten entlang der Z-Achse. In der analytischen Geometrie bezieht sich „X“ üblicherweise auf die waagerechte Achse („links oder rechts“), „Y“ auf die senkrechte Achse („unten oder oben“) und Z auf die lotrecht auf den anderen beiden stehende Achse („vorne oder hinten“). Die Fenster einer Benutzeroberfläche lassen sich als eine Reihe von Flächen vorstellen, die parallel zur Oberfläche des Bildschirms liegen. Die Fenster sind dann entlang der Z-Achse gestapelt und die Z-Ordnung legt die Sortierung der Fenster von vorne nach hinten fest.\n\nDreidimensionale Objekte können im Allgemeinen nicht auf diese Art sortiert werden. Beispielsweise ist es möglich, drei Bleistifte so auf einem Tisch anzuordnen, dass die Spitze jedes Stiftes auf dem hinteren Ende eines anderen Stiftes liegt. Keiner der Stifte ist dann über den anderen, weil sie einander alle überlappen. Dies kann mit Fenstern in einer Benutzeroberfläche nicht passieren, weil jedes Fenster parallel zum Bildschirm ist und somit alle seine Punkte die gleiche Tiefe haben. Die Fenster können also in einer definierten Reihenfolge gestapelt werden.\n\nÜblicherweise können Benutzer einer grafischen Benutzeroberfläche die Z-Ordnung dadurch beeinflussen, dass sie ein Fenster auswählen um es in den Vordergrund zu bringen (also „über“ oder „vor“ alle anderen Fenster). Einige Fenstermanager erlauben auch eine Wechselwirkung mit Fenstern die nicht im Vordergrund sind, während andere ein Fenster immer in den Vordergrund bringen, wenn es eine Benutzereingabe empfängt. Spezielle Fenster können als „immer oben“ markiert werden. Diese sind dann an der Spitze der Z-Ordnung befestigt, sodass (mit wenigen Ausnahmen) kein anderes Fenster sie überlagern kann.\n\nBei der Bearbeitung sichtbarer Objekte auf einem Computerbildschirm würde ein Objekt mit einer Z-Ordnung von 1 unterhalb eines Objekts mit einer Z-Ordnung von 2 oder mehr liegen. Dies entspricht dem Erzeugen von Schichten von Objekten, wobei die Z-Ordnung festlegt, welches Objekt über einem anderen liegt. Eine HTML-Seite kann die CSS-Eigenschaft „z-index“ im Div-Element verwenden, um die Z-Ordnung festzulegen, sodass Objekte über andere gelegt werden können.\n\n"}
{"id": "3796486", "url": "https://de.wikipedia.org/wiki?curid=3796486", "title": "Bornsche Näherung", "text": "Bornsche Näherung\n\nIn der Störungstheorie der Streuung von Wellen speziell in der Quantenmechanik wird die niedrigste Näherung in der Störungsreihe als Bornsche Näherung bezeichnet. Sie wird aber nicht nur in der Quantenmechanik, sondern z. B. auch in der Theorie der Streuung elektromagnetischer Wellen verwendet. Sie ist nach Max Born benannt, der sie in seinem Aufsatz \"Zur Quantenmechanik der Stoßvorgänge\" benutzte.\n\nAnschaulich kann man sich die Bornsche Näherung am Beispiel der Streuung von Radarwellen an einem Plastikstab vorstellen. Man nimmt dazu an, dass die durch das äußere Feld polarisierten Atome im Plastikstab (die als kleine Sender zum Gesamtfeld beitragen) im Takt des äußeren Treiberfeldes der einfallenden Radarwellen schwingen.\n\nDass die Atome dabei selbst elektromagnetische Wellen-Felder erzeugen, die wiederum die anderen Atome beeinflussen (Mehrfachstreuung), wird in dieser Näherung vernachlässigt. Dementsprechend gilt die Bornsche Näherung als gute Näherung, wenn das Streupotential klein ist im Vergleich zur Energie des einfallenden Wellenfeldes und damit das an einem einzigen Atom gestreute Feld klein im Vergleich zum einfallenden Feld.\n\nDie Lippmann-Schwinger-Gleichung für den Streuungs-Zustand formula_1 mit Impuls formula_2 und aus- oder einlaufender Richtung (formula_3) lautet:\n\nmit\n\nDiese Gleichung kann im Sinne der Bornschen Näherung vereinfacht werden zu\n\nsodass die rechte Seite nicht mehr vom unbekannten Zustand formula_1 abhängt.\n\nFür die explizite Form in Ortsdarstellung siehe Lippmann-Schwinger-Gleichung.\n\nManchmal wird ein Teil A des Streuprozesses getrennt auf analytischem oder numerischem Weg berechnet, und die Streuung an einem Rest-Potential (Teil B), das als Störung in Bornnäherung behandelt wird, hinzuaddiert. In diesem Fall werden die „\"gestörten\"“ (distorted) Wellen – im Gegensatz zu den in der üblichen Anwendung der Bornnäherung verwendeten ebenen oder Kugelwellen – aus Teil A als Ausgangswellenfunktionen für die Störungsentwicklung von Teil B genommen. Man spricht von \"Distorted Wave Born Approximation\" oder DWBA.\n\nIst formula_12 das Potential von Teil A , formula_13 das Potential von Teil B und formula_14 die Lösung des Streuproblems aus Teil A (mit der auch die Greensfunktion formula_15 berechnet wird), so ergibt sich die DWBA-Lösung aus:\n\nBeispielsweise können bei einigen Problemen der Streuung von geladenen Teilchen an anderen geladenen Teilchen (wie bei Bremsstrahlung oder dem photoelektrischen Effekt) als Ansatz für Teil A analytische Lösungen für Coulomb-Streuung (Streuung in einem Coulombpotential) gewählt werden, die dann als einfallende Welle in die Bornnäherung von Teil B einfließen. Bei einigen Kernreaktionen wird z. B. häufig die numerisch berechnete Streuung in einem optischen Potential für den Teil A gewählt.\n\n\nLehrbücher der Quantenmechanik wie\n"}
{"id": "3798418", "url": "https://de.wikipedia.org/wiki?curid=3798418", "title": "TortoiseSVN", "text": "TortoiseSVN\n\nTortoiseSVN (von tortoise [] \"englisch\": Landschildkröte, abgekürzt tsvn) ist ein freier Client für den Versionsverwaltungs-Dienst Subversion. Es steht unter der GNU General Public License und verwendet im Logo eine Schildkröte.\n\nTortoiseSVN ist als Shell-Erweiterung implementiert,\nes integriert sich in den Windows-Explorer und ist daher außerhalb und unabhängig von einer integrierten Entwicklungsumgebung verwendbar.\n\nAls Kernaufgabe wird für die Software die Versions-, Revisions- und Sourcekontrolle beschrieben. Für die Benutzung wird ein Subversion-Server empfohlen, es ist aber auch möglich, lokal auf Repositories zuzugreifen.\n\nDas Projekt wurde 2002 von Tim Kemp als Fork von \"TortoiseCVS\" begonnen. Sourceforge verzeichnete im August 2008 mehr als sieben Millionen Downloads der Software. Die Entwickler sprechen in ihrem Blog von über neun Millionen Downloads aller Pakete seit Projektstart bis April 2008. Im Jahr 2007 hat das Projekt den \"SourceForge.net 2007 Community Choice Award for Best Tool or Utility for Developers\" Award gewonnen.\n\nTortoise benutzt als grafische Oberfläche den Windows-Explorer. Eine Erweiterung der Kommandoliste in den Kontextmenüs dient dabei als Benutzerschnittstelle. Die Status werden direkt im Explorer durch Overlay-Icons angezeigt.\n\nDie Verwaltung der Arbeitskopie durch TortoiseSVN orientiert sich am Subversion Client-Programm der passenden Version (zum Beispiel svn 1.5.* vs. tsvn 1.5.*); so kann bedarfsweise zwischen beiden gewechselt werden.\n\nDie Software bietet für den Einsatz mehrere Hilfsmittel an. So können über das Merge-Tool nicht nur ASCII-basierte Dateien verglichen werden, es unterstützt auch den Vergleich proprietärer Formate wie z. B. dem von Word. Durch IDiff können Änderungen an Bilddateien nachvollzogen werden. In Verbindung mit Trackingsystemen können über den Issuetracker auch Logs oder Bugtracker verwendet werden.\n\nDie Software ist zurzeit in 52 Sprachen verfügbar. Davon sind 32 zu mindestens 85 % übersetzt.\n\nFür die Versionskontrolle stehen unter anderem eine Komplettversionierung von Verzeichnissen, atomare Übertragungen, Metadatenversionierung und Verzweigungsoptionen zur Verfügung.\n\nTortoiseSVN unterstützt die Protokolle HTTP, HTTPS, \"SVN\" und \"SVN\" + SSH. Alternativ kann auch direkt mit Dateien gearbeitet werden.\n\n\n"}
{"id": "3798494", "url": "https://de.wikipedia.org/wiki?curid=3798494", "title": "Silkypix", "text": "Silkypix\n\nSilkypix Developer Studio ist ein RAW-Konverter für digitale Fotos. Ähnlich wie Adobe Photoshop Lightroom sind verschiedene Einstellungen, vom Weißabgleich bis zur Korrektur von Objektivfehlern wie etwa chromatischen Aberrationen möglich.\n\nAuf Silkypix-Software basieren auch die Raw-Entwicklungswerkzeuge mehrerer Hersteller von Digitalkameras, wie Casio, Panasonic, Pentax/Ricoh und Samsung.\n\n"}
{"id": "3801491", "url": "https://de.wikipedia.org/wiki?curid=3801491", "title": "Zoner Photo Studio", "text": "Zoner Photo Studio\n\nZoner Photo Studio ist eine Grafiksoftware, die von der tschechischen Firma Zoner Software entwickelt wurde. Das Bildbearbeitungs- und Verwaltungsprogramm ist in Tschechien eines der gebräuchlichsten Programme zur Bearbeitung digitaler Fotos und wird weltweit verwendet. Es ist für die Betriebssysteme Windows und Android erhältlich. \n\nIm Jahr 1993 wurde das Unternehmen Zoner Inc. unter dem damaligen Namen \"Zoner Software\" von einer kleinen Gruppe von Fotografie-Liebhabern gegründet. Die Software wurde ursprünglich unter dem Namen \"Zoner Media Explorer\" veröffentlicht. Im Jahr 2004 wurde sie in \"Zoner Photo Studio\" umbenannt, da sich das Hauptaugenmerk von diesem Zeitpunkt an auf die digitale Fotografie richtete. Jedes Jahr wird eine neue Version der Software veröffentlicht.\n\nIn der Version 12 stellte Zoner Photo Studio die Trennung des Programms in die verschiedenen Module \"Manager\", \"Viewer\", \"Editor\" und \"RAW\" vor, um ein effektives, gleichzeitiges Arbeiten in der Bildverwaltung und -bearbeitung zu ermöglichen. Mit dieser Version wurde ebenfalls die anthrazitfarbene Oberfläche eingeführt, die dem Anwender ein ermüdungsfreies Arbeiten ermöglichen soll. Mit Version 13 wurde die Unterstützung für Dual-Monitore sowie eine 64-Bit-Version für Windows eingeführt. In Version 14 stellten die Entwickler die Onlinegalerie Zonerama als auch die GPU-Beschleunigung durch CUDA und OpenCL vor.\n\nMit der Version 15 wurde das neue Importmodul vorgestellt. Version 16 brachte den endgültigen Wechsel zur Seitenleiste im Editor. In der Version 17 wurde das RAW-Modul komplett überarbeitet und dem Katalog wurde im Manager eine zentrale Position zugewiesen. Die Version 18 hat eine vereinfachte und übersichtlichere Benutzeroberfläche, die mehr Platz für die Fotoansicht bietet. In dieser Version gibt es drei Module mit unifizierten Bearbeitungsprozessen: Manager, Entwickeln und Editor. ZPS 18 bietet nichtdestruktive Bildbearbeitung und einen geänderten Katalog.\n\nZoner bietet als Komplettpaket Werkzeuge für den gesamten fotografischen Workflow, nicht nur für die Bildbearbeitung. Die Oberfläche ist in sogenannte Module aufgeteilt, die jeweils eine Hauptaufgabe (Bearbeitung oder Verwaltung) innerhalb des Workflows übernehmen.\n\nDas Managermodul ist für die Bildverwaltung zuständig. Es zeigt sogenannte Metadaten an, d. h. Bildinformationen, die in den Standards Exif, IPTC, und/oder XMP gespeichert werden. Diese Metadaten können Bildtitel, Beschreibungen, Infos über den Autor, Schlüsselwörter, Audiokommentare, digitale Signaturen, GPS-Daten, Bewertungen, farbige Markierungen etc. enthalten. Im Manager können Metadaten bearbeitet, verwaltet und zum Filtern und Sortieren genutzt werden. Das Modul unterstützt die Stapelverarbeitung. Das Programm arbeitet direkt mit den Bilddateien auf der Festplatte; diese werden dabei nicht zunächst importiert und später exportiert. Die im Programm enthaltene Katalogdatenbank fungiert als Speicher für die Schnellsuche von Metadaten. Der Manager bietet mehrere verschiedene Ansichten neben der Standard-Browseransicht: Vorschau, Karte und Vergleich.\n\nDer Editor ist für die Bildbearbeitung vorgesehen. Es handelt sich dabei um einen klassischen Bitmap-Editor für Retusche-Arbeiten ohne eine vollständige Ebenenfunktion (enthalten sind lediglich temporäre Ebenen, die nach der Anwendung eingebettet werden). Der Editor unterstützt außerdem verschiedene Auswahlwerkzeuge und -masken.\n\nDas Modul Entwickeln ermöglicht die Umwandlung von Dateien im RAW-Format in Standard-Bitmap-Formate. Der eingebettete Standard unterstützt dabei nicht alle aktuellen Kameras, jedoch kann der kostenlose DNG-Converter zusätzlich integriert werden, mit dessen Hilfe so gut wie alle gängigen RAW-Formate umgewandelt werden können. Die Version 18 bietet zusätzlich nichtdestruktive Bildbearbeitung von allen anderen üblichen Bildformaten (JPEG inklusive).\n\nZoner Photo Studio 18 wurde im Oktober 2015 veröffentlicht.\n\nEine neue Benutzeroberfläche soll die Bedienbarkeit erhöhen und mehr Raum für die Fotoansicht geben.\n\nDen Mittelpunkt des Programms bildet der Katalog mit Sortierfunktionen, anhand welcher die Fotos nach Ordnern, Zeit und nach Schlüsselwörtern gegliedert können werden. Neu ist auch das Feld für die vereinfachte Suchfunktion, die dem Nutzer bereits ausgefüllte Informationen automatisch vorschlägt. Suchbar sind auch die Aufnahmeorte. Mit der neuen Funktion „Ortung bestimmen“ werden GPS-Koordinaten geografische Namen zugeordnet.\n\nDas neue Modul Entwickeln, das das Modul RAW ersetzt, arbeitet mit allen Bildformaten einschließlich RAW. Die Bearbeitung der Fotos in diesem Modul wirkt sich nichtdestruktiv aus – man kann alle Einstellungen beliebig ändern, ohne dabei das Originalbild zu überschreiben. Generell wurden alle Bearbeitungsprozesse stark beschleunigt, in manchen Fällen sogar um mehrere 100 Prozent. Einige Effekte sollen im Ergebnis verglichen mit den Vorgängerversionen deutlich natürlicher wirken.\n\nZPS 18 stellt einen neuen, eigenständigen und ins Windowssystem integrierten Browser zur Verfügung. \n\n\nZoner Photo Studio 17 wurde im September 2014 veröffentlicht.\n\nIn dieser Version wurde die Möglichkeit, Schlüsselwörter bereits während des Imports zuzuordnen und Bilder umzubenennen, integriert.\n\nIm Manager wurde die Seitenleiste erweitert und enthält nun eine kleine Bildvorschau. Bildbewertungen sind nun in Sternen und nicht mehr in Nummern verfügbar; außerdem wurde die Möglichkeit integriert, Bildern per Stapelverarbeitung in der Seitenleiste Schlüsselwörter zuzuordnen. Der Katalog indexiert Fotos nun im Hintergrund und verlangsamt die Arbeit nicht mehr. Eine Auto-Indexierung der Windows-Bibliothek für Bilder wurde integriert und dem Katalog wurde ein prominenterer Platz in der Ordnerstruktur auf der linken Seite zugewiesen. Die Katalogansicht beinhaltet ab sofort die Kontrolle über DLNA-fähige Geräte. Außerdem wurde in Version 17 ein neues Werkzeug zum Auffinden und Löschen von Duplikaten integriert. \n\nDie Oberfläche des RAW-Moduls wurde komplett überarbeitet, wobei einige Elemente hinzugefügt und einige entfernt wurden. Der „Automatisch“-Button, der automatische Vorschläge zu verschiedenen Einstellungen anbietet, wurde integriert. Als wichtigstes neues Einstellungswerkzeug wurden die Histogrammkurven vorgestellt. Die Miniaturen im Manager reflektieren nun sofort jede Änderung an der Datei im RAW-Modul. Die Unterstützung für eine Korrektur von Objektivfehlern durch die Anwendung von LCP-Profilen während der RAW-Entwicklung wurde implementiert.\n\nDiese Version wurde im Oktober 2013 veröffentlicht.\n\nVersion 16 verbessert die Funktionalität des Programms auf Tablet-PCs und Touch-Bildschirmen.\n\nIn dieser Version wurde die Ansicht des Managers in verschiedene Bereiche aufgeteilt – den traditionellen Browser, die „Vorschau“ für eine schnelle Vollbildansicht, die „Karte“ für die Arbeit mit GPS-Koordinaten sowie den „Vergleich“ für ein visuelles Vergleichen von Bildern nebeneinander. In der Version 16 wurde außerdem eine Miniaturkarte in der Seitenleiste des Managers integriert, mit deren Hilfe GPS-Koordinaten per Drag-and-Drop zugeordnet werden können.\n\nIn dieser Version wurde die vertikale Werkzeugleiste von der rechten auf die linke Seite verlegt und alle Bearbeitungsfunktionen können nun in der Seitenleiste erledigt werden. Im Editor wurde die Funktion „Inhaltsbasierte Skalierung“ integriert, bei der die Größe zentraler Inhalte während der Skalierung erhalten bleibt.\n\nZoner Photo Studio 15 wurde im Oktober 2012 veröffentlicht.\n\nDer Import von Fotos wurde in der Version 15 komplett neu organisiert.\n\nDie Seitenleiste im Editor wurde in dieser Version erstmals vorgestellt; sie ersetzt dabei die grundlegenden Bearbeitungsdialogfunktionen (z. B. Farben, Helligkeit, Weißabgleich usw.). In der Version 15 ersetzt die Vollbild-Vorschau außerdem die kleinen Vorschaubilder in den meisten Bearbeitungsfenstern. Außerdem wurden die „Schnellfilter“ integriert, die nun eine schnelle Anwendung verschiedener Funktionen für den besonderen Look garantieren (z. B. „Polaroid“, „Cross Process“ oder „Lomo“).\n\nJedes Foto wird automatisch zum Zeitpunkt der ersten Änderung durch den Benutzer in einer lokalen Datenbank gesichert. Damit kann das Originalbild nach Bedarf wiederhergestellt werden. Diese Funktion ist in der Grundeinstellung aktiviert, aber optional abschaltbar.\n\nDieses Tool synchronisiert Bildkollektionen aus verschiedenen Quellen.\n\nDiese Funktion simuliert ein Tilt-Shift-Objektiv, das den Hintergrund eines Fotos weichzeichnet (mit verschiedenen Einstellungen bezüglich des Orts und der Stärke der Weichzeichnung).\n\nZonerama ist eine Online-Galerie, die von den Entwicklern von Zoner Photo Studio bereitgestellt wird. Der Dienst kann auch in Anspruch genommen werden, ohne die Software erworben zu haben. Alben können dabei mit verschiedenen Einstellungen zur Privatsphäre, Sichtbarkeit und vielen weiteren Optionen präsentiert werden. Version 15 stellt erstmals die Integration des Services innerhalb des Programms vor, wobei die Integration von Version 15 inzwischen nochmals überarbeitet wurde. Sie ermöglicht die Bearbeitung der auf Zonerama vorgestellten Bilder in Zoner Photo Studio.\n\nDiese Systemvoraussetzungen sind für die letzte Version (Zoner Photo Studio 18) gültig.\n\n"}
{"id": "3805641", "url": "https://de.wikipedia.org/wiki?curid=3805641", "title": "ModBook", "text": "ModBook\n\nDas ModBook ist ein zum Tablet-PC umgebautes MacBook, welches bei der Firma Axiotron erhältlich ist. Über seinen multifunktionellen Touchscreen kann per zugehörigem Stift direkt gezeichnet, geschrieben und das Menü bedient werden.\n\nDas Modbook ist aus dem MacBook in der mittleren Ausstattungsvariante hergestellt und benutzt Technologien von Wacom und Axiotron. Durch die Verwendung von Wacom Technologie wird auf das in Mac OS X integrierte Handschriftenerkennungssystem Inkwell zugegriffen.\n\n"}
{"id": "3809337", "url": "https://de.wikipedia.org/wiki?curid=3809337", "title": "Prosodieerkennung", "text": "Prosodieerkennung\n\nDie Prosodieerkennung (auch Prosodieklassifikation) ist ein Teilgebiet der automatischen Mustererkennung bzw. der Musterklassifikation. Die zu klassifizierenden Muster stellen prosodische Eigenschaften der Sprache dar. Daher findet auch oft eine Klassifikation prosodischer Merkmale in Kombination mit Spracherkennung statt.\n\n\n\n\nDiese Merkmale werden häufig auf linguistische Modelle der Prosodie, insbesondere der Intonation, abgebildet, denn nur diese ermöglichen Aussagen über die Bedeutung der Messungen. Anders gesagt, sie liefern die Klassen, welche für eine Mustererkennung und Musteranalyse benötigt werden.\n\n\"Jitter\" und \"Shimmer\", bekannt aus der Mikroprosodie, erzeugen Unregelmäßigkeiten in der Amplitude und der Frequenz und müssen vor einer automatischen Klassifikation (z. B. der Intonation) aus dem Sprachsignal entfernt werden. Dies kann durch eine Glättung geschehen, indem das diskret abgetastete Sprachsignal mit einem Medianfilter geglättet wird.\n\nPlosive erzeugen einen kurzzeitigen Glottisverschluss. Während dieser Zeit schwingen die Stimmbänder nicht und es entsteht somit auch keine messbare Grundfrequenz. Somit finden sich kleine Lücken in der Abtastung, an welchen keine Information vorliegt. Dies kann einen Intonationsklassifikator dazu verleiten, in eine falsche Kategorie zu klassifizieren. Eine Interpolation kann die korrekte Erkennung verbessern.\n\nDer Intonation entspricht in etwa auf akustischer Ebene die Grundfrequenz. Diese kann mit sogenannten Pitchtrackern (das Programm Praat enthält beispielsweise eine Pitchtrackingfunktion) automatisch aus einem Audiosignal extrahiert werden. Es entstehen Serien von Grundfrequenzwerten. Diese diskreten Wertereihen können nach einer Interpolation und einer Medianglättung durch Polynome, zum Beispiel Geradenstücke, mittels Regressionsanalyse approximiert werden. Durch mehrere mehr oder weniger kleine Geradestücke kann der Verlauf der Grundfrequenz dann modelliert werden. Aus dieser angenäherten Betonungskurve der Äußerung können nun Rückschlüsse gezogen werden auf besondere prosodische Ereignisse, zum Beispiel können stark ansteigende Geradenstücke auf einen Gipfel in der Kontur hindeuten, also ein akzentuiertes Wort. Dies kann dem Dialogverständnis eines Roboters nützen, denn die reine Spracherkennung liefert keine Akzentinformation.\n\nDie Veränderungen in den suprasegmentalen Eigenschaften der Sprache werden dazu eingesetzt, Gefühlszustände aus dem Sprachsignal „abzulesen“. Erregte Menschen sprechen schneller, wütende Menschen sprechen lauter, verängstigte Menschen dagegen eher leiser. Traurige Menschen sprechen langsamer und langgezogener.\nDamit Roboter Mehrdeutigkeiten in verschiedenen, linguistischen Ebenen auflösen können, kann eine Prosodieerkennung eingesetzt werden. Dies verbessert die Leistung der Spracherkennung und steigert die Akzeptanz des Roboters als Gesprächs- oder Interaktionspartner in der Mensch-Maschine-Kommunikation. Auch erscheint ein Roboter menschlicher, wenn er die emotionalen Merkmale der Stimme einsetzen kann, um seine eigene Stimme in passender Weise zu verändern (Mitleidige Stimme bei traurig klingenden Menschen, freudige Stimme bei glücklichen Menschen) oder um seine Mimik den Emotionen anzupassen. Ebenso verbessert eine Erkennung von Ironie oder Humor die Akzeptanz als natürlichen Interaktionspartner.\n\nSprachverstehende Systeme gibt es (außerhalb der Robotik) viele, in Navigationsgeräten, Diktiergeräten, als alternatives Steuerungsgerät von Computern (z. B. Spracherkennung in Windows Vista) oder in automatischen, telefonischen Auskunftsystemen. Der Einsatz von Prosodieerkennung kann dort ebenfalls die Spracherkennung verbessern, indem Mehrdeutigkeiten (z. B. durch elliptische Sätze) oder Referenzen auf bestimmte Satzteile aufgelöst werden. Ebenfalls können Zitate mitten im Satz besser erkannt werden („Wie der Professor es in ‚Die Geschichte der Wikinger‘ erwähnte“: Eigentlich kein gültiger grammatikalischer Satz, es sei denn, man erkennt ‚Die Geschichte der Wikinger‘ als ein Zitat bzw. als zitierten Titel eines Buches).\n\nUnter anderem werden Prosodieerkennungsmodule in der Logopädie eingesetzt, um Sprachstörungen gezielt zu messen und zu behandeln.\n\nUm zu erkennen, welcher Sprecher bei vielen gleichzeitig sprechenden Menschen was gesagt hat, muss die Stimme des Sprechers genau von den Stimmen anderer Sprecher unterschieden werden können. Dabei können typische Merkmale wie Grundfrequenz, mittlere Sprechgeschwindigkeit, etc. helfen, aber auch Merkmale der Mikroprosodie, beispielsweise \"jitter\" und \"shimmer\", welche bei jedem Menschen in unterschiedlicher und charakteristischer Weise ausgeprägt sind. Das Problem, eine von vielen Stimmen zu verfolgen, tritt häufig bei Diktiersystemen auf, welche in Firmenbesprechungen oder Meetings eingesetzt werden, um das komplette Gespräch wortgetreu in Text zu übersetzen. Menschen können sich leicht auf eine von vielen gleichzeitig redenden Stimmen konzentrieren, automatischen Systemen fällt dies jedoch sehr schwer. Dieses Problem ist unter anderem als der Cocktailparty-Effekt bekannt und es existieren immer noch keine optimalen Lösungen.\n\nIn Hochsicherheitsbereichen wie in Forschungszentren dürfen nur autorisierte Mitarbeiter bestimmte Bereiche betreten. Um dies zu gewährleisten, werden häufig neben biometrischen Merkmalen auch prosodische und mikroprosodische Merkmale zur Verifikation eingesetzt. Häufig ist dies eine Passphrase.\n\nUm automatisch zu erkennen, welche Sprache ein Sprecher spricht, können neben Merkmalen der Spracherkennung auch Merkmale der Prosodie eingesetzt werden (siehe B-Prosodie). So besitzt jede Sprache einen typischen Klang, eine typische Folge von häufigen Lautkombinationen oder gar charakteristische Laute (z. B. kehlige Laute im Arabischen).\n\nIn der maschinellen Übersetzung werden Prosodiemodule zur Verbesserung der Spracherkennung und zur Auflösung von syntaktischen, semantischen und pragmatischen Mehrdeutigkeiten benutzt, um adäquat in die Zielsprache übersetzen zu können. Gutes Beispiel ist das Verbmobil Projekt.\n\n\n"}
{"id": "3809635", "url": "https://de.wikipedia.org/wiki?curid=3809635", "title": "Google Chrome", "text": "Google Chrome\n\nGoogle Chrome ist ein Webbrowser des US-amerikanischen Unternehmens Google LLC. Er war Ende 2017 der weltweit am weitesten verbreitete Browser.\n\nGoogle veröffentlicht große Teile des Quelltextes von Google Chrome in dem Open-Source-Projekt Chromium.\n\nAls erste Veröffentlichung wurde am 2. September 2008 eine Windows-Version mit der Versionsnummer 0.2 freigegeben. Die erste stabile Version 1 folgte am 11. Dezember 2008. Mit Version 4 kam die erste auch auf Linux und Mac OS X lauffähige Version als Beta-Version heraus. Mit Version 5 wurden diese in einer stabilen Version veröffentlicht.\n\nAb dem 7. Februar 2012 bot der Hersteller die erste auf der Version 16.0 basierende Vorabversion des Browsers für Android an. Laut Google lag das Augenmerk bei der Entwicklung des Browsers vor allem auf Schnelligkeit und Sicherheit. Im Vergleich zur Desktop-Version besitzt Chrome für Smartphones eine neu gestaltete Ansicht, in der Tabs übereinander gestapelt werden.\n\nSeit dem 28. Juni 2012 ist Google Chrome auch für iOS erhältlich. Da Apple für iOS keine alternativen Browser-Engines erlaubt, basieren die HTML-Rendering-Engine und die JavaScript-Implementierung auf der iOS-Komponente „WKWebView“. Somit ist auch Googles JavaScript-Implementierung V8 auf dieser Plattform nicht verfügbar. Laut Sundar Pichai, der damals noch Chrome-Entwickler war, waren diese Zugeständnisse notwendig, um Google Chrome auf iOS verfügbar zu machen. Die App setzt mindestens iOS 9.0 voraus und unterstützt sowohl iPhone, iPod touch als auch den größeren Bildschirm des iPad. Wie in Chrome für Android können auch mit Chrome für iOS sämtliche Lesezeichen, geöffnete Tabs und Passwörter synchronisiert werden, sofern sich Nutzer mit einem Google-Konto anmelden.\n\nDer Browser ist ein integraler Bestandteil des hauseigenen Betriebssystems Chrome OS.\n\nBis Version 6 wurde etwa alle vier Monate eine Hauptversion veröffentlicht, seit Sommer 2010 alle sechs bis sieben Wochen. Dazwischen erscheinen Zwischenversionen („minor update“), um gravierende Sicherheits- und Stabilitätsprobleme zu beseitigen.\n\nZusätzlich zu stabilen Versionen bietet Google Chrome drei Vorabversionen: \"Beta\", \"Dev\" (Developer) und \"Canary\". Die Beta- und Dev-Versionen werden für Android, Windows, macOS und Linux veröffentlicht. Dabei werden diese Versionen auf allen Desktop-Betriebssystemen nicht separat installiert, sondern ersetzen die bisherige Chrome-Installation.\nGegenüber den Beta- und Dev-Versionen wird die Canary-Version separat neben der anderen Google-Chrome-Version installiert. Das \"Canary Build\" entsteht automatisch aus der neuesten Version des Open-Source-Projektes Chromium und wird daher vor der Veröffentlichung nicht mehr getestet. Diese Version ist nur für Windows und Apples macOS verfügbar. Canary ist englisch für Kanarienvogel und steht symbolisch für die Verwendung von Kanarienvögeln als Warnvögel im Bergbau, bedeutet also, dass Fehler im Canary-Build erkannt werden sollen, bevor diese an stabilere Versionen weitergegeben werden.\n\nDamit die Dateigröße der Updates möglichst klein ist, wird das dafür entwickelte Datenkompressionssystem Courgette verwendet.\n\nAls zentrale Bedienelemente werden Tabs verwendet, mit denen die Inhalte übersichtlich dargestellt und auch parallel bearbeitet werden können. Die Benutzeroberfläche besteht aus einigen Kontrollschaltflächen sowie einer Adresszeile, die „Omnibox“ genannt wird. Diese macht unter anderem Vorschläge und erlaubt eine Textsuche über bisher besuchte Webseiten sowie über bisherige Suchanfragen. Außerdem wird auf der Startseite eine Suchleiste und eine automatisch generierte Liste mit den am häufigsten besuchten Webseiten angezeigt; auch ein Surfmodus („Inkognito-Fenster“), der keine Spuren auf dem lokalen System hinterlässt, ist vorhanden.\n\nAnfangs war eine besondere Stärke des Browsers seine Geschwindigkeit. Insbesondere V8, die seit der ersten Chrome-Version enthaltene virtuelle Laufzeitumgebung von JavaScript, übertraf nach Angaben von Google andere Implementierungen an Geschwindigkeit. Bei einem Test (Peacekeeper-Benchmark) im Jahr 2010 lief JavaScript in Chrome etwa doppelt so schnell wie im Mozilla Firefox 3.6 oder rund neunmal so schnell wie im Internet Explorer 8. Bei sehr rechenintensiven Tests wurden diese Werte noch übertroffen. In Folge haben auch die Hersteller anderer Browser ähnliche Optimierungen eingeführt.\n\n2015 hatte Chrome nur noch einen kleinen oder gar keinen Vorsprung vor anderen Browsern mehr. Im „Sunspider“-Benchmark war Microsoft Edge fast doppelt so schnell. Auch 2016 gewann Edge fast alle Javascript-Benchmarks, doch erfüllt Chrome die meisten HTML5-Standards und konnte in allgemeineren Leistungstests punkten, wie zum Beispiel „RoboHornet“.\n\nChrome unterstützt Plug-ins, welche über den „Chrome Web Store“ bezogen werden können. Die Erweiterungen werden über eine integrierte API eingebunden und werden in den Web-Technologien JavaScript, HTML und CSS entwickelt. Aufbauend auf dem Format wird unter dem Namen \"Browser Extensions\" ein browserübergreifender Standard für Erweiterungen erarbeitet.\n\nZudem werden „Chrome Apps“ unterstützt. Dies sind Chrome-Erweiterungen, die in einem eigenen Fenster dargestellt werden. Am 19. August 2016 gab Google bekannt, diese Funktion für Windows, OS X und Linux schrittweise einzustellen. Lediglich auf Chrome OS sollen sie weiter bestehen.\n\nNachdem Schadprogramme vielfach ungewünscht Erweiterungen in Chrome installiert haben, können diese nur noch über den von Google kontrollierten Web Store bezogen werden.\n\nZur Darstellung der Webseiten wird in Chrome der von Google und Opera Software entwickelte und von WebKit abgespaltene HTML-Renderer \"Blink\" verwendet. Die JavaScript-Implementierung \"V8\" ist als freie Software veröffentlicht worden und stammt vom dänischen V8-Team, unterstützt Mehrkernprozessoren und ein dynamisches Optimierungsverfahren, bei dem JavaScript-Objekte versteckt um geteilte Klassen erweitert werden.\n\nChrome besteht aus drei Teilen. Der Browser selbst ist für die Steuerung der Software zuständig, der Renderer ist im Browser implementiert und stellt einen Subprozess wie zum Beispiel einen Reiter dar. Chrome ist komponentenbasiert aufgebaut. Die Interprozesskommunikation arbeitet nachrichtenorientiert und benutzt „Channeling“.\n\nAnders als in vergleichbaren Browsern, in denen alle Tabs Teile eines einzelnen laufenden Programmes sind, sind die Tabs in Chrome in sich geschlossene Prozesse und als solche in einem eigenen Taskmanager kontrollierbar. Durch die Aufteilung in mehrere Prozesse soll vermieden werden, dass ein einziger Tab, in dem ein rechenintensiver Prozess läuft, die Leistung des gesamten Browsers in Mitleidenschaft zieht. Falls es mit einem Tab ein Problem gibt und der Prozess endet, so wird anstelle des Inhalts eine Fehlermeldung angezeigt. Außerdem werden die Prozesse der Tabs in einer Sandbox ausgeführt und haben damit nur sehr eingeschränkte Möglichkeiten, mit anderen Prozessen zu interagieren. Dateizugriffe sind nur über das Hauptprogramm (\"Browser\"-Prozess) möglich. Dadurch wird das Gros an Schadcode, das von geöffneten Websites aus eine Sicherheitslücke im Browser ausnutzt, gehindert, einen Computer zu befallen.\n\nBeim alljährlichen Pwn2Own-Wettbewerb versuchen Teilnehmer, Computer mit jeweils einem installierten Browser zu hacken. Der Gewinner erhält als Belohnung den Computer und einen Geldpreis.\nGoogle Chrome nimmt seit 2009 daran teil – und erweist sich dabei seit Jahren als vergleichsweise sicher im Vergleich zu anderen Browsern.\n\nDas Bundesamt für Sicherheit in der Informationstechnik empfiehlt seit Februar 2012 den Einsatz von Google Chrome auf Computern mit einem Windows-Betriebssystem mit der Begründung, durch das Sandbox-Verfahren sei die Angriffsfläche des Chrome-Browsers deutlich reduziert.\n\nGoogle Chrome ist der erste Webbrowser, der (ab Version 37) die Zwei-Faktor-Authentifizierung nach dem U2F-Standard der FIDO-Allianz ermöglicht. Mittels der frei zugänglichen \"Google Safe Browsing API\" erhält Chrome Listen gefährlicher Websites.\nAb Version 68 – im Juli 2018 – kennzeichnet Google Webseiten, die Datenübertragungen nicht per HTTPS verschlüsseln, in der Browserzeile vor der URL als unsicher. Bisher erschien bei HTTP-Seiten nur eine Warnung, wenn auf der Seite die Eingabe persönlicher Daten möglich war.\n\nGoogle Chrome enthält eine interne Passwort-Verwaltung und einen Passwort-Generator.\n\nNachdem Google Chrome zunehmend mit dem bis dato meistgenutzten Webbrowser Internet Explorer konkurriert hatte, gelang es ihm im Mai 2012 nach Angaben des globalen Statistikunternehmens \"StatCounter\" erstmals, weltweit die Spitzenposition einzunehmen. So erreichte Chrome in der Woche vom 14. bis 20. Mai einen globalen Marktanteil von 32,8 Prozent, wohingegen der Internet Explorer nur auf 31,9 Prozent kam. Repräsentativ sind die veröffentlichten Zahlen von \"StatCounter\" allerdings nicht, da das Unternehmen nur die Daten der mit ihm verbundenen Unternehmen untersucht. Gleiches gilt für die US-Marktforscher von \"Net Applications\", die im April 2012 stark von \"StatCounter\" abweichende Zahlen erhoben und dem Internet Explorer mit 54,09 Prozent noch eine deutliche Dominanz bescheinigt hatten. In deren Statistik lag Google Chrome mit 18,85 Prozent noch hinter Mozilla Firefox (20,2 Prozent) auf Rang 3.\n\nIm Mai 2015 war Google Chrome laut \"StatCounter\" mit einem Anteil an der Internetnutzung weltweit (ohne mobile Geräte) von 49 Prozent der meistgenutzte Webbrowser. Auch in Europa lag er auf Platz 1 mit einem Anteil von 44,5 Prozent. In Deutschland dagegen lag er mit einem Anteil von 27 Prozent hinter Firefox auf Platz 2.\n\nIm März 2016 lag Chrome mit 47,21 Prozent weltweitem Marktanteil (inklusive mobiler Geräte), laut \"StatCounter\", sehr deutlich vor dem zweitplatzierten Safari mit 12,68 Prozent Marktanteil.\n\nIm Juli 2018 lag der Anteil von Google Chrome laut \"StatCounter\" weltweit bei 59,69 Prozent, gefolgt von Safari mit 13,85 Prozent.\n\nEs wird bemängelt, dass bei Nutzung von Google Chrome zu viele Daten an Google gesendet würden. Der Sprecher des BSI, Matthias Gärtner, äußerte datenschutzrechtliche Bedenken. Beim Tippen in der Adresszeile, die zugleich Eingabefeld für Suchbegriffe und Web-Adressen ist, wird jedes Schriftzeichen an die vom Benutzer gewählte Suchmaschine übermittelt, um Vervollständigungsvorschläge zu ermöglichen. Dieses standardmäßig eingeschaltete Verhalten ist ausschaltbar.\n\nGoogle führt in seinen Datenschutzbestimmungen diverse Informationen auf, die von Chrome an Google gesendet werden. Bis zur Version 4.0 erhielt jede Installation eine eindeutige Identifikationsnummer, die bei der Installation, bei der ersten Verwendung und bei jeder automatischen Aktualisierungsprüfung mit weiteren grundlegenden Informationen zur Browser-Installation an Google gesendet wurde. Die Identifikationsnummer konnte manuell entfernt oder über Software-Erweiterungen unterdrückt werden. Ab Version 4.1 verzichtet Google auf die ID.\n\nChrome sendet Informationen über die Benutzung des Browsers an Google, dabei sind nicht alle Methoden optional.\n\nAnfang April 2018 wurde kritisiert, dass das im Oktober 2017 eingeführte „Chrome Cleanup“, welches in Zusammenarbeit mit dem Softwaresicherheitsunternehmen ESET in den Browser integriert wurde, unter anderem den „Dokumente-Ordner“ auf Windows-Rechnern durchsucht. Zusätzlich ist standardmäßig die Datenübermittlung dieser „Computer bereinigen“ genannten Funktion an Google aktiviert, welches vielen Anwender nicht bekannt sei. Ebenso ist die Funktion für Privatbenutzer nicht komplett abschaltbar. Die Datenerfassung lässt sich zwar deaktivieren, ist aber nach einem Neustart der Browser-Software wieder aktiv.\n\nMit Erscheinen von Chrome-Version 69 im September 2018 wurde bekannt, dass Google Teile der URL in der Browser-Adresszeile gekürzt darstellt. Dieser Schritt wurde von vielen Entwicklern kritisiert, da sich die Änderung auch auf Subdomains auswirkt und man dadurch in die Irre geführt werden kann. So werden zwei völlig unterschiedliche Adressen wie codice_1 und codice_2 in Chrome 69 durch die gekürzte Darstellung immer als codice_3 angezeigt und kann vom Benutzer nur durch einen Klick in die Adresszeile korrekt identifiziert werden. Googles Aussage nach sind URLs für viele Anwender zu technisch und zu kompliziert, weshalb man dies vereinfache. Die kritisierte Änderung wurde von Google eine Woche später in einer aktualisierten Version rückgängig gemacht.\n\nWeitere Kritik erhielt Version 69 für die übergreifende Anmeldung im Chrome-Browser und auf Google-Webseiten. Während die Anmeldung in Chrome zur Verwendung der Synchronisation von Browserdaten wie Suchverläufen, Passwörtern und Lesezeichen dient, können über die Google-Webseiten auf Dienste wie Gmail zugegriffen werden. Da ab Version 69 beide Anmeldungen miteinander verbunden sind, befürchten Datenschützer, dass viele Benutzer bei Verwendung der Google-Webdienste ungewollt ihre Browserdaten über die automatische Chrome-Anmeldung synchronisieren. Zuvor war die Anmeldung im Browser und auf Google-Webseiten voneinander getrennt, wodurch man mit unterschiedlichen Konten angemeldet sein konnte oder sich nur an einem der beiden Dienste anmelden konnte. Google reagierte auf diese Kritik und führte mit Version 70, welche im Oktober 2018 veröffentlicht wurde, die Option „Anmeldung in Chrome zulassen“ ein, mit welcher sich die automatische Chrome-Anmeldung nach einem Login an einem Google-Dienst deaktivieren lässt. Diese Einstellung ist standardmäßig allerdings aktiviert, sodass der Benutzer diese selbst abschalten muss (Opt-out).\n\nDatenschutz-Diskussionen bewegen viele Benutzer dazu, Cookies von Hand zu löschen oder automatisch nach dem Beenden des Browsers. Seit Version 69 werden Google-Cookies in Chrome, jedoch nur noch gelöscht wenn sich der Benutzer von seinem Google-Konto abmeldet. Ohne eine Abmeldung von den Google-Diensten bleiben diese Cookies, trotz Löschung erhalten.\n\n\n"}
{"id": "3813109", "url": "https://de.wikipedia.org/wiki?curid=3813109", "title": "Return into libc", "text": "Return into libc\n\nReturn into libc (auch \"return-to-libc\") ist eine Methode zum Angreifen von Computersystemen, um den Host (Server) zur Ausführung unerwünschter Programme zu missbrauchen.\n\nComputerprogramme verwenden einen speziellen Speicherbereich, den sogenannten Stack, um Variablen und Rücksprungadressen () aus Unterprogrammen zu verwalten. Wird der Stack nun so manipuliert, dass er statt der erwarteten Rücksprungadresse die Adresse einer anderen Funktion enthält, kann diese Funktion benutzt werden, um die gewünschten schädlichen Funktionen auszuführen. Die „libc“ ist eine Sammlung von Funktionen, die die Programmiersprache C zur Verfügung stellt. Damit braucht also keine neue Funktion in das laufende System gebracht zu werden, so wie dies bei Shellcode Exploits üblicherweise notwendig ist. In dieser Sammlung befindet sich z. B. eine populäre Funktion „system()“, welche die Ausführung beliebiger Systemprogramme ermöglicht. Sie benötigt dabei nur einen Parameter, nämlich das auszuführende Kommando.\n\nDie Manipulation des Stacks wird typischerweise durch einen Buffer Overflow hervorgerufen. Dabei wird eine Variable, die nur eine bestimmte Anzahl von Zeichen aufnehmen kann, mit einer längeren, genau ermittelten Zeichenkette belegt, die auf dem Stack dann die hinter der Variablen liegende Rücksprungadresse überschreibt. Ebenso werden die gewünschten Argumente für die (libc-)Funktion auf den Stack geschrieben, zum Beispiel die Adresse einer Zeichenkette.\n\nVariablen können unter anderem dann einen solchen Überlauf erzeugen, wenn sie z. B. in einer Web-Adresse als Parameter übergeben werden können und diese etwa an ein CGI-Programm übergeben werden. Voraussetzung für eine solche Attacke ist jedoch ein anfälliges Programm, das – entgegen üblichen Programmiertechniken – die Länge von Eingabevariablen nicht prüft.\n\nEin mittels NX-Bit geschützter nicht-ausführbarer Stack kann zwar der Einschleusung von Schadcode durch Code Injection vorbeugen, eine Return-to-libc-Attacke kann jedoch trotzdem durchgeführt werden, da hierbei bereits existenter Code aufgerufen wird. Eine Abwehrmöglichkeit solcher Angriffe kann durch ASLR erreicht werden. Dabei werden die Speicheradressen der Systemfunktionen zufällig erteilt, ein genauer Rücksprung auf diese Funktionen wird dabei also unwahrscheinlich.\nStack-Schutzmechanismen wie der \"Stack Smashing Protector\" können weitere Angriffe aufdecken oder verhindern.\n\n"}
{"id": "3814945", "url": "https://de.wikipedia.org/wiki?curid=3814945", "title": "Bayrische Hackerpost", "text": "Bayrische Hackerpost\n\nDie Bayrische Hackerpost war ein von 1984 bis 1988 in München herausgegebenes Informationsblatt.\n\nEs trug den Untertitel „Das Informationsblatt für den lebensbejahenden DFÜ-Benutzer“, bot technische und rechtlich-politische Informationen zur Datenfernübertragung (DFÜ) und wurde von Mitgliedern der bayerischen Hacker-Szene herausgegeben.\n\nEs gab dreizehn Ausgaben, die oktal nummeriert waren. Die letzte Ausgabe trägt deswegen die Nummer 15, und je eine Sonderausgabe zu den Messen IFA und Systems.\n\nEin Höhepunkt war der erste deutschsprachige Artikel über Virusprogramme in Ausgabe 3, dessen Aussagen in der Folgezeit durch den ersten Virus für den Commodore 64 belegt wurden. Dieser „BHP Virus“ wurde 1986 von Mitgliedern der Bayrischen Hackerpost ursprünglich nur als Beleg dafür geschrieben, dass die im Artikel beschriebene Verbreitung von Virenprogrammen auch ohne Netzwerke, sondern allein durch Diskettenaustausch erfolgen kann. Autoren der Hackerpost waren regelmäßig Gäste in der über digitale Subkultur berichtenden Rundfunksendung Bit, byte, gebissen im BR-Zündfunk.\n\nAuch die mit Anspielungen auf die Römisch-katholische Kirche durchsetzte satirische Beschreibung der fiktiven Programmiersprache \"Vatical\" stammt von der Bayrischen Hackerpost.\n\n"}
{"id": "3815089", "url": "https://de.wikipedia.org/wiki?curid=3815089", "title": "Linpus Linux", "text": "Linpus Linux\n\nLinpus Linux, aufgrund seiner weiten Verbreitung in China auch als \"chinesisches Linux\" bekannt, ist eine Linux-Distribution, die aus Fedora entstanden ist.\n\nEs besteht die Möglichkeit, vom Simple Modus in den erweiterten Modus umzuschalten, der dann eine gewohnte KDE-Oberfläche ermöglicht.\nLinpus Linux ist auf 7-Zoll-Bildschirmen noch bedienbar, läuft auf Systemen ab 366 MHz und unterstützt Wi-Fi, WiMAX, HSDPA, HSUPA, Ethernet, Bluetooth, IR und UPnP. Verwendet wird die Paketverwaltung RPM von Fedora, so dass hierbei auf Fedora-Software zurückgegriffen werden kann. Linpus Linux wurde vor allem auf Netbooks der Firma Acer ausgeliefert, wie z. B. dem Acer Aspire One.\n\nLinpus Linux Lite ist ein Live-System für \"Low-Cost\"-PCs wie Ultra-Mobile PCs, OLPC XO-1, Classmate PC, Mobile internet devices und andere mobile Computer. Die kostenlos herunterladbare Version (Dateigröße knappe 699 MB) der ersten Versionen von Lite Version lief nach Herstellerangaben bereits auf i386 32bit PCs mit weniger als 500 MHz Taktfrequenz, 128 MB DRAM und 512 MB Festplattenkapazität. Von den ersten Versionen existierten zusätzliche Versionen für weitere Systeme wie Intel StrongARM, XScale, ARM 7/9 und MIPS. Ab Version 2 wirbt der Hersteller mit Linpus Linux, die zu den am schnellsten bootenden Distributionen gehört von rund vier Sekunden. Ein vereinheitlichter Installer beschleunige die Installation, die nun via USB etwa 6 Minuten dauere. Eine weitere Neuerung ist, dass ein angepasstes Gnome 3 als Desktop verwendet wird. Die derzeit aktuelle Version 2.2 (Stand 2014) benötigt eine 64 bit CPU Intel/AMD mit mindestens 1 GHz, einem Arbeitsspeicher von 1 GB RAM und 6 GB Festplattenkapazität und ist nur noch in den Sprachen Englisch und Chinesisch verfügbar.\n\nLinpus Linux ist die Linpus Desktop-Version, im Zuge der 2007 aufgekommenen Netbooks (angeführt vom Asus Eee PC – vgl. \"Netbook#Geschichte\" – und gefolgt vom Acer Aspire One) auch bekannt als \"chinesisches Linux\", da es in China sehr weit verbreitet war und auf dem Großteil des asiatischen Marktes das beliebteste Betriebssystem für Netbooks war.\n\nEbenso wie Linpus Lite stellt die Desktop-Version keine großen Ansprüche an die erforderliche Hardware zur Installation und läuft somit nach Herstellerangaben auch auf älteren i386-PCs. Linpus Linux Desktop ist in der Basis-32-Bit-Version kostenlos auf den Herstellerseiten herunterzuladen (aktuelle Version 9.6, Dateigröße 3,62 GB), wobei zu beachten ist, dass eine ebenfalls dort verfügbare Deluxe-Version als 32- und 64-Bit-Version mit Handbuch und Support-Unterstützung kostenpflichtig ist.\n\nLinpus Linux Server ist eine professionelle (kostenpflichtige) Enterprise Business Server-Software, die besonders auf Stabilität ausgelegt ist. Aktuell ist die Version 2.1, die für PCs mit den Rechnerarchitekturen i386 und i586 verfügbar ist.\n\nLinpus Media Center ist eine \"stand alone\" Linux basierende Multimedia-Plattform, welche wie ein DVD-Spieler arbeitet und die Wiedergabe verschiedener Multimedia-Formate und das Anzeigen von digitalen Bildern ermöglicht. Linpus Media Center ist kostenlos auf den Herstellerseiten herunterzuladen, wobei zu beachten ist, dass eine ebenfalls dort verfügbare Aktualisierung („update“) kostenpflichtig ist.\n\nDer Hersteller \"Linpus Technologies Inc.\" finanziert sich hauptsächlich durch den Verkauf der kostenpflichtigen Server-Versionen.\nDa Linpus Linux von verschiedenen Herstellern zusammen mit ihren Laptops verkauft wird, ist es wahrscheinlich, dass hierbei Lizenzgebühren von den Laptop-Herstellern an \"Linpus Technologies Inc.\" gezahlt werden. Allerdings dürften diese wesentlich günstiger ausfallen als die Kosten für Microsoft-Lizenzen.\n\nKDE ist standardmäßig eingebunden.\nNachinstallieren und Einbinden zusätzlicher Pakete, um es den persönlichen Bedürfnissen anzupassen, erfordert, dass hier auf die Paketverwaltung RPM aus der Fedora-Distribution zurückgriffen werden muss. Das ist für Linux-Anfänger teilweise nicht ganz unkompliziert, so dass Linpus Linux nicht unbedingt für Anfänger optimal geeignet ist. Das kostenlose Herunterladen von Linpus Linux Lite, Linpus Linux Desktop und Linpus Linux Media Center ist derzeit nur über den Server des Herstellers möglich, wobei teilweise mit langen Ladezeiten zu rechnen ist.\n\n\n"}
{"id": "3824759", "url": "https://de.wikipedia.org/wiki?curid=3824759", "title": "MediaShow", "text": "MediaShow\n\nMediaShow ist eine Bild- und Videoverwaltungs-Software des Unternehmens CyberLink zum Ansehen, Organisieren, Bearbeiten und Brennen von Fotos und Videos. In ihrem Aufbau und ihrer Funktionalität entspricht sie anderen Fotomanagement-Programmen. Die Software ist vor allem für Familien ausgelegt. Der Funktionsumfang ist ein Kompromiss zwischen Reaktionsgeschwindigkeit und Bearbeitungsfeatures. Die Software verfügt jedoch über keine professionellen Bearbeitungsfunktionen.\n\nVon der Benutzeroberfläche her gesehen, lässt sich MediaShow in drei Module unterteilen: Fotos, Videos, und DVD-Authoring. Dateien, die in die MediaShow-Datenbank importiert werden, werden nach Video- und Grafikdateien sortiert. Das Hauptfenster zeigt Thumbnails (Miniaturansichten) der Dateien an. Die Größe der angezeigten Thumbnails kann wie bei iPhoto beliebig verändert werden.\n\nMediaShow umfasst die gängigsten Bearbeitungswerkzeuge, über die auch andere Bildbearbeitungsprogramme verfügen: Drehen, Ausschneiden, rote Augen entfernen, Kontrast, Helligkeit, Farbton und Sättigung sowie neun Fotoeffekte inklusive Vignette. Fotos können zudem in Form einer 3D-Diashow mit Hintergrundmusik wiedergegeben werden. Die Software berechnet automatisch den Rhythmus der Musik und zeigt die Bilder dann im Takt der Musik an. Die Videobearbeitungs-Werkzeuge beheben folgende Mängel: Störende Geräusche, verwackelte Videos und schlechte Belichtung, zudem können Helligkeit, Farbkontrast, Farbton, Sättigung und Weißabgleich eingestellt werden.\n\nVor dem Brennen von DVDs mit eigenen Fotos und Videos kann der Anwender Navigationsmenüs zur DVD hinzufügen. Die Erstellung von gemischten DVDs mit sowohl Fotos als auch Videos ist möglich.\n\n\n\n"}
{"id": "3825236", "url": "https://de.wikipedia.org/wiki?curid=3825236", "title": "CAT Testsystem", "text": "CAT Testsystem\n\nDas CAT Testsystem (Abkürzung für Computer-Assistiertes Testen) ist ein System der computerunterstützten Psychodiagnostik, das eine Plattform für die Durchführung vielfältiger psychologischer Testverfahren bietet. Es wurde ursprünglich als Initiative der Bundeswehr entwickelt, dann von GiKOM CSE Bonn und jetzt von Conet Solutions GmbH betreut \n\nJe nach Bedarf können die Nutzer eigene Testverfahren in das System einpflegen oder auch kommerziell erhältliche Testverfahren integrieren. \nDer Leistungsumfang von CAT reicht dabei über die Ablaufsteuerung einzelner Testverfahren deutlich hinaus. CAT ermöglicht die Datenverwaltung von großen Mengen getesteter Personen (Probanden). Dabei können durch Verknüpfung mit anderen Datenhaltungssystemen die in CAT entstandenen Testdaten mit beliebigen anderen Daten zur Erhöhung der Entscheidungssicherheit in der Diagnostik angereichert werden.\n\nDie Kombination aller verfügbaren Daten kann zur Steuerung der Vorgabe von Testverfahren eingesetzt werden. Diese Funktionalität ermöglicht es, den Probanden programmgesteuert genau jene Testverfahren vorzugeben, die zum Treffen der gewünschten diagnostischen Entscheidung notwendig sind. Schließlich ist es auch möglich, die in CAT entstandenen Daten in beliebiger Form aufzubereiten und an andere Datenhaltungssysteme weiterzugeben.\nSomit bietet das CAT Testsystem alle Möglichkeiten zur Vorbereitung, Durchführung und Nachbereitung der computerunterstützten psychologischen Diagnostik – gerade für Organisationen mit einer hohen Anzahl zu testender Personen.\n\nDas CAT Testsystem ist in den 1980er Jahren als Initiative des Psychologischen Dienstes der Bundeswehr entstanden. In den folgenden Jahren wurde das System kontinuierlich verbessert und den geänderten Anforderungen und Erkenntnissen der psychologischen Diagnostik (z. B. durch die Einführung adaptiven Testens) angepasst. Seit einigen Jahren werden die softwaretechnischen Anpassungen zentral durch die GiKOM CSE AG in Bonn konzipiert und realisiert.\n\nWährend dieses Zeitraums ist eine ganze Reihe zusätzlicher Mitglieder in den Nutzerkreis von CAT aufgenommen worden. Diese Institutionen mit ihren jeweiligen spezifischen Anforderungen an die psychologische Diagnostik haben immer neue Impulse für die Verbesserung und Erweiterung des Systems gegeben. Auf diese Weise ist ein mächtiges und bewährtes System entstanden, welches derzeit in der 5. Generation im Einsatz ist und als CAT5 bezeichnet wird.\n\nNeben den laufenden Anpassungen an aktuelle diagnostische Entwicklungen wie der Nutzung multimedialer Elemente und der Erweiterung der testmethodischen Funktionalitäten ist hier vor allem die Ermöglichung des webbasierten Testens, also die Integration einer eAssessment-Komponente, zu erwähnen.\n\nDiese Erweiterung bietet den Nutzern von CAT die Möglichkeit, verschiedene Daten von Probanden auch außerhalb der bisher genutzten örtlich gebundenen Testräume zu erhalten sowie im Vorfeld eine Vorselektion vorzunehmen, welche die Wirtschaftlichkeit steigert.\n\nZurzeit wird das CAT Testsystem hauptsächlich in Deutschland und dem deutschsprachigen Ausland eingesetzt. Die somit entstandene Interessengemeinschaft von Nutzern nennt sich CAT-Usergroup und kommuniziert über ein eigenes Portal.\n\nAktuell gehören die folgenden Mitglieder zur CAT-Usergroup (Stand 15. Oktober 2015):\n\nEinmal jährlich treffen die zur CAT-Usergroup gehörenden Spezialisten aus Psychologie und IT zu einem gemeinsamen, zweitägigen Symposium zusammen – dem sogenannten \"CAT-Usertreffen\" – und informieren einander über den aktuellen Stand sowie geplante Entwicklungen. Hierbei werden Synergiepotentiale identifiziert und gemeinsame Vorhaben geplant.\n\nDer Aufbau eine CAT-Anlage stellt sich bei allen Mitgliedern überwiegend einheitlich wie folgt dar\n\n\n\n\n\nDie hohe Flexibilität des Systems verbunden mit den laufenden technischen und psychologisch-methodischen Verbesserungen bietet den Nutzern alle Freiheiten beim Einsatz von Testverfahren. Durch die weitgehend innerhalb der Nutzergemeinschaft weitgehend einheitliche Architektur des Systems ist zudem ein relativ kostengünstiger Einsatz sowie die Gewinnung von Synergieeffekten möglich.\n\nSo können Weiterentwicklungen, welche durch ein Mitglied der Gemeinschaft initiiert wurden, durch andere Mitglieder mit erheblich reduziertem Aufwand genutzt werden. Auf diese Weise werden die Entwicklungs- und Pflegekosten auf mehrere Schultern verteilt.\n\nDurch die umfangreichen Funktionalitäten des Systems bietet sich der Einsatz von CAT nur bei regelmäßigen Testungen und höheren Probandenzahlen an. Bei einem Einsatz z. B. im universitären Kontext zu Forschungszwecken mit Fallzahlen von 200 bis 300 Probanden pro Jahr können die Vorteile des Systems kaum ausgenutzt werden, so dass dort wohl weiterhin auf bewährte Methoden der Papier-und-Bleistift-Testung oder eigenentwickelte Computerprogramme zurückgegriffen werden wird.\n"}
{"id": "3829278", "url": "https://de.wikipedia.org/wiki?curid=3829278", "title": "Killbit", "text": "Killbit\n\nDas Killbit (auch bekannt als Kill-Bit oder Kill Bit) ist ein Eintrag in der Windows-Registrierungsdatenbank, der die Aktivierung von Software-Teilen (COM-Komponenten, insbesondere ActiveX-Elemente) mit Sicherheitslücken verhindern soll. Das Killbit unterstützt eine Methode der Softwaretechnik, die in Microsoft-Betriebssystemen (insbesondere MSHTML) eingeführt wurde.\n\nDas Killbit ist kein Bit, sondern wird durch einen 4-Oktett großen Wert mit dem Namen \"Compatibility Flags\" und dem Datentyp REG_DWORD in der Windows-Registrierungsdatenbank repräsentiert.\n\nEine Software kann die dahinterstehende Methode unterstützen, in dem die Instanzierung bzw. Ausführung von COM-Komponenten trotz anderweitiger Anforderung verhindert wird, wenn folgende Bedingungen erfüllt sind:\n\n\nDas ActiveX-Control mit der CLSID {73BCFD0F-0DAA-4B21-B709-2A8D9D9C692A} hat einen Eintrag \"Compatibility Flags\" in der Windows-Registrierungsdatenbank mit dem Wert 0x00000400. Siehe Bild rechts. Da der Internet Explorer diese Methode unterstützt, wird dieses ActiveX-Control nicht ausgeführt, auch dann nicht, wenn das ActiveX-Control in einer adressierten HTML-Datei (nicht zu verwechseln mit HTAs) eingebettet ist.\n\nLiefert ein Entwickler eine neue, korrigierte Fassung seiner durch ein Killbit gesperrten Active-X-Komponente aus, muss er dieser eine neue CLSID geben. Damit Webseiten und Anwendungen nun diese neue Version verwenden, obwohl sie nur die alte CLSID kennen, kommt das sogenannte Phoenix-Bit zum Einsatz. Dabei handelt es sich um einen weiteren Registry-Eintrag an gleicher Stelle, der eine alternative CLSID für die gesperrte Komponente festlegt – die CLSID der neuen Version.\nIn diesem Registry-Schlüssel HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Internet Explorer\\Active-X Compatibility\\<CLSID> sind die Einträge für das Killbit (\"Compatibility Flags = 0x00000400 (1024)\") und das Phoenix-Bit (\"AlternateCLSID = <neue CLSID>\") enthalten.\n"}
{"id": "3832994", "url": "https://de.wikipedia.org/wiki?curid=3832994", "title": "MapReduce", "text": "MapReduce\n\nMapReduce ist ein vom Unternehmen Google Inc. eingeführtes Programmiermodell für nebenläufige Berechnungen über (mehrere Petabyte) große Datenmengen auf Computerclustern. \"MapReduce\" ist auch der Name einer Implementierung des Programmiermodells in Form einer Software-Bibliothek.\n\nBeim MapReduce-Verfahren werden die Daten in drei Phasen verarbeitet (Map, Shuffle, Reduce), von denen zwei durch den Anwender spezifiziert werden (Map und Reduce). Dadurch lassen sich Berechnungen parallelisieren und auf mehrere Rechner verteilen. Bei sehr großen Datenmengen ist die Parallelisierung unter Umständen schon deshalb erforderlich, weil die Datenmengen für einen einzelnen Prozess (und das ausführende Rechnersystem) zu groß sind.\n\nDas Programmiermodell wurde durch die in der funktionalen Programmierung häufig verwendeten Funktionen \"map\" und \"reduce\" inspiriert, auch wenn die Arbeitsweise der Bibliothek davon abweicht. 2010 wurde für MapReduce ein US-Patent erteilt. Der wesentliche Beitrag von MapReduce ist jedoch das zu Grunde liegende System, das die Berechnungen stark parallelisiert, die Reorganisation der Daten im Shuffle-Schritt optimiert, und automatisch auf Fehler im Cluster reagieren kann, wie beispielsweise den Ausfall von kompletten Knoten.\n\nDas obige Bild illustriert den Datenfluss bei der MapReduce-Berechnung.\n\nDie MapReduce-Bibliothek realisiert eine Funktion, welche aus einer Liste von Schlüssel-Wert-Paaren (Eingabeliste) eine neue Liste von Schlüssel-Wert-Paaren (Ausgabeliste) berechnet:\nErläuterung:\n\nDer Nutzer konfiguriert die Bibliothek über die Bereitstellung der beiden Funktionen Map und Reduce, die wie folgt definiert sind:\nbzw.\n\n\n\n\nAnmerkung: Diese Darstellung war etwas vereinfacht, denn in der Regel wird die Steuerung des MapReduce Verfahrens eine Anzahl formula_36 von Reduce-Prozessen anstreben, so dass, wenn es für mehr als formula_36 verschiedene Schlüssel formula_27 Zwischenergebnisse formula_39 gibt, Zwischenergebnisse formula_39 mit verschiedenen Schlüsseln formula_27 in einer gemeinsamen Liste gespeichert werden. Die entsprechenden Paare werden vor der Reduce-Berechnung nach Schlüsseln sortiert.\n\nOptional kann vor der Shuffle-Phase noch eine Combine-Phase erfolgen. Diese hat in der Regel die gleiche Funktionalität wie die Reducefunktion, wird aber auf dem gleichen Knoten wie die Map-Phase ausgeführt. Dabei geht es darum, die Datenmenge, die in der Shuffle-Phase verarbeitet werden muss, und damit die Netzwerklast zu reduzieren. Der Sinn der Combine-Phase erschließt sich sofort bei der Betrachtung des : Auf Grund der unterschiedlichen Häufigkeit von Wörtern in natürlicher Sprache, würde bei einem deutschen Text beispielsweise sehr oft eine Ausgabe der Form (\"und\", 1) erzeugt (gleiches gilt für Artikel und Hilfsverben). Durch die Combine-Phase wird nun aus 100 Nachrichten der Form (\"und\", 1) lediglich eine Nachricht der Form (\"und\", 100). Dies kann die Netzwerkbelastung signifikant reduzieren, ist aber nicht in allen Anwendungsfällen möglich.\n\nMan möchte für umfangreiche Texte herausfinden, wie oft welche Wörter vorkommen.\n\n map(String name, String document):\n\n\n\n\n\nZum Beispiel wäre folgende Berechnung auf einem klassischen Text denkbar:\nDer Text wird in Sätze aufgeteilt, dabei bietet sich eine Normalisierung an, indem man alles klein schreibt und die Satzzeichen entfernt:\nDie Eingabeliste hat drei Paare als Elemente, wir können daher drei Map-Prozesse starten:\nDie Map-Aufrufe generieren diese Zwischenergebnispaare:\nDie Map-Prozesse liefern ihre Paare an die MapReduce-Bibliothek, welche diese in den Zwischenergebnislisten sammelt. Parallel könnte folgendes geschehen (Die gleiche Taktung der 3 Map-Prozesse ist unrealistisch, tatsächlich überlappen sich die Ausführungen. Die T_wort-Listen sind lokal pro Map-Prozess vorhanden und werden nicht zwischen den Schritten synchronisiert):\n\nIm vierten Schritt sieht man, dass Zwischenergebnislisten lokal für jeden Map-Prozess existieren und nicht global wiederverwendet werden können:\n\nIm siebten Schritt kommt dann zum ersten Mal vor, dass ein weiteres Vorkommen in einer bereits angelegten Zwischenergebnisliste gesammelt wird:\nusw.\n\nNach 21 Schritten sind alle drei Map-Prozesse mit ihrer Arbeit durch, die Map-Phase endet und es beginnt die Reduce-Phase.\nDie Zwischenergebnislisten, die von verschiedenen Map-Prozessen zu demselben Wort angelegt wurden, werden zusammengefügt.\nFür jede der entstandenen Zwischenergebnislisten (hier sortiert aufgeführt)\nkönnen wir parallel einen Reduce-Prozess starten, der jeweils die Elemente aufzählt.\nDas Ergebnis von MapReduce sieht in etwa so aus:\nNachdem das Verfahren 2014 bereits zehn Jahre alt ist, bietet Google seit Kurzem eine Erweiterung \"Cloud Dataflow\" an, die größere Flexibilität bietet und das Cloud Computing noch stärker vorantreiben soll.\n\n\n\nPlasmaFS. Plasma MapReduce wurde von Gerd Stolpmann (Darmstadt) entwickelt.\n"}
{"id": "3834700", "url": "https://de.wikipedia.org/wiki?curid=3834700", "title": "Gauss Centre for Supercomputing", "text": "Gauss Centre for Supercomputing\n\nDas Gauss Centre for Supercomputing e. V. (GCS) dient dem Zusammenschluss der drei nationalen Höchstleistungsrechenzentren in Deutschland: dem Jülich Supercomputing Centre (JSC) im Rahmen des John von Neumann-Instituts für Computing (NIC) in Jülich, dem Leibniz-Rechenzentrum (LRZ) in Garching bei München und dem Höchstleistungsrechenzentrum Stuttgart (HLRS).\n\nOberstes Ziel des Gauss Centre for Supercomputing (GCS) ist die Förderung und Unterstützung des wissenschaftlichen Höchstleistungsrechnens. Die GCS-Supercomputer zählen zu den größten und leistungsfähigsten Höchstleistungsrechnern der Welt. Seit 2012 sind in jedem der drei GCS-Mitgliedszentren HLRS, JSC und LRZ Computersysteme installiert, die mit Rechengeschwindigkeiten im Petaflops-Bereich aufwarten. In Summe verfügt das GCS über eine Rechenleistung von mehr als 20 Petaflops, wodurch Forschern und Entwicklern aus allen wissenschaftlichen Bereichen und Vertretern der Industrie die mit Abstand leistungsfähigste Systeminfrastruktur in ganz Europa zur Verfügung steht.\n\nUnterstützt wird das GCS durch entsprechende Projektförderung vom Bundesministerium für Bildung und Forschung und den Wissenschaftsministerien der Länder Baden-Württemberg, Bayern und Nordrhein-Westfalen.\n\nVorstandsvorsitzender ist Michael M. Resch (Universität Stuttgart). Vice Chairmen sind Dieter Kranzlmüller (LRZ) und Thomas Lippert (JSC). Geschäftsführer und Leiter des Projektbüros in Bonn ist Claus Axel Müller. Der Hauptsitz des GCS befindet sich in Berlin.\n"}
{"id": "3835312", "url": "https://de.wikipedia.org/wiki?curid=3835312", "title": "Launchy", "text": "Launchy\n\nLaunchy (von : starten) ist ein Programm für Microsoft Windows und Linux, das es ermöglicht, per Eingabezeile Webseiten, Verknüpfungen und Dateien aufzurufen. Hierzu wird die Eingabe auf entsprechende Übereinstimmungen mit der internen Datenbank und vorheriger Suchanfragen überprüft.\n\nLaunchy wird kostenlos unter der freien GNU GPL-Lizenz veröffentlicht.\n\nEs können unter anderem URL (sowohl Internetadressen als auch lokale Dateien, letztere mit automatischer Vervollständigung), Namen von Dateien, Verknüpfungen, Lesezeichen der Webbrowser Internet Explorer und Mozilla Firefox sowie einfache Berechnungen verarbeitet werden. Außerdem lassen sich Kürzel für Websites und Programme definieren, denen auch ein Parameter übergeben werden kann. Der Benutzer kann außerdem bestimmen, welche Dateitypen aus welchen Ordnern in die Datenbank aufgenommen werden. Das Programm läuft im Hintergrund und wird mittels einer Tastenkombination aufgerufen.\n\nDas Programm ist durch zahlreiche Plugins und Skins erweiterbar.\n\n"}
{"id": "3845341", "url": "https://de.wikipedia.org/wiki?curid=3845341", "title": "Gesellschaft für Sprachtechnologie und Computerlinguistik", "text": "Gesellschaft für Sprachtechnologie und Computerlinguistik\n\nDie Gesellschaft für Sprachtechnologie und Computerlinguistik (GSCL) ist ein wissenschaftlicher Fachverband zur Förderung von Sprachtechnologie und Computerlinguistik in Forschung, Lehre und Beruf. Der Verein mit Sitz in München wurde 1975 – zunächst unter dem Namen \"LDV-Fittings e.V.\" – gegründet und hieß bis Oktober 2008 \"Gesellschaft für Linguistische Datenverarbeitung e.V. (GLDV)\".\n\nDie GSCL veranstaltet Fachtagungen, Workshops und Veranstaltungen zu aktuellen Themen aus Computerlinguistik und Sprachtechnologie. Spezielle Forschungs- und Anwendungsfelder (u. a. Dialogsysteme, E-Learning, Hypermedia, Korpuslinguistik, Lexikographie, maschinelle Übersetzung, Parsing, Texttechnologie) werden in Arbeitskreisen bearbeitet. Das Publikationsorgan der GSCL ist das \"Journal for Language Technology and Computational Linguistics\" (JLCL, ehemals \"LDV-Forum\"), das Fachbeiträge, Berichte, Diskussionen und Rezensionen zu Themen aus Computerlinguistik und Sprachtechnologie enthält.\n\n"}
{"id": "3846636", "url": "https://de.wikipedia.org/wiki?curid=3846636", "title": "Manchester Mark I", "text": "Manchester Mark I\n\nManchester Mark I, auch Manchester Automatic Digital Machine (MADM) genannt, war ein britischer Röhrencomputer, der 1948/49 an der University of Manchester konstruiert wurde.\n\nDer „Vater“ moderner Computer, Alan Turing, lehrte an der University of Manchester. 1948 entstand aus seinen Ideen die Small-Scale Experimental Machine, der erste auf der Von-Neumann-Architektur basierende Computer. Dieser war der Prototyp des Manchester Mark I, der von Frederic Calland Williams und Tom Kilburn noch an der Universität Manchester konstruiert wurde. Als Datenspeicher wurde ein Trommelspeicher verwendet, als Speichermedium für Programme wurden sogenannte Williamsröhren eingesetzt, die sich als extrem wartungsintensiv und empfindlich erwiesen. Trotzdem konnte Tom Kilburn die Funktionsfähigkeit des Rechners demonstrieren: er schrieb am 21. Juni 1948 ein erstes 17-Zeilen-Programm, um den höchsten Faktor einer Zahl zu berechnen.\n\nAus dem Manchester Mark I entwickelte die britische Firma Ferranti den Computer Ferranti Mark I, der nach dem Zuse Z4 der zweite kommerziell erhältliche Universal-Computer war. Er wurde erstmals im Februar 1951 ausgeliefert, kurz vor dem UNIVAC I.\n1950 hat Alan Turing 1104 Nullstellen der Riemannschen ζ-Funktion an dem Computer berechnen können.\n\nAn der Entwicklung des Manchester war unter anderem das Mathematikerehepaar Mary Lee Woods und Conway Berners-Lee beteiligt, dessen Sohn Tim Berners-Lee später die Hypertext Markup Language erfand und so zum Begründer des World Wide Web wurde.\n\n"}
{"id": "3847191", "url": "https://de.wikipedia.org/wiki?curid=3847191", "title": "Cray CX1", "text": "Cray CX1\n\nDer Cray CX1 ist ein Supercomputer der Firma Cray Research aus dem Jahr 2008, der für den Einsatz in einem Büro konzipiert wurde. Dieses in kleinem Format gebaute Gerät kostet je nach Ausstattung zwischen 25.000 und 60.000 US-Dollar (2008).\n\nDie Grundausstattung enthält:\n\n"}
{"id": "3847675", "url": "https://de.wikipedia.org/wiki?curid=3847675", "title": "MT-NewsWatcher", "text": "MT-NewsWatcher\n\nMT-NewsWatcher ist ein englischsprachiger Usenet-Newsreader für Mac OS X und Mac OS. Der Newsreader basiert ursprünglich auf dem Programm \"NewsWatcher\" von John Norstad, welches von Simon Fraser seit Mitte der 1990er Jahre intensiv weiterentwickelt und verbessert wurde und gehört mittlerweile zu den Veteranen unter den Newsreadern für den Mac.\n\nMT-NewsWatcher arbeitet ausschließlich im Online-Betrieb. Neben seiner Funktion als Newsreader kann er zusätzlich auch zum Empfang und Versenden von E-Mails eingesetzt werden. Zu seinen Besonderheiten zählen flexible Sortier- und Filtermöglichkeiten, die Reguläre Ausdrücke unterstützen und so die Navigation durch Threads und Postings erleichtern. Neben textorientierten Gruppen ist der Newsreader auch für binäre Gruppen (binaries) geeignet, da er in der Lage ist, die oft in viele Einzelartikel zerteilten und mit Verfahren wie yEnc kodierten Binärdateien zu laden und automatisch zusammenzufügen. Bilder, Filme und Audio-Dateien können direkt im Programmfenster angezeigt und abgespielt werden. Weiterhin verfügt MT-NewsWatcher über eine Spracherkennung und war der erste Newsreader, den man über Spracheingabe steuern kann. Es gibt die Möglichkeit, den Newsreader gleichzeitig auf mehreren Rechnern zu benutzen, indem man eine newsrc-Datei gemeinsam nutzt, die synchronisiert wird. Das Programm kann XFaces anzeigen und ist AppleScript-fähig.\n\n"}
{"id": "3859626", "url": "https://de.wikipedia.org/wiki?curid=3859626", "title": "Sysprep", "text": "Sysprep\n\nSysprep ist ein Hilfsprogramm von Microsoft für die automatisierte Verteilung des Betriebssystems Windows.\n\nSysprep wurde erstmals für die Verwendung mit Windows NT 4.0 ausgeliefert. Spätere Versionen waren kompatibel zu Windows 2000, Windows XP und Windows Server 2003. Letztere stehen kostenlos zum Download auf der Homepage von Microsoft zur Verfügung, zudem sind sie auf dem Installationsdatenträger enthalten. Windows Vista enthält eine Weiterentwicklung des Programms.\n\nDie Verteilung von Betriebssystemen findet in größeren Netzwerken typischerweise über Drittprogramme zum Klonen einer kompletten Betriebssysteminstallation statt. Sysprep wird zur Vorbereitung des Betriebssystems für die Verbreitung über ein solches, sogenanntes Systemabbild verwendet.\n\nWindows enthält eine Vielzahl von benutzer- und computerbezogenen Einstellungen, welche vor der Erstellung des Abbildes auf einen möglichst „neutralen“ Stand gebracht werden müssen, so dass der Einsatz auf verschiedenen Computern fehlerfrei möglich ist. Hierzu gehören unter anderem der Computername, die Zugehörigkeit zu einer Domäne, das Benutzerprofil des lokalen Administrators sowie der Security Identifier. Sysprep entfernt mit dem Parameter \"/generalize\" diese Informationen ebenso wie Treiber für Plug-and-Play-Geräte, die beim Windows-Setup hinzugefügt wurden.\n\nEin Administrator verwendet für Windows 2000, Windows XP oder Windows Server 2003 z. B. den ebenfalls mitgelieferten \"SetupMgr.exe\" beziehungsweise ab Windows Vista den \"System Image Manager\", um eine Antwortdatei zu erstellen, welche die Installationsroutine während der Installation ausliest. Hierdurch kann die Installation automatisch ablaufen, da während des Setups keine Benutzereingaben gemacht werden müssen.\n\n"}
{"id": "3860771", "url": "https://de.wikipedia.org/wiki?curid=3860771", "title": "Revit", "text": "Revit\n\nRevit ist ein mehrere Planungsprodukte umfassender Technologiezweig von Autodesk für Architekten, Gebäudetechniker und Tragwerksplaner.\nRevit basiert nicht auf AutoCAD, sondern enthält einen eigenen, neuen Grafikkernel. Während das Basisprodukt von Autodesk, AutoCAD, nicht objektorientiert arbeitet, unterstützt Revit die neue Technologie BIM (Building Information Modeling). Revit besteht aus \"Revit Structure\" für die Tragwerksplanung im konstruktiven Ingenieurbau und Hochbau, \"Revit Architecture\" für die Gebäudeplanung sowie \"Revit MEP\" für die Gebäudetechnik.\n\nDas Prinzip von Revit besteht in der Unterstützung sowohl einer 2D- wie auch 3D-Modellierung eines bauteilorientierten Gebäudemodells.\nWenn eine Änderung im Planungsprozess vorgenommen wird, wird sie automatisch im gesamten Projekt ausgeführt, so dass Entwürfe und Dokumentation immer konsistent und vollständig bleiben.\n\nRevit ermöglicht den Datenaustausch über die Branchenstandardformate RVT, DWG, DXF, DWF. Als offenes System werden auch Formate wie IFC, gbXML und PDF für die Planausgabe unterstützt.\n\nAuf dem deutschen Markt ist Revit seit 2004 erhältlich. Die Lösung für die Tragwerksplanung, Revit Structure, ist im Juli 2008 erschienen.\nIn Revit 2009 wurde das AccuRender-Rendermodul durch das mental ray-Rendermodul ersetzt. Revit bietet Schnittstellen zu Autodesk 3ds Max Design und Autodesk 3ds Max für die Highend-Visualisierung von Projekten.\n\n1997 wurde die Firma \"Charles River Software\" gegründet, die im November 1999 eine Version 0.1 von Revit herausbrachte. Als die ursprünglichen Entwickler von Revit gelten Irwin Jungreis und Leonid Raiz. Die Firma wurde im Jahre 2000 in Revit Technology Corporation gewandelt, die dann im April 2000 die 1.0 Version veröffentlichte. Noch im selben Jahr folgten die Versionen 2.0 und 2.1. Im Jahr 2001 folgten die Versionen 3.0, 3.1 und 4.0. Im Januar 2002 folgte noch die Version 4.1 bevor Autodesk Anfang April 2002 das Produkt übernahm. Es folgte eine Reihe von Updates; bis zum Jahr 2005 war man bei Version 8.1 angelangt. Zu dieser Zeit wurde Revit Structure eingeführt, das speziell für die Tragwerksplanung abgestimmte Werkzeuge bereithielt. Deshalb führte man die Bezeichnungen „Revit Building“ und „Revit Structure“ für die Zweige Architektur bzw. Tragwerksplanung ein. Im April 2007 wurde bei Autodesk eine grundlegende Namensanpassung der baurelevanten Produkte durchgeführt. Alle Produkte die auf AutoCAD basieren wurden mit „AutoCAD...“ benannt (das bis dato als „Architectural Desktop“ bekannte Programm hieß also ab sofort „AutoCAD Architecture“). Alle Produkte, die Revit als Basis hatten, erhielten entsprechend „Revit...“ als Namen. Es wurde also die Bezeichnung „Revit Architecture 2008“ für den vorwiegend architektonischen Einsatz und „Revit Structure“ für die Tragwerksplanung eingeführt.\n\nAutodesk veröffentlicht jedes Jahr eine neue Version seiner Produkte. Für die Jahreszahl der Versionen wird das Fiskaljahr von Autodesk verwendet. Daher heißt die im Mai 2008 erschienene Version „Revit Architecture 2009“.\n\n2011 ergänzte Autodesk die „Revit-Story“ mit dem Produkt „Revit MEP 2012“. Revit MEP ist der Gebäudetechnik-, bzw. TGA-Teil, der im Prinzip die gleichen Ansätze seiner Vorreiter Revit Architecture und Revit Structure verfolgt.\n\nMit Version 2013 ist Revit in entsprechenden Software-Suiten als „One Box“ erhältlich. Autodesk bündelt damit die gesamte „Revit-Story“ mit den Teilen Architecture, Structure und MEP in einer einzigen Software.\n\nDer Name Revit leitet sich aus den Kürzeln der Wörter „Revise Instantly“ ab. Diese Bezeichnung soll auf die Fähigkeit des Programms weisen, dass getätigte Änderungen sofort in alle betroffenen Ansichten übernommen werden. Ändert man z. B. die Lage der Schnittlinie in einem Grundriss, wird die Geometrie im Schnitt sofort entsprechend selbständig aktualisiert; wird ein Fenster im Schnitt verändert, passt sich die Darstellung im entsprechenden Grundriss automatisch an, etc...\n\nIn Revit sind Kollaborationswerkzeuge für die Bearbeitung im Team bereits integriert. Hierbei kann eine sogenannte Zentraldatei im Netzwerk bereitgestellt werden, auf der die Beteiligten Arbeitsbereiche zugeteilt bekommen. Über den Dienst Autodesk360 kann auch via Cloud-Computing auf die Daten zugegriffen werden.\nObjekte in Revit werden durch sogenannte Familien dargestellt. Diese speichern ihre Daten als Parametersätze. Überhaupt werden in Bauteil-Familien möglichst parametrisierte Daten bereitgestellt, um Bauteile flexibel einsetzen zu können und z. B. die Dimensionen eines Exemplares anpassen zu können.\nIm deutschsprachigen Raum ist durch Zusatz-Plugins als Subscription-Kunde die Erweiterung DACH erhältlich. Diese erweitert die Funktionalität von Revit um nationale Besonderheiten wie z. B. Auswertung der Wohnfläche nach DIN 277.\n\n\n\n"}
{"id": "3867236", "url": "https://de.wikipedia.org/wiki?curid=3867236", "title": "Imc FAMOS", "text": "Imc FAMOS\n\nimc FAMOS ist ein grafisches Computerprogramm zur Auswertung, also zum Analysieren, Beurteilen und Visualisieren von Messergebnissen. Das Programm, dessen Name ein Akronym für \"Fast Analysis And Monitoring Of Signals\" darstellt, wurde 1987 vom deutschen Hersteller imc Test & Measurement GmbH (integrated measurement & control) in Berlin für Windows 3.11 auf den Markt gebracht. Laut Hersteller bietet FAMOS eine hohe Geschwindigkeit bei der Darstellung und Bearbeitung von beliebig großen Datensätzen.\n\nFAMOS kann Daten in verschiedenen Formaten importieren, wie z. B. Excel-, Binär-, ASCII- oder Matlab-Dateien. Mit einem Dateiassistenten lassen sich ferner eigene Einlesefilter generieren. Diese Daten können dann auf verschiedene Arten grafisch dargestellt, beschriftet, kombiniert und verarbeitet werden. Daten speichern kann FAMOS in einem eigenen proprietären Format, im ASCII-Format oder als Excel-Datei.\n\nImportierte Daten können mit verschiedenen mathematischen Verfahren bearbeitet werden, entweder manuell oder in automatischen Abläufen. FAMOS bietet Zusatzmodule für Spezialoperationen wie elektronische Filter, zur Spektralanalyse und zur synchronisierten Darstellung von Daten und Videosequenzen und für das ASAM-ODS-Datenmodell. Auch können Daten über die PC-Soundkarte hörbar gemacht werden.\n\nFAMOS erlaubt das Erstellen von Dokumentationen / Berichten über den so genannten \"Reportgenerator\", welcher auf dem Bildschirm aus verschiedenen Dialogelementen und Plots, sowie Grafiken mit Steuerelementen besteht, welche beim Drucken automatisch ausgeblendet werden können. Die erstellten Berichte können mit unterschiedlichen Eingabedaten wiederverwendet werden, auch gibt es Vorlagen für halb- und vollautomatische Erstellung von Berichten.\n\n\n"}
{"id": "3869964", "url": "https://de.wikipedia.org/wiki?curid=3869964", "title": "Microsoft PhotoDraw", "text": "Microsoft PhotoDraw\n\nMicrosoft PhotoDraw ist ein Bildbearbeitungsprogramm für Vektor- und Rastergrafik von Microsoft. PhotoDraw füllte bereits zuvor als Plugin für Microsoft Office 97 eine Lücke bei der Grafikbearbeitung für Office-Anwender. Es ergänzt den Photo Editor, ein einfaches Bildbearbeitungsprogramm für Fotos und ähnliche Pixelgrafik. Im Jahr 2000 erschien PhotoDraw 2000, ein davon unabhängig entwickeltes eigenständiges Programm, dessen anspruchsvolle Funktionen mit Anwendungen wie Micrografx Designer oder Corel Draw konkurrieren sollte. Es hatte allerdings keinen nennenswerten Erfolg und wurde nach Erscheinen der zweiten Version eingestellt.\n\n1998 wurde erstmals ein Plugin zum Download freigegeben, das Microsoft Office 97 um zusätzliche Funktionen der Bildbearbeitung als reines Vektorgrafikprogramm ergänzen sollte. Zielgruppe waren somit die Office-Anwender, die aus Word, Excel und PowerPoint heraus vorhandene Vektorgrafik mit einfachen Mitteln bearbeiten wollten. Bilderstellung und Formatwandlungen sind damit jedoch nicht möglich.\n\nAuf der Basis von Picture It!, einem Einzelprogramm für Privatanwender und Bestandteil der Microsoft Works Suite wurde schließlich unabhängig vom vorherigen Draw-Plugin ein Grafikprogramm entwickelt, das als eigenständiges Programm (mit Clipart- und Schriftensammlungen auf zwei CDs) über Privatanwender hinaus neue Zielgruppen erschließen sollte. Es ist zwar Bestandteil von Office 2000, jedoch nur in der Premium- und Developer-Edition. Die zweite Programmversion ist Bestandteil von Office 2000 SR-1 (Premium / Developer). Als Einzelversion enthält sie auf drei CDs alle Clipart- und Schriftensammlungen, die in diesem Umfang zuvor nur mit Microsoft Publisher ausgeliefert wurden. PhotoDraw hat von \"Picture It!\" die Dateiendung „.MIX“ (Microsoft Image Extension) übernommen, allerdings sind die Dateien zwischen diesen beiden Programmen nicht kompatibel.\n\nDa Microsoft das Programm als Teil der Office-Suite vermarktete, die Funktionen des Programms jedoch eher für geschäftliche Anwendungen bestimmt waren, entstanden im Marketing Zielgruppenkonflikte, die einem klaren Nutzerprofil des Programms entgegenstanden.\n\nVorgestellt wurde PhotoDraw als professionelles Vektor- und Rastergrafikprogramm in Konkurrenz zu \"Adobe Illustrator\", \"CorelDRAW\", was auch in der großen Bibliothek von Clip-Arts und der gut durchdachten Sammlung von zusätzlichen Schriftarten zum Ausdruck kommt.\n\nDas proprietäre PhotoDraw-Dateiformat „.Mix“ kann von Microsoft Office 2000 und 2003 importiert werden, sowie von Adobe Illustrator. PhotoDraw beherrscht auch das Öffnen und Speichern in verschiedenen Datei-Formaten, einschließlich PNG. Bei den frühen Versionen von Office 2007 (vor Februar) kommt es zu Kompatibilitätsproblemen. Insbesondere beim Dateiaustausch mit PowerPoint ist volle Kompatibilität nur über das Dateiformat EMF (Enhanced Metafile) gegeben.\n\nMicrosoft hat sein Konzept für die Programmauswahl grundlegend überarbeitet und sowohl \"PhotoDraw\" als auch \"Picture It\" eingestellt. Auch das Nachfolgeprogramm Microsoft Digital Image mit voller Unterstützung von Photoshop-Plugins konnte sich nicht durchsetzen. Die letzte Version kam bis Mai 2007 als \"Digital Image Suite 2006\" auf den Markt, das Foto auf der Verkaufsversion erinnert mit seinen Heißluftballons an das Logo von \"Corel Draw\". Importfilter dieser Programme finden sich heute im \"Windows Bild- und Fax-Viewer\" (Bestandteil von Windows XP), mit dem \"Office Picture Manager\" steht Office-Anwendern (neben einer einfachen Fotoaufbesserung) ein Konvertierungsprogram für Vektorgrafik zur Verfügung. Weitere Bestandteile dieser Programme sind zunächst in die Vista \"Windows Live Photo Gallery\" und andere Konzepte rund um Vista eingeflossen. Für PhotoDraw selbst gibt es keinen Nachfolger mehr.\n"}
{"id": "3870486", "url": "https://de.wikipedia.org/wiki?curid=3870486", "title": "Microsoft Picture It!", "text": "Microsoft Picture It!\n\nMicrosoft Picture It! ist ein Programm zur Bildbearbeitung von Microsoft. Die Entwicklung wurde eingestellt. Das Programm erschien erstmals mit Version 1.0 im September 1996. Vom Microsoft Publisher wurde das Konzept der assistentenunterstützten Vorlagengenerierung übernommen, um für den privaten Endanwender eine einfache Bedienung grundlegender Funktionen bei der Bildbearbeitung zu ermöglichen. Das Programm arbeitet mit Vektor- und Rastergrafik. Es lassen sich Collagen, Grußkarten, Flugblätter etc. erstellen. Auch Formsatz und Serienbriefe werden angeboten. Fotos lassen sich u. a. auffrischen und retuschieren. Es bietet Alpha-Maskierung und als erstes Amateurprogramm Sprite-Erstellung. Bei Spezialeffekten sowie bei Auswahl und Montage hat das Programm Schwächen, ein Zauberstab fehlt ganz. 1999 wurde auf der Grundlage dieses Programms ein semiprofessionelles Programm für Geschäftszwecke entwickelt und als Microsoft PhotoDraw vermarktet. Das Programm hatte allerdings keinen nennenswerten Erfolg und wurde nach der zweiten Version wieder eingestellt.\n\nPicture It! verwendet die Endung „.MIX“ (Microsoft Image Extension), die auch von Microsoft PhotoDraw verwendet wird. Dennoch sind die Dateien zwischen beiden Programmen nicht kompatibel. Das Programm Microsoft Word enthält einen Importfilter für diese Bilddateien. Picture It! wurde von 2000 bis 2004 vor allem als Bestandteil der Microsoft Works Suite bekannt und konnte im Rahmen dieser OEM-Vermarktung einen gewissen Marktanteil erreichen.\n\nNach einer Neuausrichtung im Grafikbereich um 2003 wurde Picture It! erheblich überarbeitet und als Microsoft Foto Suite bzw. \"Digital Image Suite\" (USA) vermarktet, nachdem die Home-Publishing-Funktionen entfernt wurden und das Programm auf die Fotobearbeitung reduziert wurde. Die letzte Version wurde 2006 veröffentlicht und die Entwicklung danach ebenfalls eingestellt.\n\nEinzelne Programmfunktionen wurden in die Werkzeuge von Microsoft Windows integriert, wie etwa den Windows Bild- und Fax-Viewer (bereits in Windows XP) und die Windows Live Photo Gallery in Windows Vista. Auch der Picture Manager als Office-Bestandteil basiert auf den Konvertierungs- und Formatfiltern von Picture It!\n\n"}
{"id": "3874699", "url": "https://de.wikipedia.org/wiki?curid=3874699", "title": "Paldo", "text": "Paldo\n\nPaldo (Eigenschreibweise: paldo) ist eine GNU/Linux-Distribution, die auf dem selbstentwickelten Paketmanagementsystem UPKG basiert.\n\nPaldo nutzt Gnome als Desktop-Umgebung. Es kommt ursprünglich aus der Schweiz. Der Name \"Paldo\" steht für \"pure adaptable linux distribution\" („reine erweiterbare Linux-Distribution“). Die in Paldo enthaltene Software wird weitestgehend ohne Patches gegenüber der offiziellen Version verwendet. Wo dennoch Patches nötig sind, sollen diese an das Projekt weitergegeben werden.\n\nEs ist sehr einfach, die Distribution zu verändern. Man kann jedes Paket durch lokale Paketarchive verändern. In diesen kann man eigene Veränderungen am Quellcode und Spezifikationen in das System einspielen. Die Distribution zieht ihre Flexibilität aus UPKG, einem in Mono implementierten Paketverwaltungs-System.\n\nPaldo konfiguriert sich bei der Installation weitgehend selbst, ohne dass der Benutzer viele Einstellungen vornehmen muss. Ferner soll es nicht mehr als ein Programm für jede Aufgabe in der Distribution geben. Ein weiteres Ziel von Paldo ist es, die neuesten Technologien zu unterstützen.\n\nUPKG ist ein Paketmanager zweier Studenten der Eidgenössischen Technischen Hochschule Zürich. Die aktuelle Version UPKG wurde noch unter Mono programmiert. Im späteren Stadium wechselt die benutzte Programmiersprache nach Vala, einer selbstentwickelten Programmiersprache der zwei Programmierer. Mit dem Paketmanager kann man die Distribution installieren, aktualisieren oder deren CD/DVD-Medien erstellen.\n\nPaldo macht keine Unterscheidungen zwischen Closed- und Open-Source-Software.\n\nPaldo verwendet nur UPKG als eigene Software, alle anderen Softwarepakete oder Konfigurationswerkzeuge stammen aus diversen Open-Source-Projekten. Das Installationsprogramm der Live-CD ist ebenfalls distributionsspezifisch.\n\nPaldo veröffentlicht in einem Dreimonatszyklus eine neue stabile Version, die jeweils mit der aktuellen Software ausgestattet ist.\n\nPaldo kann man von einer zur nächsten Version aktualisieren, ohne das Betriebssystem neu zu installieren. UPKG führt den Upgradevorgang alleine und ohne aufwändige Konfiguration durch.\n"}
{"id": "3876132", "url": "https://de.wikipedia.org/wiki?curid=3876132", "title": "DOSEMU", "text": "DOSEMU\n\nDOSEMU (von DOS Emulation) ist ein DOS-Emulator für Linux. Das Projekt steht unter der GPL und ist damit freie Software. Die erste Version (0.1) wurde am 3. September 1992 veröffentlicht.\n\nBei DOSEMU wird der Prozessor nicht emuliert, sondern die DOS-Programme laufen auf dem wirklichen Prozessor des PCs ab. Dies ist schneller als die Methode von DOSBox, aber weniger fein steuerbar (z. B. können alte Spiele zu schnell mit DOSEMU laufen, was man bei DOSBox durch die Einstellung der Geschwindigkeit beheben kann) und funktionierte anfangs nur auf Computern mit x86-kompatibler CPU.\n\nEs wird ein DOS-Betriebssystem für DOSEMU benötigt, wobei meistens das ebenfalls freie FreeDOS verwendet wird. DOSEMU stellt nur die Umgebung bereit, somit ist das Ausführen von DOS-Programmen ohne MS-DOS-kompatibles Betriebssystem nicht möglich.\n\nDOSEMU enthält ab der Version 1.4.0 eine vollständige CPU-Emulation, die benutzt wird, wenn die CPU keinen VM86-Modus benutzen kann (weil sie im 64-Bit-Modus läuft) oder wenn die Unterstützung für den VM86-Modus im Linux-Kernel deaktiviert worden ist.\n\n\n\n"}
{"id": "3876799", "url": "https://de.wikipedia.org/wiki?curid=3876799", "title": "PDF Annotator", "text": "PDF Annotator\n\nPDF Annotator ist eine kommerzielle Anwendungssoftware zum Bearbeiten von PDF-Dateien mit Tablet PCs.\n\nDas Programm wurde zum Erstellen und Bearbeiten von PDF-Dateien mit einem Stift auf einem Tablet-PC entwickelt. Damit können PDF-Dateien wie gedruckte Dokumente mit Anmerkungen oder Markierungen per Stift versehen werden, Formulare oder Verträge ausgefüllt werden, oder auch Skizzen als PDF erstellt werden. Die Markierungen können auch wieder entfernt oder ausgeblendet werden.\n\nDas Programm kann auch auf normalen Computern mit Maus und Tastatur bedient werden. Auf Tablet-PCs bietet es eine Alternative zu Microsoft Journal, das zwar bei Tablet-PC-Versionen von Windows als Notizprogramm häufig integriert ist, aber einen speziellen Betrachter von Microsoft benötigt, der auch nur für Windows verfügbar ist. PDF ist demgegenüber ein Format, welches plattformübergreifend und standardisiert ist. Das Programm besitzt auch eine Kameraintegration, mit der erstellte Fotos sofort in das PDF übernommen werden können.\n\nDie mit PDF Annotator erstellten Markierungen sind nicht kompatibel zu Markierungen, die beispielsweise mit PDF XChange Viewer oder Adobe Acrobat erstellt wurden. Ein gemeinsames Bearbeiten von annotierten Dokumenten unter Verwendung anderer Software-Produkte als PDF Annotator ist daher nur mit Einschränkungen möglich.\n\nPDF-Annotator hat 2004 beim Microsoft Tablet PC Contest \"Does Your Application Think in Ink?\" gewonnen.\n\n\n\n"}
{"id": "3878525", "url": "https://de.wikipedia.org/wiki?curid=3878525", "title": "SilverFast", "text": "SilverFast\n\nSilverFast ist eine Software zum Scannen und zur Bildbearbeitung, die seit 1994 von der deutschen Firma LaserSoft Imaging AG entwickelt wird. Hauptaugenmerk liegt auf der Entwicklung einer Scannersoftware zum Digitalisieren analogen Bildmaterials wie Fotos, Dias, Dokumenten und Ähnlichem.\n\nDaneben gibt es weitere Anwendungen zur Bildbearbeitung unter Verwendung von Digitalkameras oder Druckern und für die 48-Bit-Rohdatenbildbearbeitung.\n\nSilverFast wird seit 1994 von der Firma LaserSoft Imaging entwickelt und vertrieben. Die erste Version wurde 1995 auf der CeBIT in Hannover vorgestellt. Weltweite Verbreitung fand die Software unter anderem durch die Produktebündelung mit Scannern verschiedener Hersteller. Einige der für SilverFast entwickelten Funktionen, insbesondere im Bereich des Farbmanagements, in der Fehlererkennung und der automatischen Staub- und Kratzerentfernung, wurden patentiert. 2008 erhielt die Software für die Steigerung des Dichteumfanges bei vielen Scannermodellen die Auszeichnung „Beste Farbmanagement-Software des Jahres 2008“ der European Digital Press Association. Die aktuelle Version 8 der Scansoftware ist seit August 2011 erhältlich, die HDR-Bildbearbeitungssoftware der gleichen Version seit 2012.\n\nSilverFast wird auf das jeweilige Scannermodell individuell abgestimmt angeboten. Bei Betrieb mehrerer Scanner ist daher der Erwerb einer entsprechenden Anzahl zusätzlicher Lizenzen notwendig, bei Neuanschaffung eines Scanners werden auch Upgradetarife angeboten. Die Lizenzkosten sind vom Scanner abhängig und orientieren sich am Anschaffungspreis des Gerätes.\n\nÜberblick über die erhältlichen Produkte:\noptional mit: Multi-Exposure, ICC-Druckerkalibrierung, PhotoProof, ColorServer\n\nVon allen Produkten bietet der Hersteller Demoversionen an.\n\nSilverFast ist eine Software zum Scannen und zur Bildbearbeitung von Fotos, Dokumenten, Dias und Ähnlichem. Sie lässt sich als Stand-Alone-Anwendung, als Photoshop-Plug-in oder als universelles TWAIN-Modul einsetzen. Es sind folgende Produktvarianten erhältlich:\n\n\nDie Software erlaubt die Anpassung sehr vieler Einstellungen, wobei auch Spezialfunktionen bestimmter Scanner, wie Staub- und Kratzererkennung und -entfernung oder Stapelverarbeitung, unterstützt werden. Die oft umfangreichen Konfigurationseinstellungen können für die spätere Wiederverwendung gespeichert werden. Automatische Kontrollen und der ScanPilot (eine Art Schritt-für-Schritt-Hilfe, die durch den gesamten Scanvorgang führt) erleichtern die Einarbeitung.\n\n„SilverFast Ai Studio“ für die Trommelscanner der Firma Heidelberger Druckmaschinen AG (Linotype - Hell) ist eine Lösung, um diese meist im Bereich der professionellen Druckvorstufe eingesetzten Scanner (Chromagraph 3300/3400, Tango/XL, Topaz, Nexscan, und Primescan) auch unter aktuellen Versionen der Betriebssysteme Microsoft Windows und Mac OS X zu verwenden.\n\nSilverFast X-Ray ist eine Variante von SilverFast Ai speziell für das Digitalisieren von Röntgenfilmen, geeignet für wissenschaftliche und medizinische Röntgenaufnahmen.\n\n„SilverFast DC“ ist eine Software zur Bearbeitung und Verwaltung von Bildern digitaler Kameras. Es enthält Funktionen wie das Einlesen der Bilddaten von der Kamera, das Bearbeiten, Optimieren und Archivieren der Bilder am Rechner sowie die Druckausgabe der fertigen Bilder.\n\n\nDer in allen DC-Varianten enthaltene virtuelle Leuchttisch ermöglicht es, digitale Bilder zu organisieren, zu verwalten und weiterzuverarbeiten. Es können hier Fotoarchive erstellt, Bilder mit Kommentaren versehen, Alben angelegt, Kontaktbögen der gesamten Arbeitsfläche erzeugt oder einzelne Bilder in hoher Auflösung und variabler Größe ausgedruckt werden.\n\nSilverFast DC Pro besitzt die Funktionalitäten der Scansoftware SilverFast Ai für digitale Fotos. Neben Standardformaten werden auch unbehandelte Rohdatenbilder unterstützt. Dieses Produkt unterstützt das Adobe-DNG-Format sowie Rohdatenformate etlicher Kamerahersteller. Für zahlreiche Kameramodelle enthält die Software vorgefertigte, speziell angepasste Profile. Mithilfe eines mitgelieferten IT8-Targets kann ein individuelles ICC-Profil für jede Kamera selbst angefertigt werden.\n\nSilverFast DC Pro Studio enthält zusätzliche Features wie die automatische Kontrastoptimierung AACO, Unterstützung des Formats JPEG 2000, Unscharfmaskierung USMPlus, das Stempelwerkzeug CloneTool und das um Text- und Layoutfunktionen erweiterte PrinTao-Modul.\n\nSilverFast HDR (High Dynamic Range) ist eine Software zur Bildbearbeitung von 48-Bit-Rohdatenbildern. Anstatt Bilder beim Scannen auf 24 Bit herunterzurechnen, ist es vielen neueren Scannern möglich, das Bild mit allen vorhandenen Informationen direkt auszugeben, d. h. es kann als 48-Bit-Rohbild gespeichert und später digital bearbeitet werden.\n\n\nSilverFast HDR besitzt die Funktionalitäten der Scansoftware SilverFast Ai für 48-Bit-Rohdaten, wie beispielsweise die Bestimmung der Ausgabegröße, Bildautomatik, Histogramm, Gradationskurven, selektive Farbkorrektur, Unschärfemaskierung, Farbseparierung, CMYK-Preview u. a. SilverFast HDR kann als natives Plugin zu Adobe Photoshop, als universelles TWAIN-Modul oder als SilverFast-Stand-Alone-Anwendung genutzt werden.\n\nDie HDR-Studio-Version bietet weitere Features, wie u. a. die automatische Kontrastoptimierung AACO, JPEG 2000, Kamera-Rohdaten-Konvertierung, Unscharfmaskierung USMPlus, PrinTao, sowie ein Stempelwerkzeug.\n\nAb Version 6.6.1 unterstützt jede SilverFast HDR-Version das Rohdaten-Dateiformat HDRi. Diese 64-Bit-HDRi-Farb-Dateien, bzw. 32-Bit-HDRi-Graustufen-Dateien enthalten neben den 48-Bit-Farb-Rohdaten, bzw. 16-Bit-Graustufenrohdaten zusätzlich 16-Bit-Infrarot-Rohdaten, die von speziellen Scannern als vierter Kanal erfasst werden können. Das HDRi-Format enthält damit sämtliche lesbaren Bild-Informationen als Rohdaten, und erlaubt deren Weiterverwendung zu einem späteren Zeitpunkt. Ein Haupteinsatzzweck dieser Informationen ist die nachträgliche Korrektur von Staub und Kratzern auf der Vorlage.\n\nUnter der Bezeichnung „SilverFast Archive Suite“ werden die beiden Programme SilverFast Ai IT8 Studio und SilverFast HDR Studio mit einem integrierten Farbmanagementsystem gebündelt angeboten. Das Produktepaket eignet sich vorwiegend für die Archivierung von Dias, Negativen und Auflichtvorlagen, mit der Möglichkeit zur nachträglichen Bearbeitung der Bilder nach dem Einscannen.\n\n\nPrinTao ist eine Software zum Drucken von Bildern und Dokumenten. Durch die Verwendung von Vorlagen („templates“) wird die Arbeit erleichtert, indem die Anzahl ähnlicher, sich wiederholender Bearbeitungsschritte reduziert wird. Es enthält eine Reihe vordefinierter Vorlagen, die durch den Anwender mit eigens entworfenen ergänzt werden können.\n\nEinige weitere Fähigkeiten:\nDas PrinTao-Modul ist in den SilverFast Ai / DC / HDR Studio-Versionen bereits enthalten.\n\nEinige Produktvarianten sind mit Zusatzfunktionen ausgestattet. Varianten mit Multi-Exposure ermöglichen ein mehrmaliges Scannen derselben Vorlage, um den Dichteumfang des Scanergebnisses zu erhöhen. Mit der \"ICC-Drucker-Kalibrierung\" können Drucker mittels eines Scanners und eines IT8-Targets kalibriert werden. \"PhotoProof\" dient zur farb- und rechtsverbindlichen Bearbeitung von Bildern digitaler Kameras, wobei auch farbverbindliche Kontraktproofs auf einem kalibrierten Drucker oder ein Softproof auf dem Bildschirm ausgegeben werden können. \"ColorServer\" ermöglicht eine automatische Farbumwandlung, Skalierung, Schärfung, Farbseparation und Optimierung von Bilddaten im 48-Bit-Farbraum. Die Daten können als High-Dynamic-Range-Daten im JPEG 2000 gespeichert werden.\n\nSilverFast ist mit einem Farbmanagementsystem ausgestattet, das eine weitgehend automatisierte Kalibrierung von Scannern mittels vom Hersteller gelieferter IT8-Targets ermöglicht. Zudem unterstützen einige Programmversionen die Kalibrierung von Druckern, indem eine Farbtafel auf diesen gedruckt und auf einem bereits kalibrierten Scanner wiederum eingelesen und analysiert wird.\n\n\n"}
{"id": "3880777", "url": "https://de.wikipedia.org/wiki?curid=3880777", "title": "Netsh", "text": "Netsh\n\nnetsh (network shell) ist ein Programm für die Microsoft Windows NT-Linie, das das Konfigurieren von lokalen und entfernten Netzwerkeinstellungen ermöglicht.\n\nEine häufige Einsatzmöglichkeit von netsh ist das Zurücksetzen des TCP/IP-Stacks, was unter Windows 98 noch einer Neuinstallation des TCP/IP-Adapters bedurfte.\n\nNetsh hat noch viele weitere Funktionen, so kann beispielsweise die IP-Konfiguration geändert werden.\n\nAnzeigen des Passwortes einer SSID:\n\nZurücksetzen des TCP/IP-Stacks:\n\nVerfügbare Schnittstellen anzeigen:\n\nFeste IP-Adresse setzen:\n\nFeste IP-Adresse und Gateway setzen:\n\nZwei feste IP-Adressen setzen:\nDynamische IP-Adresse:\n\nIm Internet Explorer eingetragenen Proxy systemweit nutzen:\n\nMit netsh können ebenfalls IPv6-Informationen aus dem Stack gelesen werden, und es ist benutzerfreundlicher als IPv6.exe, welches dieselben Informationen zur Verfügung stellt.\n\nDie IPv6-Adresse mit netsh anzeigen:\n\nSiehe auch PNRP\n\nMit netsh kann man DHCP-Server speichern und wiederherstellen.\n\nAlle DHCP Informationen sichern\n\nAlle DHCP Informationen wiederherstellen\n\nDHCP Informationen dumpen\n\n"}
{"id": "3882594", "url": "https://de.wikipedia.org/wiki?curid=3882594", "title": "Minicom", "text": "Minicom\n\nMinicom ist eine textbasierende Terminalemulation, für Modem- und serielle Kommunikation für Linux. Die Software wurde von Miquel van Smoorenburg entwickelt und gilt als einer der wichtigsten Terminalemulatoren auf Linux-Systemen und ist Bestandteil der meisten Distributionen.\n\nMinicom unterstützt ANSI- und VT100-Emulation.\n\nAufgrund der geringen Systemanforderungen ist die Software auch auf Intel-80386-Rechnern lauffähig.\n\n\n"}
{"id": "3886770", "url": "https://de.wikipedia.org/wiki?curid=3886770", "title": "Rakarrack", "text": "Rakarrack\n\nRakarrack ist ein Gitarren-Effekte-Prozessor für GNU/Linux. Rakarrack vereint unterschiedliche Effekte in einem Programm, darunter Equalizer, Kompressor, Distortion, Overdrive, Echo, Chorus, Phaser, Flanger, Reverb, Harmonizer und Wah-Wah. Weiterhin ist die Simulation diverser Lautsprecher (Cabinets) wählbar. Das Programm enthält ca. 80 typische, von Musikern erstellte Presets, die von akustischem Klang bis zu extremer Verzerrung reichen. Fertige Presets und eigene Einstellungen können in Banks gruppiert werden. Die Bedienung von Rakarrack, z. B. Wechsel zwischen den Presets einer Bank sowie Änderung einzelner Parameter ist über MIDI möglich, wobei der Wechsel bei den Presets über einen Wechsel des Midi-Instruments (Midi: Program Change) erfolgt und die Zuordnung einzelner Parameter über die Klangbeeinflussung (Midi: Control Change) geschieht und auch gelernt werden kann. Rakarrack verfolgt den Ansatz, alle typischen Effektgeräte (Bodentreter) eines Gitarristen in Software abzubilden. Neben den Sound-Effekten verfügt das Programm daher über einen Tuner und einen MIDI-Konverter.\n\nEin großer Teil der Effekte basiert auf denen des Software-Synthesizers ZynAddSubFX.\n\nRakarrack benötigt die Pro-Audio-Schnittstelle JACK. Im typischen Einsatz erzeugt das Programm nur mäßige CPU-Last; der Betrieb mit Netbooks ist daher problemlos möglich.\n\nDer Quellcode ist unter der GNU General Public License frei verfügbar. Die grafische Benutzerschnittstelle verwendet FLTK.\n\n\n"}
{"id": "3900971", "url": "https://de.wikipedia.org/wiki?curid=3900971", "title": "Rawstudio", "text": "Rawstudio\n\nRawstudio ist eine freie Software zum Lesen, Bearbeiten und Umwandeln von Bildern, die im RAW-Dateiformat digitaler Kameras vorliegen. Es verwendet das GTK+ Toolkit und ist konzipiert für die Arbeit mit einer großen Anzahl an Bildern.\n\nDurch die Verwendung von DCRaw unterstützt Rawstudio RAW-Bilder einer Vielzahl verschiedener Formate. Es bietet Farbmanagement via LittleCMS und erlaubt so die Anwendung von Farb-Profilen.\n\n\n"}
{"id": "3903434", "url": "https://de.wikipedia.org/wiki?curid=3903434", "title": "Hunger (1974)", "text": "Hunger (1974)\n\nHunger ist ein kanadischer Animationsfilm aus dem Jahre 1974. Er war einer der ersten computeranimierten Filme.\n\nEin Mann arbeitet im Büro und isst nebenbei ein paar Kleinigkeiten. Nach Büroschluss fährt er zum Supermarkt und kauft eine Unmenge von Waren ein. Danach fährt er zu einem Restaurant und bestellt sich große Mengen von Speisen und isst alles. Dabei bekommt er unzählige Münder und Arme. Als er daheim ist, legt er sich ins Bett und bekommt Bauchweh. Er nimmt Medikamente und legt sich wieder nieder. Plötzlich fängt er an zu fallen und landet inmitten lauter abgemagerter Kinder, die anfangen, ihn zu essen.\n\n\n"}
{"id": "3912227", "url": "https://de.wikipedia.org/wiki?curid=3912227", "title": "Tomato (Firmware)", "text": "Tomato (Firmware)\n\nTomato ist eine alternative Firmware für einige WLAN-Router, die auf dem Broadcom-Chipsatz basieren. Tomato ist eine Modifikation bzw. Erweiterung der Original-Firmware von Linksys. Um Tomato zu kompilieren, wird zusätzlich zum auf der Tomato-Website erhältlichen Quellcode der Quellcode der Linksys-Firmware benötigt.\n\nTomato kann anstelle der vorinstallierten Firmware des Herstellers installiert werden. Es handelt sich um eine schlanke Linux-Distribution, welche zusätzliche Funktionen gegenüber den Herstellerfirmwares bietet und flexibler angepasst werden kann. Bereitgestellt werden unter anderem ein Secure-Shell-Zugriff (BusyBox, Dropbear), eine leicht bedienbare grafische Benutzeroberfläche mit als SVG dargestellten Statistiken, IPP2P, Wake On LAN, Quality of Service, unterschiedliche Wireless-Modi beispielsweise für Bridging, Unterstützung für das Wireless Distribution System und ein CIFS-Client. Anders als bei vielen Original-Firmwares werden die meisten Einstellungen von Tomato ohne Neustart übernommen.\n\nAb Version 1.24 unterstützt Tomato auch das NAT Port Mapping Protocol.\n\nWelche Routertypen gegenwärtig unterstützt werden ist auf der Herstellerwebseite ersichtlich.\n\nÄhnliche Firmware-Alternativen sind unter anderem OpenWrt, DD-WRT, FreeWRT, HyperWRT Thibor, Tarifa oder X-Wrt.\n\n"}
{"id": "3918452", "url": "https://de.wikipedia.org/wiki?curid=3918452", "title": "Wicd", "text": "Wicd\n\nWiCD (Wireless Interface Connection Daemon) ist eine Anwendung zur Verwaltung von Netzwerkverbindungen ausschließlich für Linux. WiCD ist ein Freie-Software-Projekt, welches als Ziel hat, vor allem die drahtlosen Verbindungen einfacher und benutzerfreundlicher zu verwalten.\n\nDas Programm basiert auf Python und ist die verbreitetste Alternative zu GNOMEs NetworkManager. Die Anwendung ist in Gegensatz zu diesem nicht von GNOME-Bibliotheken abhängig, was eine hohe Attraktivität insbesondere für alleinstehende Fenstermanager wie Fluxbox oder Openbox begründet.\n\nDie erste Version 1.0.0 wurde am 14. November 2006 im englischsprachigen ubuntuforums.org veröffentlicht.\nDer ursprüngliche Name war Connection Manager, der aber aufgrund vieler Verwirrungen mit dem GNOME NetworkManager in wicd geändert wurde.\n\nWiCD ist rückwärtskompatibel zu Unix-Befehlen wie codice_1 oder codice_2 und unterstützt die gängigsten Verschlüsselungen, wie etwa Wi-Fi Protected Access (WPA), Wired Equivalent Privacy (WEP) oder \"Tunneled Transport Layer Security\" (TTLS). Es ist auch vollständig über die Konsolen-Benutzerschnittstelle (\"wicd-curses\") konfigurierbar.:\n\n"}
{"id": "3923934", "url": "https://de.wikipedia.org/wiki?curid=3923934", "title": "Flächenrückführung", "text": "Flächenrückführung\n\nUnter Flächenrückführung (engl.: \"surface reconstruction\") versteht man im Bereich des Computer-aided design den Prozess, bei dem eine Polygonfläche in \"Non-Uniform Rational B-Spline\" (\"NURBS\")-Flächen umgewandelt wird. Dafür wird eine spezielle Software verwendet. Aus mathematischer Sicht bedeutet die Umwandlung eine Reduktion der beschreibenden Parameter. Die NURBS-Fläche ist im Normalfall eine Kleinste-Quadrate-Approximation, es liegen also nicht alle originalen Punkte in der Fläche. Angewendet wird die Flächenrückführung für Freiformflächen.\n\nDie Flächenrückführung ist Teil des so genannten Reverse-engineering-Prozesses. Dazu gehört die Digitalisierung, die Filterung der gemessenen Punkte, die Umwandlung der Punktewolken in Polygonflächen und schließlich die eigentliche Flächenrückführung.\n\nDie Notwendigkeit der Flächenrückführung gründet sich auf zwei Voraussetzungen. Zum einen muss es sich bei den Objektflächen um Freiformflächen handeln, zum anderen müssen die rückgeführten Flächen Ausgangspunkt für mindestens einen weiteren Prozessschritt sein. Das können eine FEM- oder CFD-Rechnung sein oder die Weiterkonstruktion eines Bauteils.\n\nTypische Branchen für den Einsatz der Flächenrückführung sind die Automobilindustrie, die Luftfahrtbranche, der Werkzeugbau und der Schiffbau. Gebrauchte Schaufeln von Gasturbinen sind hier ein typisches Beispiel. Die rückgeführten Schaufeln werden als Grundlage für Berechnungen benutzt, um festzustellen, welchen Einfluss die Abnutzung und dadurch bedingte Geometrieveränderungen auf die Leistung haben.\n\nZahlreiche Unternehmen bieten die Flächenrückführung zusammen mit der Digitalisierung als Dienstleistung an. \n\nDie Genauigkeit der Flächenrückführung lässt sich an dem Abstand der ursprünglichen Dreieckspunkte zu der generierten NURBS-Fläche festmachen. Bei komplexen Flächen sind 1/100 mm eher als Untergrenze zu sehen, 3-5/100 mm sind praxistypische Genauigkeiten.\n\nEine Flächenrückführung mit einer herkömmlichen CAD-Software ist theoretisch möglich, aufgrund von fehlenden Mechanismen aber sehr aufwendig und deswegen nur in Ausnahmefällen sinnvoll. Es gibt deswegen zahlreiche Programme zur Flächenrückführung, etwa \"Rapidform\", \"Geomagic\", \"PolyWorks\", \"Pointmaster\", \"Rhino Reverse\", \"VRMesh\" und \"Imageware\".\n\nDer aufwändigste Teilprozess bei der Flächenrückführung ist die unten beschriebene Patchgenerierung. Einige Softwareprodukte haben dafür eine Automatisierungsfunktionalität integriert.\n\nAusgangspunkt der Flächenrückführung ist eine Polygonfläche, die aus Dreiecken aufgebaut ist. Diese Flächen werden im STL-Format gespeichert, welches binär oder ASCII sein kann. \n\nDer Hauptaufwand bei der Flächenrückführung ist die Einteilung der Polygonfläche in Patches. Einige hunderttausend Dreiecke für das Polygonnetz eines Bauteils sind keine Besonderheit. Die Einteilung wird in einem Kurvennetzwerk festgehalten. Nur sehr einfache Flächen lassen sich durch einen einzigen Patch modellieren. Da die originale Fläche eine flache Struktur aufweist, muss das Kurvennetzwerk in einer Software von Hand oder durch einen automatischen Algorithmus generiert werden. Ein wichtiges Kriterium dabei ist die lokale Krümmungsverteilung der Fläche. \n\nDie eigentliche Generierung der NURBS-Flächen (aus einem Patch wird eine NURBS-Fläche generiert) passiert dann automatisch, kann allerdings bei größeren Modellen längere Zeit (bis zu einigen Stunden) in Anspruch nehmen. Die geforderten Stetigkeiten werden dabei gleich berücksichtigt. \n\nDie Parameter der NURBS-Flächen können in einer STEP- oder IGES-Datei gespeichert werden, oder in ein proprietäres Datenformat exportiert werden.\n\nDie erzeugten Patches sind, da sie durch einfache Kurven getrennt werden, a priori formula_1-stetig, d. h. die Patches haben keine Spalte oder Überlappungen. Zusätzlich kann formula_2-Stetigkeit (keine Knicke) und formula_3-Stetigkeit (keine Sprünge in der Krümmungsverteilung) gefordert werden. Die Einhaltung von Stetigkeiten sind zusätzliche Randbedingungen und können zu einer Erhöhung der Abweichung von dem STL-Netz führen.\n\nDie Digitalisierung kann keine Kanten erfassen, deren Radius unterhalb des Punkteabstandes liegt. Deshalb müssen die Kanten bei der Flächenrückführung synthetisch generiert werden. Einige Softwareprodukte bieten hierzu Funktionen an.\n\n\n \n"}
{"id": "3924014", "url": "https://de.wikipedia.org/wiki?curid=3924014", "title": "Global Processing Unit", "text": "Global Processing Unit\n\nDie Global Processing Unit ist eine Anwendung auf Gnutella-/Pastellabasis. Die Grundidee ist es, Rechenleistung auf Rechner aufzuteilen und so bei der Berechnung von Werten effektiver und schneller zu sein.\n\nMehrere Rechner (ein Cluster) bilden durch das GPU ein Netzwerk. Auf jedem Rechner in diesem Netzwerk läuft ein Client. Dieser Client kann durch Plugins in Rechenoperationen einbezogen werden. So errechnet nicht nur ein Computer den Wert, sondern viele Computer einen. Jeder errechnete Einzelwert wird dann von den Clients auf einen Server übertragen.\nBei dieser Art von Ressourcenteilung muss jeder Clientrechner Leistung an diesen Prozess abtreten, wodurch das System verlangsamt wird.\n\nEin Cluster hat etwa eine Größe zwischen 5 und 17 Rechnern. Anwendung findet die Global Processing Unit zum Beispiel bei dem Rendern von Videos, zum Erstellen einer Suchmaschine, Berechnung digitaler Landschaften etc.\n\nLetztendlich wird durch das P2P-Netz ein Supercomputer mit enormer Rechenleistung erschaffen.\n\n"}
{"id": "3924693", "url": "https://de.wikipedia.org/wiki?curid=3924693", "title": "File Commander", "text": "File Commander\n\nFile Commander/2 für OS/2 (FC/2), File Commander/L für Linux (FC/L) sowie File Commander/W für Windows (FC/W) sind Klones des MS-DOS-Programms Norton Commander und in ihrem Look and Feel dem Original sehr ähnlich. Als Dateimanager erlauben sie das komfortable Kopieren, Löschen, Umbenennen oder Suchen von Dateien und Verzeichnissen unter OS/2 sowie unter den Windows-Versionen ab NT und 9x. Das Programm wird als Shareware vertrieben.\n\nEntwickelt wird das Programm seit 1993 (Version 0.1) von Brian Havard, einem australischen Programmierer. Geschrieben wurde es ursprünglich nur für das Betriebssystem OS/2. Seit Version 1.51 im Jahr 1997 gab es erstmals parallel dazu auch eine Windows-Version. Seit Version 2.40 werden zudem Versionen für 64-Bit-Versionen von Windows und verschiedene Linux-Distributionen angeboten.\n\nFile Commander beherrscht Komprimierungsfunktionen mit den separat zu installierenden Programmen Zip, RAR, LHarc (lzh) und dessen Nachfolger LHA, ARJ, ACE, HA, GZip, bzip2, tar, und 7z. Sowohl sein integrierter Editor als auch der integrierte Viewer beherrschen die Syntaxhervorhebung und Unicode-Text-Dateien. Auch können die Datei- und Verzeichnisnamen abhängig von bestimmten Eigenschaften in verschiedenen Farben dargestellt werden. In allen Versionen werden auch lange Dateinamen unterstützt.\n\n"}
{"id": "3936435", "url": "https://de.wikipedia.org/wiki?curid=3936435", "title": "SmarTeam", "text": "SmarTeam\n\nSmarTeam ist ein von der Firma Dassault Systèmes entwickeltes PDM/PLM-System. Es integriert sich in zahlreiche CAD-Systeme der führenden CAD-Anbieter wie AutoCAD. Für die Integration mit CATIA und SolidWorks stehen native Integrationen zur Verfügung, da diese Systeme ebenfalls von der Firma Dassault Systèmes hergestellt werden. SmarTeam bietet neben der klassischen Zeichnungsverwaltung und Stücklisten-Strukturen ein umfassendes Dokumenten-Management mit flexibler Workflow-Verwaltung. Unter anderem gibt es auch eine Integration für MS-Office. Daneben kann SmarTeam den Datenaustausch mit der Produktion, dem Verkauf oder dem Service automatisieren und vereinheitlichen. SmarTeam bietet dafür Schnittstellen zu ERP-Systemen wie beispielsweise für SAP. Die Benutzeroberfläche wurde mit dem Release 18 überarbeitet.\n\nNeben dem SmarTeam Fat Client gibt es auch ein in einem Browser nutzbares Benutzer-Interface (Thin Client) für SmarTeam. Diese Variante wird SmarTeam WebEditor genannt. SmarTeam Navigator erlaubt nur den Lesezugriff auf die SmarTeam-Inhalte. WebPDM benötigt mindestens einen zusätzlichen Web-Application-Server.\n\nIm Zusammenspiel mit der Software 3Dvia Composer können durch SmarTeam zahlreiche Funktionalitäten aus dem Gebiet der Content-Management-Systeme abgedeckt werden.\n\n"}
{"id": "3943788", "url": "https://de.wikipedia.org/wiki?curid=3943788", "title": "XCSoar", "text": "XCSoar\n\nXCSoar ist ein Endanflugrechner mit Moving-Map für die Betriebssysteme Android, Linux, macOS, Windows und Windows CE. Ursprünglich wurde es als kommerzielle Software veröffentlicht, bis Anfang 2005 der Quellcode freigegeben wurde. Eine Open-Source-Community hat sich des Codes angenommen, ihn verbessert und weiterentwickelt.\n\nXCSoar hat seit dieser Zeit viele neue Features erhalten, die nun auch in den meisten Fällen vergleichbar mit den bekannten kommerziellen Produkten sind und teilweise darüber hinausgehen.\n\n"}
{"id": "3944839", "url": "https://de.wikipedia.org/wiki?curid=3944839", "title": "Computervirus", "text": "Computervirus\n\nEin Computervirus ( ‚Gift, Schleim‘; im Deutschen neutralen, auch maskulinen Geschlechts, Plural \"-viren\") ist ein sich selbst verbreitendes Computerprogramm, welches sich in andere Computerprogramme, einen Bootsektor oder den RAM einschleust, und sich damit reproduziert. Die Klassifizierung als Virus bezieht sich hierbei auf die Verbreitungs- und Infektionsfunktion.\n\nEinmal gestartet, kann es Veränderungen am Betriebssystem oder an weiterer Software vornehmen (Schadfunktion), mittelbar auch zu Schäden an der Hardware führen. Als typische Auswirkung sind Datenverluste möglich. Computerviren beeinträchtigen die Computersicherheit und zählen zur Malware.\n\nDer Ausdruck \"Computervirus\" wird umgangssprachlich auch für Computerwürmer und Trojanische Pferde genutzt, da es oft Mischformen gibt und für Anwender der Unterschied kaum zu erkennen ist.\n\nWie sein biologisches Vorbild benutzt ein Computervirus die Ressourcen seines Hauptprozessors, und schadet ihm dabei häufig, schlimmsten falls werden ihre Bankdaten ausgelesen und verwendet. Auch vermehrt es sich meist unkontrolliert. Durch vom Virenautor eingebaute Schadfunktionen oder durch Fehler im Virus kann das Virus das Wirtssystem oder dessen Programme auf verschiedene Weisen beeinträchtigen, von harmloseren Störungen oder Datenverlust bis zu Hardwareschäden.\n\nViren sind oft in einem Wirtsprogramm eingebettet. Wird dieses Wirtsprogramm aufgerufen, wird das Virus ausgeführt, und kann sich weiter verbreiten.\n\nHeutzutage sind Computerviren fast vollständig von Würmern verdrängt worden, da fast jeder Rechner an das Internet oder lokale Netze angeschlossen ist und die aktive Verbreitungsstrategie der Würmer in kürzerer Zeit eine größere Verbreitung ermöglicht. Viren sind nur noch in neuen Nischen () von Bedeutung.\n\nComputerviren und -Würmer verbreiten sich beide auf Rechnersystemen, jedoch basieren sie zum Teil auf vollkommen verschiedenen Konzepten und Techniken.\n\nEin Virus verbreitet sich, indem es sich selbst in noch nicht infizierte Dateien kopiert und diese so anpasst, dass das Virus mit ausgeführt wird, wenn das Wirtsprogramm gestartet wird. Zu den infizierbaren Dateien zählen normale Programmdateien, Programmbibliotheken, Skripte, Dokumente mit Makros oder anderen ausführbaren Inhalten sowie Bootsektoren (auch wenn letztere normalerweise vom Betriebssystem nicht als Datei repräsentiert werden).\n\nDie Verbreitung auf neue Systeme erfolgt durch Kopieren einer infizierten Wirtsdatei auf das neue System durch einen Anwender. Dabei ist es unerheblich, auf welchem Weg diese Wirtsdatei kopiert wird: Früher waren die Hauptverbreitungswege Wechselmedien wie Disketten, heute sind es Rechnernetze (zum Beispiel via E-Mail zugesandt, von FTP-Servern, Web-Servern oder aus Tauschbörsen heruntergeladen). Es existieren auch Viren, die Dateien in freigegebenen Ordnern in lokalen Netzwerken infizieren, wenn sie entsprechende Rechte besitzen.\n\nIm Gegensatz zu Viren warten Würmer nicht passiv darauf, von einem Anwender auf einem neuen System verbreitet zu werden, sondern versuchen, aktiv in neue Systeme einzudringen. Sie nutzen dazu Sicherheitsprobleme auf dem Zielsystem aus, wie zum Beispiel:\n\n\nEin Wurm kann sich dann wie ein Virus in eine andere Programmdatei einfügen; meistens versucht er sich jedoch nur an einer unauffälligen Stelle im System mit einem unauffälligen Namen zu verbergen und verändert das Zielsystem so, dass beim Systemstart der Wurm aufgerufen wird (wie etwa die Autostart-Funktion in Windows-Systemen).\n\nIn der Umgangssprache werden Computerwürmer wie „I Love You“ oft als Viren bezeichnet, da der Unterschied für Anwender oft nicht ersichtlich ist.\n\nDas verwendete Betriebssystem hat großen Einfluss darauf, wie hoch die Wahrscheinlichkeit einer Virusinfektion ist oder wie hoch die Wahrscheinlichkeit für eine systemweite Infektion ist. Grundsätzlich sind alle Betriebssysteme anfällig, die einem Programm erlauben, eine andere Datei zu manipulieren. Ob Sicherheitssysteme wie beispielsweise Benutzerrechtesysteme vorhanden sind und verwendet werden, beeinflusst, inwieweit sich ein Virus auf einem System ausbreiten kann.\n\nBetriebssysteme ohne jegliche Rechtesysteme wie etwa MS-DOS, auf MS-DOS basierende Windows- oder Amiga-Systeme sind die anfälligsten Systeme. Wenn der Benutzer ausschließlich als Administrator arbeitet und somit das Rechtesystem des Betriebssystems nicht eingreifen kann, sind jedoch auch neuere Windows-Versionen, Unix und unixoide Systeme wie Linux und macOS genauso anfällig.\n\nBesonders bei Windows NT und darauf basierenden Systemen wie Windows 2000 oder XP besteht das Problem, dass zwar ein gutes Benutzerrechtesystem vorhanden ist, dieses aber in der Standardeinstellung nicht verwendet wird, um die Rechte des Anwenders einzuschränken. Ein Grund dafür ist, dass nach der Installation von einigen Windows-Versionen die automatisch eingerichteten Benutzerkonten Administratorenrechte besitzen. Anders jedoch ab Windows Vista, wo die Einrichtung eines Standardkontos nicht die vollen Administratorrechte hat, und mit Hilfe der Benutzerkontensteuerung (UAC) wird zudem das System geschützt. Die meisten Linux-Distributionen richten bei der Installation ein Nutzerkonto ohne administrative Rechte ein, so dass beim normalen Benutzen des Computers zunächst nur beschränkte Rechte zur Verfügung stehen und nur der spezielle Root-Account Administratorenrechte besitzt.\n\nWenn ein Anwender mit einem Benutzerkonto mit eingeschränkten Rechten arbeitet, kann ein Virus sich nur auf Dateien verbreiten, für die der Benutzer die entsprechenden Rechte zur Veränderung besitzt. Dieses bedeutet normalerweise, dass Systemdateien vom Virus nicht infiziert werden können, solange der Administrator oder mit Administratorrechten versehene Systemdienste nicht Dateien des infizierten Benutzers aufrufen. Eventuell auf dem gleichen System arbeitende Benutzer können meist ebenfalls nicht infiziert werden, solange sie nicht eine infizierte Datei des infizierten Benutzers ausführen oder die Rechte des infizierten Benutzers es erlauben, die Dateien von anderen Benutzern zu verändern.\n\nDa Windows-Systeme heute die weiteste Verbreitung auf PCs haben, sind sie derzeit das Hauptziel von Virenautoren. Die Tatsache, dass sehr viele Windows-Anwender mit Konten arbeiten, die Administratorrechte haben, sowie die Unkenntnis von Sicherheitspraktiken bei der relativ hohen Zahl unerfahrener Privatanwender macht Windows-Systeme noch lohnender als Ziel von Virenautoren.\n\nWährend für Windows-Systeme über hunderttausende Viren bekannt sind, liegt die Zahl der bekannten Viren für Linux und das klassische Mac OS deutlich niedriger.\nIn „freier Wildbahn“ werden allerdings weitaus weniger verschiedene Viren beobachtet, als theoretisch bekannt sind.\nDas erste Virus für Apples Mac-OS-X-Betriebssystem wurde am 13. Februar 2006 im Forum einer US-amerikanischen Gerüchteseite veröffentlicht. Bis dahin galt das Betriebssystem der Macintosh-Computer als gänzlich von Viren und Würmern unbelastet. Der Hersteller von Windows-Antivirenprogrammen Sophos stellt in seinem \"Security Report 2006\" öffentlich fest, dass Mac OS X sicherer sei als Windows.\n\nBei Unix- und Linux-Systemen sorgen ebenfalls die hohen Sicherheitsstandards und die geringe Verbreitung dieser Systeme bei Endanwendern dafür, dass sie für Virenautoren momentan kein lohnendes Ziel darstellen und Viren „in freier Wildbahn“ praktisch nicht vorkommen. Anders sieht es bei Computerwürmern aus. Unix- bzw. Linux-Systeme sind wegen der hohen Marktanteile bei Internet-Servern mittlerweile ein häufiges Ziel von Wurmautoren.\n\nAnwender sollten niemals unbekannte Dateien oder Programme aus unsicherer Quelle ausführen und generell beim Öffnen von Dateien Vorsicht walten lassen. Das gilt insbesondere für Dateien, die per E-Mail empfangen wurden. Solche Dateien – auch harmlos erscheinende Dokumente wie Bilder oder PDF-Dokumente – können durch Sicherheitslücken in den damit verknüpften Anwendungen auf verschiedene Weise Schadprogramme aktivieren. Daher ist deren Überprüfung mit einem aktuellen Antivirenprogramm zu empfehlen.\n\nBetriebssystem und Anwendungen sollten regelmäßig aktualisiert werden und vom Hersteller bereitgestellte Service Packs und Patches/Hotfixes eingespielt werden. Dabei ist zu beachten, dass es einige Zeit dauern kann, bis Patches bereitgestellt werden. Einige Betriebssysteme vereinfachen diese Prozedur, indem sie das automatische Herunterladen und Installieren von Aktualisierungen unterstützen. Manche unterstützen sogar das gezielte Herunterladen und Installieren nur derjenigen Aktualisierungen, die sicherheitskritische Probleme beheben. Dazu gibt es auch die Möglichkeit, die Service Packs und Hotfixes für Windows 2000 und Windows XP via „Offline-Update“ einzuspielen. Diese Offline-Updates sind besonders bei neuen PCs zu empfehlen, da andernfalls der PC bereits beim ersten Verbinden mit dem Internet infiziert werden könnte.\n\nDie eingebauten Schutzfunktionen des Betriebssystems sollten ausgenutzt werden. Dazu zählt insbesondere, nicht als Administrator mit allen Rechten, sondern als Nutzer mit eingeschränkten Rechten zu arbeiten, da dieser keine Software systemweit installieren darf.\n\nDas automatische Öffnen von Dateien aus dem Internet sowie das automatische Ausblenden von bekannten Dateianhängen sollte deaktiviert werden, um nicht versehentlich Dateien auszuführen, die man sonst als getarnten Schädling erkennen würde. Auch durch die Autostartfunktion für CD-ROMs und DVD-ROMs können Programme bereits beim Einlegen eines solchen Datenträgers ausgeführt und damit ein System infiziert werden.\n\nEs existieren auch Computerviren für Nicht-Microsoft-Betriebssysteme wie Symbian OS, Linux, Mac OS und Betriebssysteme der BSD-Reihe. Da diese Viren jedoch kaum verbreitet sind, stellen sie für den Benutzer keine große Gefahr dar. Ein Grund dafür ist einerseits die geringere Verbreitung dieser Plattformen (deren Verbreitung lag Anfang 2009 bei ca. fünf Prozent), sodass Virenentwickler diese Systeme in der Vergangenheit eher verschont haben und es andererseits für die Schadprogramme eine erhebliche Schwierigkeit bietet, weitere Infektionsopfer zu finden. Ein weiterer, technischer Grund ist die explizite Rechtetrennung vieler anderer Betriebssysteme. Bei quelloffenen Betriebssystemen kommt noch hinzu, dass es viele verschiedene Distributionen gibt, was wiederum eine Einschränkung für Viren darstellt.\n\nPersonal Firewalls zeigen gegen Viren keine Wirkung, da ihre Funktionalität auf die Arbeitsweise von Würmern zugeschnitten ist und Viren nicht beeinträchtigt.\n\nAntivirenprogramme schützen im Wesentlichen nur vor bekannten Viren. Daher ist es bei der Benutzung eines solchen Programms wichtig, regelmäßig die von den Herstellern bereitgestellten aktualisierten Virensignaturen einzuspielen. Viren der nächsten Generation (Tarnkappenviren) können von Antivirensoftware fast nicht mehr erkannt werden (siehe auch Rootkit).\n\nMit Hilfe dieser Programme werden Festplatte und Arbeitsspeicher nach schädlichen Programmen durchsucht. Antivirenprogramme bieten meist zwei Betriebsmodi: einen manuellen, bei dem das Antivirenprogramm erst auf Aufforderung des Benutzers alle Dateien einmalig überprüft (\"on demand\"), und einen automatischen, bei dem alle Schreib- und Lesezugriffe auf die Festplatte und teilweise auch auf den Arbeitsspeicher überprüft werden (\"on access\"). Es gibt Antivirenprogramme, die mehrere für das Scannen nach Viren verantwortliche Programmmodule (\"engines\") nutzen. Wenn diese unabhängig voneinander suchen, steigt die Erkennungswahrscheinlichkeit.\n\nAntivirenprogramme bieten nie vollständigen Schutz, da die Erkennungsrate selbst bei bekannten Viren nicht bei 100 % liegt. Unbekannte Viren können von den meisten dieser Programme anhand ihres Verhaltens entdeckt werden („Heuristik“); diese Funktionen arbeiten jedoch sehr unzuverlässig. Auch entdecken Antivirenprogramme Viren oft erst nach der Infektion und können das Virus unter Umständen nicht im normalen Betrieb entfernen.\n\nBesteht der berechtigte Verdacht einer Infektion, sollten nacheinander mehrere On-Demand-Programme eingesetzt werden. Dabei ist es sinnvoll, darauf zu achten, dass die Programme unterschiedliche \"Engines\" nutzen, damit die Erkennungsrate steigt. Es gibt Antivirenprogramme verschiedener Hersteller, welche die gleichen Scan-Methoden anwenden, also damit auch ein ähnliches Risiko haben, bestimmte Viren zu übersehen. Verschiedene On-Access-Antivirenprogramme („Wächter“, „Guard“, „Shield“ usw.) sollten nie gleichzeitig installiert werden, weil das zu Fehlfunktionen des PC führen kann: Da viele dieser On-Access-Scanner bereits beim Hochfahren des Betriebssystems nach Bootsektorviren suchen, werden sie quasi gleichzeitig gestartet und versuchen einen alleinigen und ersten Zugriff auf jede zu lesende Datei zu erlangen, was naturgemäß unmöglich ist und daher zu schweren Systemstörungen führen kann bzw. muss.\n\nWerden mehrere On-Demand-Scanner installiert und – auch unabhängig, also nicht gleichzeitig – gestartet und ausgeführt, sind falsche Virenfunde häufig, bei denen das eine Programm die Virensignaturen des anderen auf der Festplatte oder im Arbeitsspeicher als Virus erkennt bzw. schon gesicherte Virendateien im sogenannten „Quarantäne-Ordner“ des anderen Programms findet. Auch ein On-Access-Scanner kann deshalb bei einem zusätzlich gestarteten On-Demand-Scanvorgang eines anderen Virensuchprogramms im Konkurrenzprodukt also fälschlich eine oder mehrere Viren finden.\n\nGrundsätzlich sollte gelegentlich, aber regelmäßig der gesamte PC auf Viren untersucht werden, da – mit Hilfe neuer Virensignaturen – alte, früher nicht erkannte Virendateien entdeckt werden können und darüber hinaus auch die „Wächtermodule“ ein und desselben Herstellers manchmal anders suchen und erkennen als der zugehörige On-Demand-Scanner.\n\nLive-Systeme wie Knoppix, die unabhängig vom installierten Betriebssystem von einer CD gestartet werden, bieten nahezu vollständigen Schutz, wenn keine Schreibgenehmigung für die Festplatten erteilt wird. Weil keine Veränderungen an Festplatten vorgenommen werden können, kann sich kein schädliches Programm auf der Festplatte einnisten. Speicherresidente Malware kann aber auch bei solchen Live-Systemen Schaden anrichten, indem diese Systeme als Zwischenwirt oder Infektionsherd für andere Computer dienen können. Malware, die direkt im Hauptspeicher residiert, wird erst bei einem Reboot unschädlich gemacht.\n\n\"Bootviren\" zählen zu den ältesten Computerviren. Diese Viren waren bis 1995 eine sehr verbreitete Form von Viren. Ein Bootsektorvirus infiziert den Bootsektor von Disketten und Festplattenpartitionen oder den Master Boot Record (MBR) einer Festplatte.\n\nDer Bootsektor ist der erste physische Teil einer Diskette oder einer Festplattenpartition. Festplatten haben außerdem einen sogenannten Master Boot Record. Dieser liegt wie der Bootsektor von Disketten ganz am Anfang des Datenträgers. Bootsektoren und MBR enthalten mit den Bootloadern die Software, die von einem Rechner direkt nach dessen Start ausgeführt wird, sobald die Firmware bzw. das BIOS den Rechner in einen definierten Startzustand gebracht hat. Üblicherweise laden Boot-Loader das installierte Betriebssystem und übergeben diesem die Kontrolle über den Computer.\n\nWie beschrieben sind Boot-Loader solche Programme, die vor dem Betriebssystem ausgeführt werden und deshalb für Viren sehr interessant: Bootviren können in das Betriebssystem, das nach ihnen geladen wird, eingreifen und dieses manipulieren oder komplett umgehen. Dadurch können sie sich beispielsweise auf Bootsektoren eingelegter Disketten verbreiten.\n\nLädt ein Rechner nicht den MBR der Festplatte, sondern den infizierten Bootsektor einer Diskette, versucht das enthaltene Bootvirus meist, sich in den MBR der Festplatte zu verbreiten, um bei jedem Start des Computers ohne Diskette aktiv werden zu können.\n\nBootviren haben jedoch mit den technischen Limitierungen, die mit dem Speicherort „Bootsektor“ oder vor allem „MBR“ einhergehen, zu kämpfen: Sie können maximal 444 Bytes groß sein, sofern sie nicht noch weitere Bestandteile auf anderen Bereichen der Festplatte verstecken. Der MBR ist nach Industrienorm ein Sektor, also 512 Byte groß, aber einige Bytes werden für die Hardware- und BIOS-Kompatibilität verbraucht. Außerdem müssen sie die Aufgaben des Boot-Loaders übernehmen, damit das System funktionsfähig bleibt, was von dem ohnehin schon sehr geringen Platz für die Virenlogik noch weiteren Platz wegnimmt. Da sie vor einem Betriebssystem aktiv werden, können sie außerdem nicht auf von einem Betriebssystem bereitgestellte Funktionen wie das Finden und Öffnen einer Datei zurückgreifen.\n\nSeit 2005 gibt es auch Bootsektorviren für CD-ROMs. Diese infizieren bootfähige CD-ROM-Abbilddateien. Es ist technisch möglich, einen Bootsektorvirus für ein bootfähiges lokales Netzwerk oder für einen USB-Stick zu erstellen, dies ist aber bis jetzt noch nicht geschehen.\n\nHeutzutage gibt es beinahe keine Bootsektorviren mehr, da BIOS und Betriebssysteme meistens einen gut funktionierenden Schutz vor ihnen haben. Zwar gibt es Bootsektorviren, die diesen Schutz umgehen können, doch ist ihre Verbreitung im Allgemeinen sehr langsam. Durch die technischen Probleme, die mit diesem Virentyp einhergehen, fordern sie vom Virenautor außerdem deutlich mehr Wissen und Programmierfertigkeiten als bei anderen Virenformen notwendig, während sie zugleich seine Möglichkeiten stark einschränken.\n\nLinkviren oder Dateiviren sind der am häufigsten anzutreffende Virentyp. Sie infizieren ausführbare Dateien oder Programmbibliotheken auf einem Betriebssystem.\n\nUm eine ausführbare Datei zu infizieren, muss das Virus sich in diese Wirtsdatei einfügen (oft direkt am Ende, da dies am einfachsten ist). Außerdem modifiziert das Virus die Wirtsdatei so, dass das Virus beim Programmstart aufgerufen wird. Eine spezielle Form von Linkviren wählt eine andere Strategie und fügt sich in eine bestehende Programmfunktion ein.\n\nZu den verschiedenen Arten von Linkviren siehe Infektionsarten.\n\nMakroviren benötigen Anwendungen, die Dokumente mit eingebetteten Makros verarbeiten. Sie befallen Makros in nicht-infizierten Dokumenten oder fügen entsprechende Makros ein, falls diese noch nicht vorhanden sind.\n\nMakros werden von vielen Office-Dokument-Typen verwendet. Aber auch andere Dokumentdateien können Makros enthalten. Sie dienen normalerweise dazu, in den Dokumenten wiederkehrende Aufgaben zu automatisieren oder zu vereinfachen.\n\nHäufig unterstützen Anwendungen mit solchen Dokumenten ein spezielles Makro, das automatisch nach dem Laden des Dokuments ausgeführt wird. Dies ist ein von Makroviren bevorzugter Ort für die Infektion, da er die höchste Aufrufwahrscheinlichkeit hat. Wie Linkviren versuchen auch Makroviren, noch nicht infizierte Dateien zu befallen.\n\nDa die meisten Anwender sich nicht bewusst sind, dass beispielsweise ein Textdokument ausführbare Inhalte und damit ein Virus enthalten kann, gehen sie meist relativ sorglos mit solchen Dokumenten um. Sie werden sehr oft an andere Anwender verschickt oder auf öffentlichen Servern zum Herunterladen angeboten. Dadurch können sich Makroviren recht gut verbreiten. Um das Jahr 2000 herum stellten sie die größte Bedrohung dar, bis sie darin von den Computerwürmern abgelöst wurden.\n\nEin Schutz gegen Makroviren besteht darin, dafür zu sorgen, dass nur zertifizierte Makros von der Anwendung ausgeführt werden. Dies ist insbesondere für (größere) Unternehmen und Behörden von Interesse, wo eine zentrale Zertifizierungsstelle Makros zum allgemeinen Gebrauch vor deren Freigabe überprüft und akzeptierte Makros zertifiziert.\n\nEs empfiehlt sich weiterhin, das automatische Ausführen von Makros in der entsprechenden Anwendung auszuschalten.\n\nEin Skript ist ein Programm, welches nicht durch einen Kompilierer in Maschinensprache übersetzt wird, sondern durch einen Interpreter Schritt für Schritt ausgeführt wird. Ein Skript wird häufig auf Webservern verwendet (zum Beispiel in Form der Skriptsprache Perl oder PHP) bzw. durch in Webseiten eingebettete Skriptsprachen (zum Beispiel JavaScript).\n\nEin Skript wird gerne in Webseiten zusätzlich zu normalem HTML oder XML eingesetzt, um Funktionen zu realisieren, die sonst nur unter Zuhilfenahme ausführbarer Programme auf dem Server (CGI-Programme) realisierbar wären. Solche Funktionen sind zum Beispiel Gästebücher, Foren, dynamisch geladene Seiten oder Webmailer. Skriptsprachen sind meist vom Betriebssystem unabhängig. Um ein Skript auszuführen, wird ein passender Interpreter – ein Programm, das das Skript von einer für den Menschen lesbaren Programmiersprache in eine interne Repräsentation umsetzt und dann ausführt – benötigt. Wie alle anderen Viren auch sucht das Skriptvirus eine geeignete Wirtsdatei, die es infizieren kann.\n\nIm Falle von HTML-Dateien fügt sich das Skriptvirus in einen speziellen Bereich, den Skriptbereich, einer HTML-Datei ein (oder erzeugt diesen). Die meisten Browser laden diesen Skriptbereich des HTML-Dokuments, um ihn schließlich auszuführen. Diese speziellen Skriptviren verhalten sich also fast genauso wie die oben beschriebenen Makroviren.\n\nUnix-, Mac-OS-X- und Linux-Systeme benutzen für die Automatisierung vieler Aufgaben Skripte, welche zum Beispiel für eine Unix-Shell wie Bash, in Perl oder in Python geschrieben wurden. Die Kommandozeileninterpreter aus MS-DOS und Windows können ebenfalls spezielle Skripte ausführen. Auch für diese Skriptsprachen gibt es Viren, die allerdings nur Laborcharakter haben und in der „freien Wildbahn“ so gut wie nicht anzutreffen sind. Sie können außerdem nicht, wie in HTML eingebettete Skriptviren, versehentlich eingefangen werden, sondern man muss – wie bei einem Linkvirus – erst ein verseuchtes Skript auf sein System kopieren und ausführen.\n\nNicht alle Computerviren fallen eindeutig in eine spezielle Kategorie. Es gibt auch Mischformen wie zum Beispiel Viren, die sowohl Dateien als auch Bootsektoren infizieren (Beispiel: Kernelviren) oder Makroviren, die auch Programmdateien infizieren können. Bei der Zusammensetzung ist beinahe jede Variation möglich.\n\nDie EICAR-Testdatei ist eine Datei, die benutzt wird, um Virenscanner zu testen. Sie ist kein Virus und enthält auch keinen „viralen“ Inhalt, sondern ist nur per Definition als Virus zu erkennen. Jeder Virenscanner sollte diese Datei erkennen. Sie kann deswegen benutzt werden, um auf einem System – das von keinem Virus infiziert wurde – zu testen, ob der Virenscanner korrekt arbeitet.\n\nCompanion-Viren infizieren nicht die ausführbaren Dateien selbst, sondern benennen die ursprüngliche Datei um und erstellen eine Datei mit dem ursprünglichen Namen, die nur das Virus enthält, oder sie erstellen eine Datei mit ähnlichem Namen, die vor der ursprünglichen Datei ausgeführt wird. Es handelt sich also nicht um ein Virus im eigentlichen Sinne, da kein Wirtsprogramm manipuliert wird.\n\nUnter MS-DOS gibt es beispielsweise Companion-Viren, die zu einer ausführbaren EXE-Datei eine versteckte Datei gleichen Namens mit der Endung „.com“ erstellen, die dann nur das Virus enthält. Wird in der Kommandozeile von MS-DOS ein Programmname ohne Endung eingegeben, sucht das Betriebssystem zuerst nach Programmen mit der Endung „.com“ und danach erst nach Programmen mit der Endung „.exe“, so dass der Schädling vor dem eigentlichen Programm in der Suchreihenfolge erscheint und aufgerufen wird. Der Schädling führt, nachdem er sich meist im Arbeitsspeicher festgesetzt hat, das ursprüngliche Programm aus, so dass der Benutzer oft nichts von der Infektion bemerkt.\n\nÜberschreibende Computerviren sind die einfachste Form von Viren, wegen ihrer stark zerstörenden Wirkung aber am leichtesten zu entdecken. Wenn ein infiziertes Programm ausgeführt wird, sucht das Virus nach neuen infizierbaren Dateien und überschreibt entweder die ganze Datei oder nur einen Teil derselben (meist den Anfang) mit einer benötigten Länge. Die Wirtsdatei wird dabei irreparabel beschädigt und funktioniert nicht mehr oder nicht mehr korrekt, wodurch eine Infektion praktisch sofort auffällt.\n\nDiese Art von Computerviren fügt sich am Anfang der Wirtsdatei ein. Beim Ausführen der Wirtsdatei wird zuerst das Virus aktiv, das sich weiterverbreitet oder seine Schadwirkung entfaltet. Danach stellt das Virus im Arbeitsspeicher den Originalzustand des Wirtsprogramms her und führt dieses aus. Außer einem kleinen Zeitverlust merkt der Benutzer nicht, dass ein Virus gerade aktiv wurde, da die Wirtsdatei vollkommen arbeitsfähig ist.\n\nEin \"Appender-Virus\" fügt sich an das Ende einer zu infizierenden Wirtsdatei an und manipuliert die Wirtsdatei derart, dass es vor dem Wirtsprogramm zur Ausführung kommt. Nachdem das Virus aktiv geworden ist, führt es das Wirtsprogramm aus, indem es an den ursprünglichen Programmeinstiegspunkt springt. Diese Virusform ist leichter zu schreiben als ein Prepender, da das Wirtsprogramm nur minimal verändert wird und es deshalb im Arbeitsspeicher nicht wiederhergestellt werden muss. Da Appender einfach zu implementieren sind, treten sie relativ häufig auf.\n\nDer Fachbegriff „Entry Point Obscuring“ (kurz: EPO) heißt übersetzt „Verschleierung des Einsprungspunktes“. Viren, die diese Technik benutzen, suchen sich zur Infektion einen bestimmten Punkt in der Wirtsdatei, der nicht am Anfang oder am Ende liegt. Da dieser Punkt von Wirt zu Wirt variiert, sind Viren dieses Typs relativ schwierig zu entwickeln, da unter anderem eine Routine zum Suchen eines geeigneten Infektionspunktes benötigt wird. Der Vorteil für diesen Virentyp besteht darin, dass Virenscanner die gesamte Datei untersuchen müssten, um EPO-Viren zu finden – im Gegensatz zum Erkennen von Prepender- und Appender-Viren, bei denen der Virenscanner nur gezielt Dateianfang und -ende untersuchen muss. Sucht ein Virenscanner also auch nach EPO-Viren, benötigt er mehr Zeit – wird der Virenscanner so eingestellt, dass er Zeit spart, bleiben EPO-Viren meist unentdeckt.\n\nFür das Entry Point Obscuring sucht sich das Virus einen speziellen Ort, wie etwa eine Programmfunktion, irgendwo in der Datei, um diese zu infizieren. Besonders lohnend ist zum Beispiel die Funktion zum Beenden des Programms, da sie meist ein leicht zu identifizierendes Erkennungsmuster hat und genau einmal aufgerufen wird. Würde das Virus eine zeitkritische Funktion oder eine sehr häufig aufgerufene Funktion infizieren, fiele es leichter auf. Das Risiko für EPO-Viren besteht darin, dass sie sich unter Umständen einen Punkt in einem Wirt aussuchen können, der nie oder nicht bei jeder Ausführung des Wirtes aufgerufen wird.\n\nSpeicherresidente Viren verbleiben auch nach Beendigung des Wirtprogramms im Speicher. Unter MS-DOS wurde eine Technik namens TSR (Terminate and Stay Resident) verwendet, in Betriebssystemen wie Windows, Unix oder Unix-ähnlichen Systemen (Linux, Mac OS X) erzeugt das Virus einen neuen Prozess. Das Virus versucht dem Prozess in diesem Fall einen unverdächtig wirkenden Prozessnamen zu geben oder seinen Prozess komplett zu verstecken. Gelegentlich versuchen diese Viren auch Funktionen des Betriebssystems zu manipulieren oder auf sich umzuleiten, sofern das Betriebssystem dieses ermöglicht bzw. nicht verhindert.\n\nComputerviren dieser Art ergreifen besondere Maßnahmen, um ihre Existenz zu verschleiern. So werden Systemaufrufe abgefangen, so dass zum Beispiel bei der Abfrage der Größe einer infizierten Datei die Größe vor der Infektion angegeben wird (manche Viren verändern die ursprüngliche Größe auch gar nicht, weil sie sich in unbenutzte Bereiche der Datei kopieren) oder auch beim Lesen der Datei die Daten der ursprünglichen Datei zurückgeben.\nDieser Typ von Viren verschlüsselt sich selbst. Der Schlüssel kann dabei von Infektion zu Infektion variieren. Das soll Antivirenprogramme daran hindern, einfach nach einer bestimmten Zeichenfolge in Dateien suchen zu können. Die Routine zum Entschlüsseln muss aber naturgemäß in normaler Form vorliegen und kann von Antivirenprogrammen erkannt werden.\n\nDiese Art von Viren ändern ihre Gestalt von Generation zu Generation, teilweise vollkommen. Das geschieht oft in Kombination mit Verschlüsselung – hierbei wird eine variable Verschlüsselung benutzt. Ein Teil des Virus muss jedoch in unverschlüsselter Form vorliegen, um bei der Ausführung den Rest zu entschlüsseln. Um auch diesen Teil variabel zu gestalten, wird die Entschlüsselungsroutine bei jeder Infektion neu erstellt. Die Routine, die die Entschlüsselungsroutine immer neu erstellt, befindet sich dabei selbst im verschlüsselten Teil des Virus und kann zum Beispiel voneinander unabhängige Befehle austauschen und Operationen mit verschiedenen Befehlssequenzen kodieren, so dass verschiedene Varianten entstehen.\n\nIm Gegensatz zu polymorphen Viren, die nur die Gestalt des Codes (durch variable Verschlüsselung oder Permutation) ändern, wird bei Metamorphismus der Virus temporär in eine Metasprache umgeschrieben (daher der Name). Die Metasprache wird unter Anwendung von einem Obfuscator wieder kompiliert. Die formale Grammatik des Virus bleibt immer dieselbe.\n\nDiese Technik ist möglich, da die Assemblersprache für einen Befehl verschiedene Möglichkeiten bietet, diesen auszuführen. Zum Beispiel kann der Befehl \"mov eax, 0x0\" in \"xor eax, eax\" oder \"sub eax, eax\" umgewandelt werden. Da eine Mutation eine Veränderung der Befehlsfolge des Virus ist (und nicht nur eine andere Darstellung der gleichen Befehlsfolge), sind metamorphe Viren schwerer zu erkennen als polymorphe.\n\nBeispiele sind Win32.ZMist, Win32.MetaPHOR oder Win32.SK. Obwohl diese Viren hochkomplex sind und vielen Antiviren-Herstellern Probleme bereitet haben, sind sie vom theoretischen Standpunkt aus gesehen noch trivial.\n\nRetroviren zielen darauf ab, Virenschutzprogramme und Personal Firewalls zu deaktivieren. Da sie sich dadurch nicht nur selbst vor Entdeckung schützen, sondern auch anderen Schadprogrammen Tür und Tor öffnen, gelten sie als sehr gefährlich.\nComputerviren sind vor allem gefürchtet, weil sie den Ruf haben, sämtliche Daten zu zerstören. Das ist aber nur in sehr wenigen Fällen richtig. Die meisten Computerviren versuchen hauptsächlich sich selbst möglichst weit zu verbreiten und deswegen nicht aufzufallen.\n\nEine Eigenschaft, die jedes Virus hat, ist das Stehlen von Rechnerzeit und -speicher. Da ein Virus sich selbst verbreitet, benutzt es die Leistung des Prozessors und der Festplatten. Viren sind aber im Normalfall so geschrieben, dass sie für das System keine spürbare Beeinträchtigung darstellen, so dass sie der Benutzer nicht erkennt. Bei der Größe aktueller Festplatten fällt auch der zusätzlich benötigte Festplattenplatz nicht mehr auf.\n\n\nViele Computerviren enthalten Fehler, welche unter gewissen Umständen zu fatalen Folgen führen können. Diese Fehler sind zwar meistens unbeabsichtigt, können trotzdem Dateien durch eine falsche Infektion zerstören oder gar in Einzelfällen ganze Datenbestände vernichten.\n\n\nManche Viren geben dem Benutzer ihre Existenz bekannt. Beispiele für Meldungen von Viren können zum Beispiel sein:\n\n\nDie meisten dieser Existenzmeldungen sind harmlos und erfolgen oft nur zu bestimmten Uhrzeiten oder nur an bestimmten Tagen, um nicht zu schnell aufzufallen und so eine höhere Verbreitung zu erlangen. Es gibt auch „Viren“, die keine eigentliche Schadroutine enthalten, sondern lediglich derartige Meldungen. Dabei handelt es sich um sogenannte Joke-Programme. Beispiele hierfür sind etwa Eatscreen oder FakeBlueScreen.\n\n\nDurch das Infizieren von Dateien werden die darin enthaltenen Daten manipuliert und möglicherweise zerstört. Da jedoch die meisten Viren vor Entdeckung geschützt werden sollen, ist eine Rekonstruktion der Daten in vielen Fällen möglich.\n\nEinige wenige Viren wurden speziell zur Zerstörung von Daten geschrieben. Das kann vom Löschen von einzelnen Dateien bis hin zum Formatieren ganzer Festplatten führen. Diese Art von Payload wird von den meisten Menschen unmittelbar in Verbindung mit allen Viren gebracht. Da der Speicher der „Lebensraum“ von Viren ist, zerstören sie sich mit diesen Aktionen oft selbst.\n\n\nDirekte Hardwarezerstörung durch Software und somit durch Computerviren ist nur in Einzelfällen möglich. Dazu müsste dem Virenautor bekannt sein, wie eine bestimmte Hardware so extrem oder fehlerhaft angesteuert werden kann, dass es zu einer Zerstörung kommt.\nEinige (z. T. eher theoretische) Beispiele für solche Möglichkeiten sind:\n\n\nDa im heutigen PC-Bereich die Hardwarekomponentenauswahl sehr heterogen ist, gilt bisher die Meinung, dass es sich für Virenautoren nicht lohnt, solche Angriffe durchzuführen.\n\n\nEin als Hardwareschaden missinterpretierter Schaden ist das Überschreiben des BIOS, das heute meist in Flash-Speichern gespeichert ist. Wird dieser Flash-Speicher böswillig überschrieben, kann der Rechner nicht mehr starten. Da der Rechner nicht mehr startet, wird oft fälschlicherweise ein Hardwareschaden angenommen. Der Flash-Speicher muss in diesem Fall ausgebaut und mit einem korrekten BIOS neu bespielt werden. Ist der Flash-Speicher fest eingelötet, ist das Ausbauen wirtschaftlich oft nicht rentabel und die gesamte Hauptplatine muss ausgetauscht werden. Bei Hauptplatinen mit SPI- oder JTAG-Interface für den Flash-Speicher kann ein gelöschtes oder überschriebenes BIOS mittels geeigneter Programmiergeräte erneuert werden.\n\nDer wirtschaftliche Schaden durch Computerviren ist geringer als der Schaden durch Computerwürmer. Grund dafür ist, dass sich Viren nur sehr langsam verbreiten können und dadurch oft nur lokal verbreitet sind.\n\nEin weiterer Grund, warum der wirtschaftliche Schaden bei Computerviren nicht so hoch ist, ist die Tatsache, dass sie den angegriffenen Computer oder die angegriffene Datei im Allgemeinen für einen längeren Zeitraum brauchen, um sich effektiv verbreiten zu können. Computerviren, die Daten sofort zerstören, sind sehr ineffektiv, da sie mit dieser Aktion auch ihren eigenen Lebensraum zerstören.\n\nIm gab es trotzdem einige Viren, die erheblichen Schaden angerichtet haben. Ein Beispiel ist das Virus \"DataCrime\", das gesamte Datenbestände vernichtet hat. Viele Regierungen reagierten auf dieses Virus und verabschiedeten Gesetze, die das Verbreiten von Computerviren zu einer Straftat machen.\n\nAuch unter Windows gab es vereinzelt Fälle von Computerviren, die gravierende finanzielle Schäden für einzelne Unternehmen bedeuteten. So wurde Anfang 1998 das XM/Compat-Virus entdeckt, ein Makro-Virus, das Microsoft-Excel-Dateien mit einer äußerst bösartigen Schadfunktion befällt: Immer, wenn Excel beendet wird, durchforstet der Schädling ein zufälliges Dokument aus der Bearbeitungs-History nach ungeschützten Zellen mit numerischen Werten. In diesen Zellen ändert er die Werte mit einer einprozentigen Wahrscheinlichkeit zufällig in einem Rahmen von +5 bis −5 % ab. Aufgrund der zunächst nur unwesentlichen Veränderungen fallen die so manipulierten Daten möglicherweise erst nach Wochen oder gar Monaten auf. Wird der Schaden entdeckt, lässt er sich nur durch die Einspielung eines Backups wieder beheben – dazu muss natürlich bekannt sein, wann der Erstbefall genau stattgefunden hat. Zwar hat der Schädling keine sonderlich hohe Verbreitung gefunden, aber es gab Fälle von Unternehmen, deren Geschäftsbilanzen und Umsatzberichte durch einen XM/Compat-Befall völlig unbrauchbar geworden sind.\n\nEin Virus mit hohem wirtschaftlichen Schaden war auch \"Win32.CIH\", auch „Tschernobyl-Virus“ genannt (nach dem Atomunfall von Tschernobyl vom 26. April 1986), das sich großflächig verbreitete und am 26. April 2000 den Dateninhalt von mehr als 2000 BIOS-Chips in Südkorea zerstörte. Laut dem Antivirenhersteller Kaspersky sollen im Jahr davor sogar 3000 PCs betroffen gewesen sein.\n\nEin weiterer wirtschaftlicher Faktor war früher vor allem der Image-Schaden der betroffenen Unternehmen, heute ist dieser immaterielle Schaden nicht mehr so hoch, da ein Computervirus schon eher als normale und übliche Gefahr akzeptiert wird.\n\nComputerviren haben viele unterschiedliche Formen, daher ist es nur schwer möglich, zu beschreiben, wie ein Virus grundsätzlich aufgebaut ist. Der einzige nötige Bestandteil, der aus einem Computerprogramm per Definition einen Computervirus macht, ist die Vermehrungsroutine.\n\nDie folgende Erklärung ist keineswegs ein Standard für alle Viren. Manche Viren können mehr Funktionen haben, andere wiederum weniger.\n\n\nDamit ein klassischer reaktiver Virenscanner ein Virus identifizieren kann, benötigt er dessen Signatur. Ein Virus versucht ein System zu infizieren, und dies geschieht zum Beispiel bei einem Linkvirus durch das Anhängen an ein bestehendes Programm. Dabei muss es (abgesehen von überschreibenden Viren) zuerst prüfen, ob es dieses Programm bereits infiziert hat – sprich, es muss in der Lage sein, sich selbst zu erkennen. Würde es dies nicht machen, könnte es ein Programm theoretisch beliebig oft infizieren, was aufgrund der Dateigröße und der CPU-Belastung sehr schnell auffallen würde. Dieses Erkennungsmuster – die Signatur – kann unter gewissen Umständen auch von Virenscannern genutzt werden, um das Virus zu erkennen. Polymorphe Viren sind in der Lage, mit verschiedenen Signaturen zu arbeiten, die sich verändern können, jedoch stets einer Regel gehorchen. Daher ist es den Herstellern von Anti-Viren-Software relativ einfach und schnell möglich, ein neues Virus nach dessen Bekanntwerden zu identifizieren.\n\nViele Viren benutzen anstelle von polymorphen Signaturen sehr kleine Kennzeichnungen wie zum Beispiel ein ungenutztes Byte im Portable-Executable-Format. Ein Virenscanner kann dieses eine Byte nicht als Erkennungsmuster nutzen, da es zu viele falsch positive Treffer geben würde. Für ein Virus ist es jedoch kein Problem, wenn es unter ungünstigen Verhältnissen einige Dateien nicht infiziert.\n\nJohn von Neumann veröffentlichte im Jahr 1949 seine Arbeit \"Theory and Organization of Complicated Automata\". Darin stellt er die These auf, dass ein Computerprogramm sich selbst reproduzieren kann. Das war die erste Erwähnung von computervirenähnlicher Software. Im Jahr 1961 wurde die Theorie von Victor Vyssotsky, Robert Morris Sr. und Doug McIlroy, Forscher der Bell Labs, erfolgreich in ein Computerspiel mit dem Namen Darwin umgesetzt. Zwei oder mehrere Spieler ließen Software-Organismen um die Kontrolle über das System kämpfen. Die Programme versuchten dabei, einander zu überschreiben. Spätere Versionen des Spiels wurden als \"Core Wars\" bekannt. Breite Bekanntheit erfuhr das Konzept Core Wars durch einen Artikel von Alexander K. Dewdney in der Kolumne Computer Recreations der Zeitschrift Scientific American.\n\n1972 veröffentlichte Veith Risak den Artikel \"Selbstreproduzierende Automaten mit minimaler Informationsübertragung\". Darin wird über einen zu Forschungszwecken geschriebenen Virus berichtet. Dieser enthielt alle wesentlichen Komponenten. Er wurde im Maschinencode des Rechners SIEMENS 4004/35 programmiert und lief einwandfrei. Der Science-Fiction-Autor David Gerrold hat 1972 in der Geschichte \"When Harlie Was One\" (in Teilen in \"GOD Machine\" u. a. vorveröffentlicht) über die G.O.D.-Maschine als Erster den Begriff „Computervirus“ erwähnt.\n\n1975 veröffentlichte der britische Autor John Brunner den Roman \"Der Schockwellenreiter\", in dem er die Gefahr von Internetviren vorausahnt. Sein Kollege Thomas J. Ryan schilderte 1979 in \"The Adolescence of P-1\", wie sich eine Künstliche Intelligenz virenähnlich über das nationale Computernetz ausbreitet.\n\nIm Jahr 1980 verfasste Jürgen Kraus an der Universität Dortmund eine Diplomarbeit, in welcher der Vergleich angestellt wurde, dass sich bestimmte Programme ähnlich wie biologische Viren verhalten können.\n\n1982 wurde von dem 15-jährigen amerikanischen Schüler Rich Skrenta ein Computerprogramm geschrieben, das sich selbst über Disketten auf Apple-II-Systemen verbreitete. Das Programm hieß \"Elk Cloner\" und kann als das erste Bootsektorvirus bezeichnet werden.\n\nDie Grenze von Theorie und Praxis bei Computerviren verschwimmt jedoch, und selbst Experten streiten sich, was tatsächlich das erste war.\n\nProfessor Leonard M. Adleman verwendete 1984 im Gespräch mit Fred Cohen zum ersten Mal den Begriff „Computervirus“.\n\nFred Cohen lieferte 1984 seine Doktorarbeit \"Computer Viruses – Theory and Experiments\" ab. Darin wurde ein funktionierendes Virus für das Betriebssystem Unix vorgestellt. Dieses gilt heute als das erste Computervirus.\n\nIm Januar 1986 wurde die erste Vireninfektion auf einem Großrechner an der FU Berlin entdeckt.\n\nZwei Software-Händler aus Pakistan verbreiteten im Jahr 1986 das erste Virus für das Betriebssystem MS-DOS, das Pakistani-, Ashar- oder auch Brain-Virus genannt wird.\nDiese Händler verkauften billige Schwarzkopien von Originalsoftware. Dies war möglich, da dort das Kopieren von Software nicht strafbar war. Jeder Softwarekopie legten sie das Virus bei, das den Zweck haben sollte, die Kunden an den Händler zu binden. Überraschenderweise verbreitete sich dieses Virus sogar bis in die Vereinigten Staaten. Das Programm war relativ harmlos, da es nur das Inhaltsverzeichnis der befallenen Disketten in \"Brain\" umbenannte.\n\nSchließlich wurde 1987 das erste Virus für Macintosh-Rechner entdeckt. Apple lieferte daraufhin sein System gleich komplett mit einem Virensuchprogramm aus. Allerdings konnte es nur diese eine Virenfamilie finden und war für andere Virustypen sozusagen blind. Somit war das Programm also nur bedingt brauchbar.\n\nKurz darauf wurde in Deutschland zum ersten Mal das Cascade-Virus gefunden. Es war das erste Virus, das speicherresident wurde und in Dateien auch verschlüsselt auftrat. Aufgrund dieser Eigenschaften wird es zur zweiten Generation der Viren gerechnet.\n\nZu einem der ersten Viren gehört auch das Jerusalem- oder PLO-Virus. Es wurde auch unter dem Namen Freitag-der-13.-Virus bekannt, da es an einem solchen Tag alle COM- und EXE-Dateien löscht. An allen anderen Tagen verlangsamt es nach etwa 30 Minuten die Rechnergeschwindigkeit.\n\nNicht nur MS-DOS wurde von Viren angegriffen, sondern auch andere Systeme wie Apple Macintosh, Amiga, Atari und Unix.\n\nIm selben Jahr, 1987, erschien im Data-Becker-Verlag das erste Buch zum Thema Computerviren, \"Das große Computervirenbuch\" von Ralf Burger. Da Burger den Quellcode einiger Viren im Buch veröffentlichte, erschienen in den folgenden Monaten Dutzende Varianten der von ihm beschriebenen Viren in der Öffentlichkeit.\n\n1988 erschien der erste Baukasten für Viren (Virus Construction Set). Damit war es auch Anfängern möglich, Viren nach Maß zu erstellen. Das Programm wurde für den Computer Atari ST geschrieben.\n\nIn diesen Jahren wurden die ersten Antivirenprogramme herausgebracht, vor allem, um große Unternehmen zu schützen. Im Jahr 1989 erschien mit \"V2Px\" dann das erste polymorphe Virus, das sich selbst immer wieder neu verschlüsseln konnte und nur sehr schwer zu entdecken war.\n\nIn diesen Jahren wurden Viren zunehmend komplexer konstruiert, um sich besser weiterverbreiten zu können und gegen die Entdeckung durch Antivirenprogramme geschützt zu sein. Am Anfang des Jahres 1991 verbreitet sich der erste polymorphe Virus, der Tequilavirus. Wenig später, 1992, veröffentlichte ein Virenschreiber namens Dark Avenger den ersten polymorphen Programmgenerator, MTE. Damit konnten sich auch einfachste Viren leicht vor einer Erkennung schützen. Einige der damaligen Hersteller von Antiviren-Software konnten dieses Problem nicht lösen und stoppten die Entwicklung ihres Programms.\n\n1992 löste auch das Michelangelo-Virus eine enorme Medienhysterie aus. Mit ihm wurde die Existenz der Viren in der breiten Öffentlichkeit bekannt.\n\nIn diesen Jahren wurden immer wieder neue Techniken in Viren entdeckt, wie zum Beispiel die gleichzeitige Infektion von Dateien und Bootsektor, OBJ-Dateien oder Quellcode-Dateien. 1992 wurde mit \"Win16.Vir_1_4\" das erste Computervirus für das Betriebssystem Microsoft Windows 3.11 registriert. Dieses Proof-of-Concept-Virus wurde nie in „freier Wildbahn“ entdeckt.\n\nViren wie \"ACG\" und \"OneHalf\" markieren das Ende der MS-DOS-Viren. Bis heute zählen sie zu den komplexesten Viren überhaupt. Sie sind stark polymorph und enthalten auch Techniken wie Metamorphismus.\n\nAb 1995, mit dem Erscheinen von Microsoft Windows 95 und dem ständigen Zuwachs an Benutzern, wurden auch Viren für dieses Betriebssystem (und dessen obligate Programme wie Microsoft Office) geschrieben. 1995 erschien das erste Makrovirus für Microsoft Word. Da Dokumente öfter als Programme getauscht wurden, wurden Makroviren ein sehr großes Problem für die Anwender. In den Jahren darauf erschienen die ersten Makroviren für Excel (1997), Powerpoint und Access (beide 1998) und Visio (2000). 1996 wurde das erste Virus Constructor Kit für Makroviren geschrieben, das es auch Personen ohne Programmierkenntnisse ermöglichte, Viren zu erstellen.\n\n1996 erschien dann mit \"Boza\" das erste Virus für Microsoft Windows 95. Damit wurde gezeigt, dass das neueste Microsoft-Betriebssystem für Viren doch nicht, wie vorher behauptet, unantastbar war.\n\nAls der Kampf zwischen Antivirenherstellern und Virenautoren zugunsten der Antivirenhersteller gewonnen schien, wurden 1998 mit \"W32.HPS\" und \"W32.Marburg\" die ersten polymorphen Windows-32-Bit-Viren geschrieben. Kurze Zeit später entstand mit \"Regswap\" das erste metamorphe Virus für diese Betriebssysteme.\n\n1998 und 1999 erschienen die ersten VBS- und JavaScript-Viren und als logische Konsequenz auch die ersten HTML-Viren. Diese Viren arbeiteten mit dem umstrittenen Zusatzprogramm „Windows Scripting Host“. Nun konnten auch Webseiten von Viren infiziert werden.\n\nIn dieser Zeit wurden einige andere, für den Benutzer ungefährliche Viren geschrieben, die dennoch interessant sind. Beispiele sind das \"OS2.AEP\"-Virus, das als erstes ausführbare Dateien des Betriebssystems OS/2 infizieren konnte, oder die ersten Viren für HLP-Dateien, für PHP-Dateien, für Java, für AutoCAD, für Bash, für Palm OS und für Flash.\n\nAm Ende dieser Ära tauchten wieder (wie in der DOS-Ära) die komplexesten Viren auf, die es bis zu dieser Zeit gab. Beispiele sind \"Win32.MetaPHOR\" oder \"Win32.ZMist\", die sehr stark metamorph sind und nicht von Antivirenprogrammen aller Hersteller vollständig entdeckt werden können. Diese Viren wurden von Mitgliedern der Virenschreibergruppe 29A geschrieben, die die Techniken Polymorphismus und Metamorphismus in den vorangegangenen Jahren signifikant weiterentwickelt haben.\n\nUngefähr ab 2002 traten Viren mehr und mehr in den Hintergrund und wurden durch Würmer ersetzt. Die Entwicklung von Viren geht trotzdem weiter und bezieht sich vor allem auf neue Nischen.\n\nIm Jahr 2002 wurde das erste Virus geschrieben, das sowohl Win32-Anwendungen als auch ELF-Dateien (zum Beispiel Linux-Anwendungen) infizieren konnte. Dieses Virus kann als das Einläuten eines neuen Zeitalters der Viren gesehen werden.\n\nIm Jahr 2004 brach dann endgültig eine neue Ära für Viren an. Das erste Virus für PocketPCs (mit dem Betriebssystem Windows CE) tauchte auf und zeigte, dass auch diese viel verwendeten Kommunikationsgeräte nicht verschont werden.\n\nEinige Monate später wurde der Virus \"Win64.Rugrad\" entdeckt. Dieses Virus konnte die Anwendungen des neu erschienenen Microsoft Windows XP 64-bit Edition infizieren und hat eine Vorreiterrolle in der Entwicklung neuer Viren.\n\nWieder einige Monate später, im Jahr 2005, wurde das erste Virus für Handys (mit dem Betriebssystem Symbian OS) geschrieben. Es kann, nachdem vorher schon Würmer für dieses Betriebssystem erschienen sind, auch Dateien infizieren.\n\nMitte 2005, kurz nach der Veröffentlichung der ersten Beta-Version des XP-Nachfolgers Microsoft Windows Vista, wurde das erste Virus für die Microsoft Command Shell (Codename \"Monad\") veröffentlicht. Zunächst wurde propagiert, dass es ein erstes Virus für das neue Windows gäbe. Jedoch ließ Microsoft nach Bekanntwerden der Viren verlautbaren, dass Monad doch nicht wie geplant in Vista enthalten sein werde. Somit wäre dies ein Virus für eine Betaversion mit extrem geringen Chancen auf Verbreitung.\n\nDas erste wirkliche Computervirus für MS Windows Vista trat einige Monate später, im Oktober 2005 auf. \"MSIL.Idoneus\" nutzt .NET Framework 2.0, um sich zu verbreiten.\n\nIn dieser Zeit wurden die ersten Viren für Ruby, MenuetOS, F#, CHM, IDA und Microsoft Office Infopath entdeckt, die aber weder jetzt noch in Zukunft eine Gefahr für Anwender sein werden, da diese Plattformen kaum verbreitet sind und sich die Viren daher kaum vermehren können.\n\n\n\n"}
{"id": "3945446", "url": "https://de.wikipedia.org/wiki?curid=3945446", "title": "Hierarchisches Layout", "text": "Hierarchisches Layout\n\nDie Entwicklung von Algorithmen für hierarchisches Layout ist ein Themengebiet der Informatik und beschäftigt sich mit der Definition von Berechnungsvorschriften zum Layout und Zeichnen hierarchischer Graphen. Die Berechnung des Layouts und das Zeichnen der Graphen kann statisch (das Layout wird in einem Durchlauf berechnet und gezeichnet) oder dynamisch (es wird ein Start-Layout berechnet und in mehreren Durchläufen optimiert und gezeichnet) erfolgen.\n\nIn seiner reinen Form ist der hierarchische Graph ein gerichteter Graph und hat eine Quelle (Knoten ohne eingehende Kanten) sowie mehrere Senken (Knoten ohne ausgehende Kanten). Die zwischen Quelle und Senke liegenden Knoten können abhängig von ihrer Entfernung von der Quelle in Äquivalenzklassen unterteilt werden.\n\nUm die Charakteristik eines hierarchischen Graphen herauszuarbeiten bieten sich zwei Ansätze für das Layout, also die Anordnung der Knoten und Kanten des Graphen zueinander, an.\n\nEin hierarchisches Layout als Eiskristall ermöglicht viele unterschiedliche konkrete Ausprägungen bei i. d. R. geringem Flächenverbrauch gegenüber einem Layout als Baum, kann aber keine Ordnung zwischen Knoten derselben Äquivalenzebene darstellen.\n\nBei diesem Layoutansatz steht die Quelle im Mittelpunkt und alle Knoten einer Äquivalenzklasse haben den gleichen Abstand vom Knoten der übergeordneten Äquivalenzklasse. Die konkrete Ausprägung des Layouts unterscheidet sich einerseits dadurch, ob\nund andererseits dadurch, ob\n\nDer einfachste statische Algorithmus für Eiskristall-Layout berechnet die Anordnung der Knoten auf konzentrischen Kreisen um die Quelle. Er ermittelt das Gewicht jedes Knotens anhand der Anzahl der mit ihm verbundenen Knoten aller untergeordneten Äquivalenzebenen. Dieses Gewicht ist dann Maß für den Winkel, den der Knoten auf dem seiner Äquivalenzebenen zugeordneten konzentrischen Kreis beansprucht. Sowohl die Berechnung des Gewichts der Knoten als auch das Zeichnen von Knoten und Kanten wird durch rekursive Methodenaufrufe realisiert. Für das Zeichnen muss darüber hinaus ein Startwinkel festgelegt werden.\n\nDie folgende Klasse gibt ein Beispiel für die Implementierung einer Node-Klasse in C#, anhand derer man die Funktionsweise gut nachvollziehen kann.\n\nDie folgende Methode gibt ein Beispiel für die Implementierung des vollständigen Zeichnens in C#, wobei die Methoden der Node-Klasse für die Berechnung des Gewichtes und das Zeichnen aufgerufen werden.\n\nEin hierarchisches Layout als Baum ermöglicht die Darstellung einer Ordnung zwischen Knoten derselben Äquivalenzebene (z. B. durch horizontale oder vertikale Anordnung), beansprucht aber i. d. R. mehr Fläche als ein Layout als Eiskristall.\n\nBei diesem Layoutansatz ist eine Richtung (z. B. von Oben nach Unten) vorgegeben, in der sich der Graph ausbreitet. Alle Knoten einer Äquivalenzklasse liegen auf derselben Ebene. Die konkrete Ausprägung des Layouts wird gekennzeichnet durch die Entwicklungsrichtung (horizontal oder vertikal) je Äquivalenzebene.\n\nDer einfachste statische Algorithmus für Baum-Layout berechnet die Anordnung der Knoten auf allen Äquivalenzebenen in derselben, z. B. horizontalen, Entwicklungsrichtung. Er ermittelt das Gewicht jedes Knotens anhand der Anzahl der mit ihm verbundenen Knoten aller untergeordneten Äquivalenzebenen. Dieses Gewicht ist dann Maß für den Anteil, den der Knoten auf der seiner Äquivalenzebenen zugeordneten Entwicklungsrichtung beansprucht. Sowohl die Berechnung des Gewichts der Knoten als auch das Zeichnen von Knoten und Kanten wird durch rekursive Methodenaufrufe realisiert.\n\nDie folgende Methode gibt ein Beispiel einer Implementierung zur Erweiterung der Node-Klasse in C#, anhand derer man die Funktionsweise gut nachvollziehen kann.\n\nDie folgende Methode gibt ein Beispiel für die Implementierung des vollständigen Zeichnens in C#, wobei die Methoden der Node-Klasse für die Berechnung des Gewichtes und das Zeichnen aufgerufen werden.\n\nHierarchische Graphen eignen sich zur Darstellung von Strukturen ohne Zyklen (Rückschleifen und Zusammenführung von Teilpfaden) wie:\n"}
{"id": "3961196", "url": "https://de.wikipedia.org/wiki?curid=3961196", "title": "Tak und die Macht des Juju", "text": "Tak und die Macht des Juju\n\nTak und die Macht des Juju ist eine computeranimierte Fernsehserie von 2007 und 2008. \n\nIm Mittelpunkt der Serie steht der 14-jährige Dschungeljunge Tak. Er lebt zusammen mit seinem Stamm, den Pupununu, und ist Lehrling seines Onkels, des Schamanen des Stamms. Dadurch erhält er die mysteriöse \"Macht des Juju\" und somit Fähigkeiten, die sonst nur 177-jährige Schamanen haben. \n\nTak hat damit die Aufgabe dem Stamm zu helfen und zwischen ihm und der Welt der Juju Geister zu vermitteln. Sehr oft erweist sich Tak für diese Aufgabe als zu unreif und unerfahren. Nur mit der Hilfe seiner Freunde kann er seine täglichen Aufgaben erfüllen.\n\n\n\n\n\n\nDie Jujus sind eine Art Geister, die in Parallelwelten leben, aber auch in der Welt der Pupununu. Je nach Art haben sie verschiedene (magische) Fähigkeiten, Verhaltensweisen und Vorlieben. Mit Hilfe seines Stabes kann Tak sie herbeirufen oder in ihre Welten reisen.\n\nDie Serie wurde 2007 unter der Regie von Mark Risley nach dem Drehbuch von Nicole Dubuc von Nickelodeon Animation Studios produziert. Die Musik komponierten Shawn Patterson und Guy Moon, die Animationen wurden teils von Red Eye Animation Studios hergestellt. \n\nNickelodeon sendete die Serie erstmals vom 31. August 2007 bis zum 29. November 2008 in den USA. Übersetzungen folgten unter anderem ins Spanische, Niederländische und Portugiesische. Die deutsche Erstausstrahlung erfolgte ab dem 8. September 2008 bei Nick Premium, später folgte die Ausstrahlung bei dem Free-TV-Sender Nick.\n\nDie Serie war 2008 in der Sparte Soundeffekte für den Golden Reel Award nominiert.\n\nEs wurden bereits mehrere Jump'n'Run-Spiele aus dem Hause Avalanche Software veröffentlicht, sowohl für den Game Boy Advance, Nintendo GameCube, Nintendo DS, die PlayStation 2 und die Xbox.\n\n"}
{"id": "3966522", "url": "https://de.wikipedia.org/wiki?curid=3966522", "title": "Spotify", "text": "Spotify\n\nSpotify (aus „entdecken“ und „identifizieren“) ist ein Musikstreamingdienst, der seit Oktober 2006 von dem schwedischen Start-up-Unternehmen Spotify AB entwickelt wird. Neben Musik können auch Hörbücher, Podcasts und Videos gestreamt werden. Der Onlinedienst ist mittlerweile in fast 80 verschiedenen Ländern verfügbar, darunter in großen Teilen von Europa und Amerika.\n\nMittels Spotify können über 35 Millionen DRM-geschützte Musiktitel von einer Reihe großer Musiklabels wie Sony, Warner Music Group und Universal sowie zahlreicher kleiner Labels mit Internetverbindung gehört und/oder auf die eigene Bibliothek abgelegt werden. Der Dienst ist auf den meisten modernen Geräten verfügbar, darunter PCs, Smartphones und Tablets. Nutzer können mithilfe der bereitgestellten Apps den gesamten verfügbaren Musikkatalog durchsuchen und Wiedergabelisten erstellen und diese mit anderen Nutzern teilen.\n\nSpotify benutzt ein Freemium-Modell. Einfache und grundlegende Dienstleistungen sind kostenlos und werbefinanziert, erweiterte oder zusätzliche Funktionen sind Teil eines „Premium“-Angebots. Mit 207 Millionen aktiven Nutzern, von denen 96 Millionen zahlende Abonnenten sind, ist Spotify aktuell einer der weltweit größten Musikstreaming-Dienste.\n\nAlle angebotenen Musikstücke werden von Musiklabels zur Verfügung gestellt und von diesen lizenziert. Die Lizenzgebühren werden über zwei Wege finanziert: Entweder bezahlen Kunden ihr Konto mit einem Abonnement, oder sie müssen Werbeeinblendungen akzeptieren.\n\nBei einem Premium-Konto gibt es zusätzlich zur Werbefreiheit die Möglichkeit, Musikabspiellisten im „Offline-Modus“, also ohne Internetverbindung, abzuspielen. Zudem kann die Musik mit einem Premium-Konto mit einer maximalen Bitrate von 320 kbit/s empfangen werden statt lediglich mit einer maximalen Bitrate von 160 kbit/s. Gratis-Nutzer können Spotify 14 Tage lang im Ausland nutzen, bevor sie sich wieder in ihrem Heimatland anmelden müssen. Für Nutzer mit Premium-Abonnement gibt es keine derartige Begrenzung. Mittels Geotargeting der IP-Adresse des Benutzer-Computers wird geprüft, aus welchem Land auf den Dienst zugegriffen wird.\n\nSeit 2017 gibt es Spotify Premium Family. Über dieses Abonnement können bis zu sechs Familienmitglieder die Premium-Vorteile zu einem ermäßigten Preis nutzen. Für dieses Abo ist es erforderlich, dass alle Nutzer die gleiche Anschrift haben.\n\nSpotify wird seit 2006 von der Firma Spotify AB mit Sitz in Stockholm, Schweden entwickelt. Das Unternehmen wurde von Daniel Ek und Martin Lorentzon (ehemaliger CEO von TradeDoubler) in Stockholm, gegründet. Laut Aussage von Axel Bringéus, Vorstand für internationales Wachstum bei Spotify, ist der Musikdienst „als legale Alternative zur Piraterie entstanden“.\n\nSpotify AB ist ein Tochterunternehmen von Spotify Limited mit Sitz in London, welches wiederum ein Tochterunternehmen von Spotify Technology S.A. in Luxemburg ist. Spotify hat Büros in 18 Ländern.\n\nDer Dienst wurde am 7. Oktober 2008 gestartet, nachdem Lizenzverträge mit großen Musiklabels geschlossen wurden.\n\n2011 schaffte Spotify einen Umsatz von rund 188 Millionen Euro und verbuchte einen Verlust von 40 Millionen Euro. Im April dieses Jahres wurde das Geschäftsmodell geändert. Nutzer, die den kostenlosen Dienst länger als sechs Monate in Anspruch genommen hatten, durften nur noch ungefähr zehn Stunden im Monat kostenlos Musik hören. Außerdem durfte ein Lied über den gesamten Nutzungszeitraum höchstens fünfmal abgespielt werden. Damit sollten mehr Nutzer dazu gebracht werden, ein kostenpflichtiges Konto zu nutzen. Es wurden bis zu einer Obergrenze von zehn Stunden wöchentlich 2,5 Stunden gutgeschrieben. Im April 2012 wurden diese Einschränkungen in den USA auf unbestimmte Zeit aufgehoben. Eine Beschränkung besteht für Deutschland seit Januar 2014 nicht mehr.\n\nIm März 2014 übernahm Spotify das Unternehmen \"The Echo Nest\", ein Anbieter der sich auf die genaue Analyse von Musikstücken spezialisiert hatte, um den automatischen Empfehlungsdienst von Spotify zu verbessern.\n\nSpotify war erstmals im Jahr 2018 ein profitables Unternehmen. Am 28. Februar 2018 hat die in Luxemburg ansässige Holding \"Spotify Technology S.A.\" ihren Börsengang an der New York Stock Exchange angemeldet. Er erfolgte am 3. April 2018 mittels Direktplatzierung (Direct Public Offering), ohne Ausgabe neuer Aktien, ohne Sperrfrist, ohne im Vorfeld von Banken organisierten Preisbildungsprozess und damit ohne Kursgarantie, was ein Novum in der Geschichte der NYSE darstellte. Vor dem Start der Notierung hatte die Börse zur Orientierung einen Referenzpreis von 132 US-Dollar pro Aktie festgesetzt.\n\nIm Februar 2019 gab Spotify die Übernahme der Podcast-Dienste Anchor und Gimlet bekannt, um sich stärker in diesem Markt zu positionieren.\n\nAm 2. März 2009 erreichte Spotify eine Million Mitglieder, nachdem das Programm im Oktober 2008 erstmals in Schweden zum Download angeboten worden war. In der Zeit zwischen 2011 und Ende 2013 hat sich sowohl die Zahl der angemeldeten Nutzer auf 30 Millionen als auch die Anzahl der zahlenden Abonnenten auf 8 Millionen verdoppelt.\n\nAm 21. Mai 2014 gab Spotify bekannt, 10 Millionen zahlende Abonnenten und 40 Millionen aktive Benutzer in 56 Ländern zu haben. Zum Jahresbeginn 2015 wurden bereits 60 Millionen Nutzer und 15 Millionen Premium-Abonnenten vermeldet. Am 21. März 2016 verkündete Spotify 30 Millionen zahlende Abonnenten. Im Juni 2016 hatte Spotify nach eigenen Angaben 100 Millionen aktive Nutzer. Mitte September 2016 gab Daniel Ek bekannt, dass Spotify nun 40 Millionen zahlende Kunden besitzt.\n\nAm 2. März 2017 wurden 50 Millionen zahlende Abonnenten gemeldet, nachdem der größte Konkurrent Apple Music im Dezember 2016 20 Millionen zahlende Abonnenten erreichte. Am 1. August 2017 vermeldete Spotify insgesamt über 60 Millionen zahlende Abonnenten. Bis Anfang 2018 wurden mehr als 71 Millionen zahlende Abonnenten und insgesamt 159 Millionen aktive Nutzer registriert. Für das zweite Quartal wurden 83 Millionen zahlende Mitglieder gemeldet.\n\nFinanziert wird das Unternehmen durch Investoren, die bis Anfang 2014 etwa 388 Millionen Euro bereitstellten. Dem gegenüber stand ein geschätzter Marktwert von etwa 2,9 Milliarden Euro. Zu den Unterstützern gehören das US-amerikanische Investmentbanking- und Wertpapierhandelsunternehmen Goldman Sachs, zu 15 Prozent das US-amerikanische Finanzdienstleistungsunternehmen Fidelity Investments, der US-amerikanische Getränkekonzern The Coca-Cola Company, Morgan Stanley, Credit Suisse und die Deutsche Bank.\n\nAm 30. März 2016 wurde bekannt, dass Spotify sich 1 Milliarde US-Dollar von Investoren leiht.\n\nDerzeit lässt sich ein Konto in 78 Ländern erstellen. Darunter sind große Teile Europas, darunter Deutschland, Österreich und die Schweiz, und Amerikas. In Österreich ist Spotify seit dem 15. November 2011 verfügbar, in Belgien und der Schweiz seit dem 16. November 2011. In Deutschland startete Spotify sein Angebot am 13. März 2012, nachdem sich der Start aufgrund der Gebührenverhandlungen verzögert hatte. Am 12. Februar 2013 wurde Spotify auch in Polen, Portugal und Italien freigeschaltet. Seit dem 16. April 2013 ist Spotify auch in Mexiko, Malaysia, Hongkong, Singapur, Estland, Lettland, Litauen und Island verfügbar. Am 24. September 2013 startete Spotify seinen Dienst mit Argentinien erstmals in einem südamerikanischen Staat sowie in Griechenland, Taiwan und der Türkei. Seit dem 12. März 2018 ist Spotify auch in Israel verfügbar.\n\nAm 2. Oktober 2012 startete die Deutsche Telekom in Kooperation mit Spotify den Mobilfunktarif „Special Complete Mobil Music“, mit dem es möglich ist, über Spotify Musik zu übertragen, ohne dass das Spotify-Datenaufkommen dem Inklusivvolumen des Tarifs angerechnet wird. Beobachter sehen das als Verstoß gegen das Prinzip der Netzneutralität. Im Januar 2013 begann Orange Schweiz (heute: Salt Mobile) eine Kooperation mit Spotify, im Juni 2014 der österreichische Mobilfunkanbieter Hutchison Drei Austria. Spotify ist aktuell neben anderen Streaminganbietern Teil des Zero-Rating-Programmes StreamOn der Deutschen Telekom. Auch im Vodafone Music Pass ist ein Streaming unabhängig vom Datenvolumen möglich.\n\nDer Nutzer muss ein Spotify-Konto anlegen, um den Dienst verwenden zu können. Dieses Konto kann dann von beliebig vielen Geräten aus genutzt werden, gleichzeitiges Wiedergeben (englisch „streaming“) auf mehreren Geräten wird aber technisch unterbunden.\n\nJeder Nutzer kann die Titel aller Major-Labels und kleinerer Labels anhören, wobei das Repertoire ständig um neue Labels erweitert wird. Die Musikstücke können über eine Suche nach Interpreten, Titeln oder Alben gefunden werden. Weiterhin können auf der Festplatte gespeicherte Musikdateien in Spotify importiert werden, um Musik abzuspielen, die über Spotify nicht verfügbar ist. Die Nutzer können sich zudem Musikabspiellisten (Playlists) erstellen, diese mit anderen Nutzern austauschen und gemeinsam bearbeiten. Zu diesem Zweck kann der Playlist-Link direkt in ein E-Mail- oder ein Instant-Messaging-Fenster gezogen werden. Klickt der Empfänger auf den Link, lädt sich die Playlist im Spotify-Konto des Empfängers. Diese Playlist-Links können wie normale Links überall, zum Beispiel auf Webseiten, eingesetzt werden. Das gleiche Prinzip gilt für einzelne Musikstücke.\n\nSpotify überträgt Musikdateien über das Internet durch On-Demand-Streaming. Bis Ende 2014 wurde zudem das Peer-to-Peer-Verfahren (P2P) benutzt, bei dem häufig verwendete Musikdaten aus dem Cache der Computer anderer Spotify-Nutzer übertragen wurden. Die Musik wird im Vorbis-Format mit bis zu ~320 KBit/s für Premium Nutzer übertragen. Streaming an DLNA-Geräte ist nicht möglich. Spotify kann die gehörte Musik zu Last.fm „scrobbeln“.\n\nSeit 2016 ist es auch möglich, auf Spotify Podcasts zu hören. Manche Podcasts bietet Spotify exklusiv an, während andere gleichzeitig bei iTunes und anderen Plattformen gelistet sind. Die Folgen können wie Musiksongs bei einem Premium-Zugang offline gespeichert werden. Die einzelnen Sendungen sind wie die Musik in Kategorien eingeteilt, zudem gibt Spotify individuelle Empfehlungen. Im Jahr 2018 ist die Podcast-Nutzung auf Spotify weltweit um 367 Prozent im Vergleich zum Vorjahr angestiegen (in Deutschland um 150 Prozent).\n\nSpotify Connect ist auf einer Vielzahl von netzwerkfähigen Abspielgeräten verfügbar. Dazu gehören zum Beispiel Netzwerkplayer, AV-Receiver, Blu-ray-Player usw. Spotify Connect ist kein selbständiger Client, sondern eine Schnittstelle. Die Nutzung von Spotify Connect erfordert teilweise ein Premium-Konto. Über einen der im Abschnitt Plattformen aufgeführten Clients kann mit dem Spotify Connect fähigen Abspielgerät Kontakt aufgenommen werden. Dieses nimmt dann mit dem Spotify Server Kontakt auf, übernimmt den Musikstream und die Wiedergabe. Die komplette Steuerung (Abspielen, Pause, Vor/Zurück, Lied/Album/Playlistauswahl, …) bleibt beim aufrufenden Client.\n\nEin wesentlicher Vorteil dieser Methode gegenüber anderen in dem Bereich üblichen Kopplungsmethoden (z. B. Bluetooth) ist, dass auf dem aufrufenden Client keine (umgeleitete) Tonausgabe erfolgt. Dieser steht damit wieder für eigenständige Tonausgaben (z. B. im Falle eines Mobiltelefons Anrufe) zur Verfügung.\n\nAls Betriebssystem wird mindestens Windows 7 beziehungsweise macOS ab „Mavericks“ vorausgesetzt. Auch Linux wird nativ unterstützt, der Client kann unter Verwendung zusätzlicher Paketquellen heruntergeladen werden (Snap oder Flatpak). Unter Ubuntu 18.04 kann Spotify ohne Umwege oder Anmeldung aus dem Software Center geladen werden. Spotify ist zudem in Programme wie Clementine, Shazam und Musixmatch eingebunden.\n\nSeit dem 20. Juni 2017 ist Spotify auch als App im Microsoft Store erhältlich.\n\nIm November 2012 hat Spotify damit begonnen, eine webbasierte Variante an einige Nutzer auszuliefern. Seit Anfang 2013 ist der Dienst für alle Nutzer verfügbar.\n\nEs existieren Apps für Android, iOS und Windows-Phone sowie für Amazons Fire Tablets und einige Symbian-, MeeGo- und Blackberry-Smartphones.\n\nSeit dem 11. Dezember 2013 kann Musik auf Android- und iOS-Smartphones kostenlos gehört werden, allerdings nur mit Zufallswiedergabe. Seit 2014 wird auch Windows Phone unterstützt. Außerdem können nur sechs Titel pro Stunde übersprungen werden. Auf Desktop-Computern und Tablets, mit den gleichen Betriebssystemen, gibt es diese Einschränkung nicht.\n\nSpotify unterstützt zudem die Smart-Home-Geräte Amazon Echo und Google Home.\n\nSeit dem 29. September 2015 unterstützt Spotify Google Cast und am 18. Mai 2016 wurde eine App für Android TV veröffentlicht. Es gibt zudem eine Spotify-App für neuere Samsung-Fernseher, dort können unter anderem Wiedergabelisten abgerufen werden.\n\nApps gab es bis Mitte 2017 auch für Smart-TVs von Philips, LG und Amazon Fire TV.\n\nSeit dem 30. März 2015 ersetzt Spotify auf der PlayStation 3 und PlayStation 4 den hauseigenen Musik-Streaming-Dienst \"Music Unlimited\" von Sony. Aufgrund der Einstellung von \"Music Unlimited\" erhielt jeder bisherige Abonnent einen kostenlosen, zweimonatigen Testzugang zu Spotify Premium.\n\nSeit dem 8. August 2017 ist Spotify auch auf der Xbox One verfügbar.\n\nDer schwedische Automobilhersteller Volvo integriert Spotify in einige seiner Fahrzeuge. Das wird mit Hilfe der Ericsson-Technik „Connected Vehicle Cloud“ ermöglicht. Im November 2014 verkündete Spotify eine Kooperation mit BMW und Mini, die es Fahrern dieser Marken erlauben soll, die Spotify-App auf Apple-iOS-Geräten über den Onboard-Computer ihres Fahrzeugs zu bedienen.\nAuch das kalifornische Unternehmen Tesla Motors stattet die Modellreihen S und X standardmäßig mit einem kostenlosen integrierten Spotify-Account aus.\n\nDie Nutzung von Spotify war anfangs ausschließlich in Verbindung mit einem Facebook-Konto möglich, was massive Kritik von Datenschützern zur Folge hatte. Peter Schaar war der Ansicht, der Dienst verstoße gegen das Telemediengesetz, andere Datenschutzbeauftragte rieten von der Nutzung ab. Im Juni 2012 hob Spotify diese Einschränkung in Deutschland auf.\n\nRecherchen des Hessischen Rundfunks vom April 2013 zufolge bekam ein Künstler im besten Fall nur 0,164 Cent pro Abspielvorgang. Zum Vergleich: Verkauft ein Künstler ein Album mit 13 Liedern auf CD, bleiben ihm im besten Fall rund 3 Euro. Das Album müsse also rund 145-mal übertragen werden, damit der Künstler auf einen ähnlichen Ertrag kommt. Das Plattenlabel entscheidet letztlich darüber, ob die Musik auf Spotify angeboten wird. Die Zahlen gehen aus einer Abrechnung hervor, die eine Band dem Hessischen Rundfunk vorgelegt hat. Dagegen wurde eingewandt, dass die Erlöse pro Stream deutlich über denen pro abgespieltem Lied im Radio liegen.\n\nAnfang Dezember 2013 reagierte Spotify mit einer detaillierten Aufstellung über Einnahmen, Ausgaben und Ausschüttungen an die Künstler. Dieser zufolge gehen 70 % der Einnahmen an die Rechteinhaber, pro übertragenem Musikstück würden diese momentan 0,6–0,84 US-Cent erhalten.\n\nKritik an dem Vergütungsmodell wurde auch von verschiedenen Künstlern und Musikern geäußert, die sich deshalb zunächst der Veröffentlichung ihres Werks bei Spotify widersetzten. Der Schlagzeuger der \"The Black Keys\", Patrick Carney, begründete die Entscheidung seiner Band, das Album El Camino nicht auf Spotify bereitzustellen, damit, dass Streaming-Dienste für „eine Band, die von der Musik lebt, finanziell keinen Sinn haben.“ Auch der Sänger Thom Yorke kündigte an, dass die Musik seiner Band Atoms for Peace wegen der schlechten Vergütung nicht auf Spotify verfügbar sein werde. Über Twitter verkündete er: „Macht euch keine Illusionen. Neue Künstler, die ihr auf Spotify entdeckt, werden nicht bezahlt.“ Ende 2014 haben sich Taylor Swift und andere Künstler (z. B. Sven Regener, Farin Urlaub und Herbert Grönemeyer) gegen Spotify ausgesprochen und ihre Musik teilweise nicht bei Spotify zur Verfügung gestellt. Taylor Swift sagte dazu in einem Interview mit Yahoo Music: „Ich möchte mit meinem Lebenswerk nicht zu einem Experiment beitragen, das nach meinem Gefühl Autoren, Produzenten und Künstler nicht fair entschädigt“. Im September 2015 führte sie weiter aus, nachdem sie begonnen hat, ihre Musik exklusiv auf Apple Music zum Streamen anzubieten: „Apple hat mich wie die Stimme der kreativen Community behandelt, die ihnen tatsächlich am Herzen liegt. Es war für mich sehr ironisch, dass ein Multi-Milliarden-Unternehmen auf Kritik mit Demut reagiert, während ein Startup ohne Cashflow darauf wie eine riesige Konzern-Maschine reagiert.“ Seit Juni 2017 ist der komplette Musikkatalog von Taylor Swift jedoch wieder bei Spotify verfügbar. Auch das Werk der anderen genannten Künstler stand im Dezember 2018 bei Spotify größtenteils zur Verfügung.\n\nAb dem 19. August 2015 galten bei Spotify neue Datenschutzrichtlinien für die Verwendung der Nutzerdaten. Darin heißt es, dass nur mit ausdrücklicher Zustimmung des Nutzers die mit Spotify geteilten Fotos und Kontakte sowie der Zugriff auf das Mikrofon und die Standortdaten des Endgerätes, auf dem die Spotify-App installiert ist, von der Spotify AB in Stockholm, Schweden gesammelt und genutzt werden dürfen, um die Dienste zu verbessern und so dem Nutzer beispielsweise eine verbesserte personalisierte Songauswahl zu ermöglichen. Jedoch lasse sich dieser Zugriff auf die Daten jederzeit in den Einstellungen des jeweiligen Endgerätes nachträglich einzeln aktivieren oder sperren. Nach massiven Protesten von Kunden über diverse Internetdienste, wegen der Ausweitung der Datensammlung, entschuldigte sich der Spotify-Chef daraufhin bei seinen Kunden und versuchte, die neuen Regelungen zu erklären. Des Weiteren kündigte Daniel Ek eine überarbeitete Version der neuen Allgemeinen Geschäfts- und Nutzungsbedingungen sowie der Datenschutzbestimmungen für die kommenden Wochen an, die mit ausführlicheren Erklärungen versehen werden sollten und am 9. September 2015 veröffentlicht worden sind.\n\nEnde März 2018 teilte Spotify mit, dass sich das Protokoll für Spotify-fähige Endgeräte wie WLAN-Lautsprecher ändert und deshalb für viele nicht upgradefähige Geräte der Support ersatzlos gestrichen wird.\n\n\n"}
{"id": "3988577", "url": "https://de.wikipedia.org/wiki?curid=3988577", "title": "Tinker Bell", "text": "Tinker Bell\n\nTinker Bell ist ein computeranimierter Direct-to-DVD-Film von Disney aus dem Jahr 2008. Darin wird die Geschichte einer gleichnamigen Fee erzählt, die in der Märchenwelt des Peter Pan lebt und alles daran setzt, das ihr zugewiesene Dasein mit allen erlaubten und unerlaubten Mitteln zu ändern und die Welt der Menschen zu erreichen. Das damit gestiftete Chaos löst sie jedoch durch stetiges Über-sich-Hinauswachsen und ihren Einfallsreichtum gekonnt und sehr humorvoll auf und erreicht schließlich ihr Ziel. Die Figur stammt aus dem Kinderbuch \"Peter Pan\" von J. M. Barrie; in dessen deutschen Übersetzungen wird sie als „Glöckchen“ oder auch „Naseweis“ bezeichnet. Damit wird auch der Name der Fee aufgegriffen, das englische Wort Tinker bedeutet auf Deutsch auch Bastler oder Kesselflicker.\n\nDer Film erzählt von der Kindheit der Fee \"Tinker Bell\" im Feenort Pixie-Hollow. Tinker Bell träumt davon, das Dorf zu verlassen und die Welt zu sehen. Als die Vorbereitungen der Feen für das Frühjahr kommen – sie helfen dabei, die Jahreszeit einzuläuten – will auch Tinker Bell mit von Nimmerland in die weite Welt. Doch dafür muss sie zunächst von ihren Feen-Freundinnen lernen. Schließlich kann auch sie mit ihren Fähigkeiten den Frühling in die Welt bringen.\n\nDer Film wurde 2008 nach einem Drehbuch von Jeffrey M. Howard unter der Regie von Bradley Raymond von den DisneyToon Studios produziert. Der Film wurde zuerst am 28. Oktober 2008 auf DVD und Blu-ray Disc veröffentlicht. Es existieren Versionen auf Englisch, Französisch, Spanisch, Deutsch, Italienisch und anderen Sprachen. Die deutsche Free-TV-Premiere erfolgte am 14. November 2008 bei Super RTL.\n\nDie Musik des Films wurde komponiert von Joel McNeely und gespielt von der Hollywood Studio Symphony. \n\nAm 14. Oktober 2008 wurde eine Soundtrack-CD zum Film veröffentlicht, die neben Liedern aus dem Film auch weitere enthält. Sie umfasst insgesamt zehn Titel. \n\n\n"}
{"id": "3990792", "url": "https://de.wikipedia.org/wiki?curid=3990792", "title": "Z88 (Software)", "text": "Z88 (Software)\n\nZ88 ist ein Softwarepaket für die Finite-Elemente-Methode (FEM) und die Topologieoptimierung in der Struktur- und Kontinuumsmechanik. Die Software wird von einem Team unter der Leitung von Frank Rieg an der Universität Bayreuth seit 1985 entwickelt und von einer Reihe von Universitäten in Lehre und Forschung sowie mehreren kleinen und mittleren Unternehmen in der Produktentwicklung eingesetzt. Z88 kann sowohl zwei- als auch dreidimensionale Elemententypen mit linearem Ansatz berechnen. Zur Software gehören mehrere Solver und zwei Post-Prozessoren. Z88 läuft plattformunabhängig und ist in der SUSE-Linux-Distribution enthalten. 2007 ergaben Benchmark-Vergleiche des Solvers eine Leistung, die der kommerzieller Programme nicht nachstand.\n\nDie Software wurde von Frank Rieg, einem Universitätsprofessor für Konstruktionslehre und CAD an der Universität Bayreuth, entwickelt. Die ursprünglichen Quellen wurden in FORTRAN 77 geschrieben. In den frühen 1990er Jahren wurde das Programm in die Programmiersprache C portiert.\nEs existieren zwei Programme zur Finite-Elemente-Analyse:\n\nZusätzlich gibt es seit 2014 zwei Apps für Android-Endgeräte:\n\nSeit 2016 ergänzt ein Programm für die Topologieoptimierung die Z88 Produktfamilie:\n\nDie aktuelle Version von Z88Aurora beinhaltet folgende Berechnungsmodule:\nUnabhängig vom gewählten Modul kann die Finite-Elemente-Analyse mit Z88Aurora in drei Bereiche gegliedert werden: Präprozessor, Solver (Prozessor) und Postprozessor.\n\nIm Präprozessor wird das FE-Modell aufgebaut. Die zu berechnende Struktur kann direkt in Z88Aurora aus Strukturelementen wie Balken und Stäben erstellt oder in verschiedenen Formaten importiert werden.\nGeometrien können in Form von STEP-Dateien (*.STP), STL-Dateien im ASCII- und Binär-Format (*.STL) oder Autocad-Dateien (*.DXF) eingelesen werden. Für FE-Strukturdaten ist der Import von NASTRAN- (*.NAS), ABAQUS- (*.INP), ANSYS- (*.ANS) oder COSMOS-Dateien (*.COS) möglich. Z88Aurora beinhaltet insgesamt 25 verschiedene Element-Typen, darunter 2D-Elemente (Stab, Balken, Scheibe, Welle, Torus) und 3D-Elemente (Stab, Balken, lineare und quadratische Tetraeder und Hexaeder). Die Vernetzung erfolgt über zwei Freeware Tetraedervernetzer (TetGen von Dr. Hang Si (WIAS Berlin) und NETGEN von Prof. Joachim Schöberl (TU Wien)). Weiterhin dienen ein Tetraederverfeinerer für bestehende Tetraedervernetzungen (linear und quadratisch), ein Mapped-Mesher für Superelementstrukturen (Hexaeder, Schalen usw.), ein Schalenaufdicker, welcher aus 2D Schalen Volumenschalen produziert und eine Trimmfunktion um ebene Schnitte aus 3D-Bauteilen zu erzeugen (Modellreduktion) zur Verfeinerung des Modells. Das Setmanagement ermöglicht eine einfache Selektion von Flächen, Knoten und Elemente, um diese mit Randbedingungen, Materialien etc. zu verknüpfen. die Materialbdatenbank enthält 52 vordefinierte Werkstoffe und ist vom Benutzer editier- und erweiterbar. Verschiedene Randbedingungen wie Kräfte, Verschiebungen, Drucklasten oder thermische Randbedingungen können über das Graphische User Interface aufgegeben werden.\n\nDer Solver berechnet je nach aktivem Berechnungsmodul Verschiebungen, Spannungen, Temperaturen und Knotenkräfte.\nFür die lineare Finite-Elemente-Analyse bietet Z88Aurora vier numerische Gleichungslöser: ein direkter Cholesky-Gleichungslöser mit Jenningsspeicherung für kleine Balken und Stab-Strukturen, zwei unterschiedlich präkonditionierte, iterative Gleichungslöser mit Sparse-Speicherung für große Finite-Elemente-Strukturen und einen mehrprozessorfähigen Sparse-Solver für mittelgroße Finite-Elemente-Strukturen. Für stationär thermische bzw. thermomechanische Berechnungen werden die iterativen Gleichungslöser und der direkte Multicore-Gleichungslöser verwendet. Für nichtlineare Berechnungen steht ein iterativer Solver zur Verfügung. Der Gleichungslöser zur Eigenschwingungsberechnung verwendet das Lanczos-Verfahren.\n\nIm Postprozessor werden die Ergebnisse aus den Solvern visualisiert. Hierbei ist eine Filterung der Ergebnisse und Clipping des Bauteils möglich. Einzelne Ergebnisse können im Text- bzw. CSV-Format exportiert werden und durch die Analysefunktion ist es möglich, Werte einzelner Knoten auszugeben. Zudem kann die verformte Strukturen im STL-Format ausgegeben werden und so in anderen Programmen weiterverarbeitet werden.\n\nDie Software hat eine Windows-Bedienoberfläche mit kontextsensitiver Online-Hilfe. Handbücher zeigen den Umgang mit Z88 und Z88Aurora an Beispielen.\nDie Freeware ist verfügbar für Windows, Linux und OS X.\n\nBei der Topologieoptimierung wird eine vorhandene Struktur im Hinblick auf eine vorgegebene Zielfunktion durch Veränderung der Topologieklasse in einem definierten Bauraum optimiert. So soll durch das Entfernen von Material an geeigneten Stellen eine optimale Struktur erzeugt werden. Ziel der Topologieoptimierung ist die automatische Erzeugung einer optimalen Struktur unter definierten Lasten im virtuellen Produktentwicklungsprozess zu ermöglichen.\nBasis stellt ein Ausgangsentwurf dar. Eine Strukturanalyse liefert Systemantworten wie beispielsweise Verformungen, Spannungen oder Eigenfrequenzen, die vom Optimierungsmodell ausgewertet werden. An dieser Stelle werden das Modell und die Designvariablen zur Optimierung definiert. Es werden nicht nur die Zielfunktion, sondern auch Nebenbedingungen und Restriktionen festgelegt. Das Optimierungsproblem wird über einen Algorithmus, welcher die Eigenschaften der Designvariablen variiert, gelöst. Am Ende steht ein verbesserter Entwurf, welcher die Schleife solange durchläuft bis ein optimaler Entwurf, der sogenannte Designvorschlag, erreicht ist.\n\nBei Z88Arion kann der Benutzer je nach Ziel der Topologieoptimierung zwischen den folgenden Verfahren wählen:\nDas OC-Verfahren erzeugt einen Designvorschlag, der eine minimale Nachgiebigkeit bzw. maximale Steifigkeit in Bezug auf ein vorher festgelegtes relatives Volumen aufweist. Beim SKO-Verfahren findet eine Optimierung auf maximale Festigkeit statt. Der eigens vom Lehrstuhl entwickelte TOSS-Algorithmus stellt eine Kombination beider Methoden dar. Dieses Hybridverfahren aus OC und SKO bezieht sich auf die optimale, steife Struktur des OC-Verfahrens und generiert daraus einen spannungsoptimierten Designvorschlag. Dabei wird Material an überbelasteten Stellen wieder angelagert und an unterbelasteten Stellen entfernt.\n\nIm Postprozessor wird der ermittelte Designvorschlag angezeigt. Der Benutzer kann hier z. B. verschiedene Iterationen zu betrachten und die Darstellungsgrenzen zu variieren. Zudem ist es seit Z88Arion V2 möglich, die entstandene Struktur zu glätten und als STL zu exportieren, um so eine direkte Weiterverwendung des optimierten Bauteils in anderen Programmen zu gewährleisten. Weiterhin existiert eine direkte Schnittstelle zu Z88Aurora.\n\nSeit 1998 dient Z88 im Rahmen der Vorlesung an der Universität Bayreuth der Ausbildung von Ingenieurstudenten. Durch die mögliche manuelle Eingabe der Struktur- und Randbedingungsdaten sowie der Lastsätze veranschaulicht es den Studierenden die Funktionsweise eines Finite-Elemente-Programms. Auf Grund der offenen Dateiquellen kann die Software für Forschungszwecke im FE-Bereich eingesetzt und entsprechend modifiziert werden.\n\nUnter anderem wird Z88 im Rahmen von Lehre und Forschung an der HS Ravensburg-Weingarten, der Universität Ioannina, der Penn State University, der Universidad de Buenos Aires, der Universität Cagliari, der Universität Maribor, und an der Zonguldak Karaelmas Üniversitesi eingesetzt. Im Rahmen von Diplom- und Seminararbeiten wurde Z88 bisher unter anderem an den Hochschulen Darmstadt, Hamburg-Harburg, München, Karlsruhe, Bern und Peking verwendet.\n\nZusätzlich zu Präsenz-Lehrveranstaltungen findet Z88 in zwei Lehrbüchern des Maschinenbaus Einsatz. Das Buch \"Finite Elemente Analyse für Ingenieure: Eine leicht verständliche Einführung\" wurde bisher über 6.000 mal verkauft. Dieses Fachbuch wendet sich an den Einsteiger in die Finite-Elemente-Analyse und benutzt Z88, damit der Leser alle im Buch angeführten Beispiele am eigenen Rechner nachvollziehen kann. Im Lehrbuch \"Maschinenelemente – Funktion, Gestaltung und Berechnung\" von Decker (bisher 19 Auflagen) wird anhand praktischer Anwendungen mit Z88 die Berechnung von Maschinenelementen mit der Finiten-Elemente-Analyse gelehrt.\n\nDurch den Open-Source Ansatz greifen viele Anwendungen auf Z88-Solver, Plotausgaben und ähnliches zurück. Unter anderem wurde Z88 für ein Programm zur Berechnung von punkt- und linienförmigen Lasten auf Glasplatten im Hochbaubereich erweitert. Für die Bestimmung von Elastizitätsmoduln und Biegefestigkeiten von Holz wurden Routinen implementiert und ein Unterprogramm in Z88 zur Berechnung von Druckbehältern entwickelt.\nZ88 wird unter anderem von\nverwendet.\n\nDurch die Verfügbarkeit des Programmcodes und somit der Nachvollziehbarkeit der verwendeten Algorithmen und Materialmodelle diente Z88 wiederholt als Vergleichsberechnungsprogramm für kommerzielle Tools wie NASTRAN und ABAQUS.\n\n\n"}
{"id": "3993550", "url": "https://de.wikipedia.org/wiki?curid=3993550", "title": "Litchie", "text": "Litchie\n\nLitchie ist eine Software,\nmit der Turniere und Ligen verwaltet und ausgewertet werden können. Die Besonderheit der Software ist, dass sie an die Bedürfnisse der jeweiligen Sportart angepasst werden kann.\n\n\"Nils Beckmann\" programmierte den Vorgänger von \"Litchie\" 2002 und veröffentlichte die Software im Jahr 2007. 2008 wurde \"Litchie\" in das Heise-Themenspezial \"Unsere Besten\" aufgenommen, in das auch Programme wie Google Earth, Gnuplot und Eclipse aufgenommen worden sind.\n\n\nDas Programm bietet die Möglichkeit, Spielpläne (Jeder gegen Jeden, Schweizer System, K.O.-System usw.) zu erstellen und Ergebnisse einzugeben. Daraus können dann Tabellen (mit individuell einstellbaren Sortierkriterien und u. a. auch 2- oder 3-Punktesieg) und Statistiken (inkl. Elo-Zahl und Buchholz-Wertung) berechnet werden. Alle Ergebnisse können als Webseiten mit anpassbarem Design exportiert oder gedruckt werden. Begriffe wie \"Treffer\"/\"Tore\"/\"Körbe\"/\"Sätze\" oder \"Spieler\"/\"Mannschaft\"/\"Verein\" können individuell angepasst werden.\n\nLitchie wird u. a. bereits für Ligen und Turniere in den Sportarten American Football, Backgammon, Billard, Basketball, Darts, Eishockey, Fußball, Handball, Hockey, Korbball, Schach, Squash, Tischfußball, Unreal Tournament, Volleyball, Zatre eingesetzt.\n\n\n"}
{"id": "4002516", "url": "https://de.wikipedia.org/wiki?curid=4002516", "title": "IPTraf", "text": "IPTraf\n\nIPTraf ist ein konsolenbasiertes Netzwerk-Statistikprogramm für Linux. Es sammelt unter anderem Informationen über TCP-Verbindungen und zählt Pakete und Bytes, stellt Schnittstellen-Statistiken und Indikatoren für Aktivitäten zur Verfügung und kann zum Erkennen von TCP/UDP-Verkehrsunterbrechungen verwendet werden. Das Programm arbeitet mit dem eingebauten \"Raw Socket Interface\" des Linux-Kernels und unterstützt dadurch eine große Anzahl von Netzwerkkarten und Hardware.\n\nDas Programm kann mit TCP-Flag-Information umgehen, ICMP-Details und OSPF-Pakettypen anzeigen.\nEs liefert allgemeine und detaillierte Schnittstellen-Statistiken von IP, TCP, UDP, ICMP, Non-IP- und anderen IP-Paket-Zählungen. Außerdem gibt es IP-Checksum-Fehler, Schnittstellen-Aktivitäten und Paket-Parameter aus.\n\n\nNon-IP-Pakete werden als \"Non-IP\" gekennzeichnet und in Ethernet-LANs wird die zugehörige MAC-Adresse angegeben.\n\n\n"}
{"id": "4004373", "url": "https://de.wikipedia.org/wiki?curid=4004373", "title": "Madagascar 2", "text": "Madagascar 2\n\nMadagascar 2 ist ein computeranimierter Trickfilm, der am 4. Dezember 2008 in den deutschen Kinos anlief. Die Produktion stammt, wie sein Vorgänger Madagascar, von DreamWorks Animation.\n\nDer kleine Löwe Alekey lebt mit seinem Vater Zuba, dem Anführer des Rudels, in einem Reservat in Afrika. Eines Tages fordert Makunga, der selbst das Rudel anführen möchte, Zuba zu einem Kampf heraus. In der Zwischenzeit folgt Alekey einem als Falle ausgelegten Seil und wird an der Grenze des Reservates von Tierfängern gefangen genommen. Zuba versucht vergeblich, Alekey vom Truck der Tierfänger zu retten. Während des Seetransportes fällt die Kiste mit dem gefangenen Alekey unbemerkt ins Meer, bis er gefunden und gerettet wird. Im Central Park Zoo in New York wächst er unter seinem neuen Namen Alex zu einem stattlichen Löwen heran und wird zur Attraktion des Zoos.\n\nJahre später beschließen die vier New Yorker Zootiere Alex der Löwe, Marty das Zebra, Melman die Giraffe und Gloria das Nilpferd, die im Vorgängerfilm auf Madagaskar gestrandet waren, wieder nach New York in ihren Zoo zurückzukehren. Ihre Gastgeber, die Lemurenkolonie unter dem verrückten King Julien, und die aus dem Vorgängerfilm bekannte schlagkräftige Pinguin-Gang helfen ihnen, ein seit Jahrzehnten dort liegendes Flugzeugwrack wieder flottzumachen. Aus Treibstoffmangel muss die Gruppe nach kurzer Flugzeit in Afrika bruchlanden. Sie landen in einem Tierreservat, dessen einzige Wasserstelle Treffpunkt zahlreicher Arten ist.\n\nDie folgenden Szenen illustrieren die individuellen Anpassungsprobleme der vier Protagonisten an die „alte Heimat“: Alex findet seine Eltern wieder, kann jedoch die Aufnahmeprüfung für das Rudel nicht bestehen, weil er denkt, es wäre ein Tanzwettbewerb (und kein Zweikampf), und muss deshalb aus dem Rudel verbannt werden. Da dies sein Vater nicht übers Herz bringen kann, gibt er die Führung des Rudels an den machthungrigen und eitlen Makunga ab. Marty lernt eine Herde von optisch identischen Artgenossen kennen und glaubt frustriert, seine Einzigartigkeit verloren zu haben. Da es sogar Alex unmöglich ist, ihn von den anderen zu unterscheiden, streiten sie sich und Marty wendet sich enttäuscht von Alex ab. Gloria lernt mit „Moto Moto“ einen attraktiven, aber eher unromantischen Nilpferdbullen kennen. Melman, der Gloria insgeheim verzehrend liebt, zieht sich darum zurück, entwickelt eine neue eingebildete Krankheit und wartet auf den Tod. Die Pinguine – als einzige völlig unbeeindruckt von Afrika – sammeln inzwischen Bodenschätze, stehlen Geländewagen von den Touristen und bauen aus den Fahrzeugteilen ein neues Flugzeug zusammen.\n\nDie Situation am Wasserloch eskaliert, als die von ihren Fahrzeugen getrennten Touristen mithilfe eines aus Baumstämmen selbst errichteten Staudamms die Wasserzufuhr zum Reservat unterbrechen (dabei werden sie von der alten New Yorker Dame angeführt, die sich schon im ersten Film in der Grand Central Station mit dem Löwen Alex furchtlos angelegt hatte). Alex will sich auf den gefahrvollen Weg aus dem Reservat hinaus machen, um die Ursache für das Versiegen des Wassers zu finden. Nachdem er Marty unter Hunderten von Zebras wiedererkannt hat, ist auch die alte Freundschaft zwischen Löwe und Zebra wiederhergestellt und beide machen sich gemeinsam auf den Weg. King Julien empfiehlt inzwischen, einen Freiwilligen als Opfer in den Vulkan zu werfen, um dadurch den Wassergott freundlich zu stimmen und so das Wasser zurückzubekommen. Der liebeskranke Melman, der wegen seiner eingebildeten Krankheit sowieso glaubt, nicht mehr lange leben zu können, meldet sich daraufhin. Gerade noch rechtzeitig hatte er Gloria seine Liebe gestanden, die ihn dann auch in letzter Sekunde vor dem Fall in den Vulkan retten kann. Alex und Marty entdecken den Staudamm der Touristen. Nach zwischenzeitlicher Gefangennahme und anschließender Befreiungsaktion zerstören die Tiere den Staudamm mithilfe der Pinguine und ihres Flugzeuges. Alex und seinem Vater gelingt es auch durch einen Trick, Makunga wieder zu entmachten und ihre Ehre und Stellung im Rudel zurückzuerlangen.\n\nZum Schluss wollen alle vorerst in Afrika bleiben, bis auf Skipper, der seine geliebte Puppe heiratet und zusammen mit den anderen Pinguinen und den Schimpansen mit dem Flugzeug und einer Ladung Gold und Diamanten in die Flitterwochen nach Monte Carlo fliegt.\n\n\nDer Film wurde vom Studio DreamWorks Animation unter der Regie von Eric Darnell und Tom McGrath produziert. Die Musik wurde komponiert von Hans Zimmer und gesungen von Will.i.am. Vertrieben wird der Film in den USA von Paramount Pictures. Die Kinopremiere in den Vereinigten Staaten war am 7. November 2008. Bereits am 30. Oktober kam der Film in die russischen und ukrainischen Kinos.\n\nAm 3. Dezember hatte der Film Premiere in Frankreich und Belgien, seit dem 4. Dezember 2008 war er in Deutschland zu sehen.\n\nDer Film wurde von der Berliner Synchron AG übersetzt und synchronisiert. Für Synchronbuch und Dialogregie war Dr. Michael Nowka zuständig.\nAm 19. Dezember 2008 erhielt der Film die Goldene Leinwand für über drei Millionen Kinozuschauer.\n\nDer Film erhielt überwiegend positive Kritiken und erreichte bei Rotten Tomatoes eine Bewertung von 64 %, basierend auf 150 Kritiken. Bei Metacritic konnte ein Metascore von 61, basierend auf 25 Kritiken, erzielt werden.\n\nEine Fortsetzung mit dem Titel \"\" wurde am 8. Juni 2012 veröffentlicht. Alex der Löwe, Marty das Zebra, Gloria das Nilpferd und Melman, die Giraffe versuchen immer noch nach Hause nach New York zu kommen. Dieses Mal geht ihre Reise zu einem Reisezirkus in Europa, den sie im Madagascar-Stil neuerfinden wollen.\n\n"}
{"id": "4011181", "url": "https://de.wikipedia.org/wiki?curid=4011181", "title": "Tonica fugata", "text": "Tonica fugata\n\ntonica fugata ist ein Kompositionsprogramm für Microsoft Windows, das von dem Unternehmen \"capella-software\" aus Söhrewald (Landkreis Kassel) vertrieben wird. Es vermittelt Grundlagen der musikalischen Kompositionslehre und erzeugt selbständig Kompositionen (Tonsätze, Variationen, Kanons und Fugen) zu eingegebenen Melodien oder Themen. tonica fugata ist für das Selbststudium konzipiert, es ist aber auch für den Kompositionsunterricht an Schulen und Hochschulen geeignet. \n\nDas Programm wurde ursprünglich unter dem Namen tonica als Lern- und Anwendungsprogramm für die Bereiche Akkordlehre, Tonsatzlehre und Komposition entwickelt. Mit Version 4.0 wurde eine Kompositionsautomatik für Tonsätze integriert. Seither wurde es um weitere musikalische Formen und Stile erweitert. Mit der Komposition von Fugen ab Version 9.0 erfolgte die Umbenennung in tonica fugata.\n\nNeben tonica fugata gibt es weiter eine eingeschränkte Version mit der Bezeichnung tonica, mit der unter anderem keine Fugenkomposition möglich ist.\n\ntonica fugata befasst sich mit Aufgabenstellungen, die Bestandteil der Tonsatzlehre an Schulen und Hochschulen sind:\n\nDas Programm löst die oben genannten Aufgaben eigenständig zu den Vorgaben des Benutzers, beispielsweise zu einem eingegebenen Thema. Es bedient sich dabei der Stilimitation: Der Benutzer kann das Ergebnis durch die Wahl eines Harmonisierungsstils bzw. eines Variationsstils beeinflussen. Dabei handelt es sich um einen Personalstil (z. B. Johann Sebastian Bach) oder Gattungsstil (z. B. Jazz). In tonica fugata sind derzeit folgende Harmonisierungsstile vordefiniert:\n\nNeben diesen vordefinierten Stilen kann der Benutzer durch Vorgabe entsprechender Musikbeispiele eigene Harmonisierungsstile erzeugen lassen.\n\nAls Variationsstile bietet das Programm verschiedene Stile nach Art der Partiten von Johann Pachelbel und der Präludien von J. S. Bach an.\n\ntonica fugata setzt Verfahren der Algorithmischen Komposition ein. Als Technologie kommen künstliche neuronale Netze zum Einsatz, deren Struktur und Gewichte an konkrete Lernbeispiele (z. B. Choräle von J. S. Bach) angepasst werden.\n\nAnstatt mit fest vorgegebenen Regeln zu arbeiten und damit den Suchraum zu beschränken, verzichtet tonica fugata bis auf wenige Grundregeln wie beispielsweise dem Verbot der Quintparallelen auf ein festes Regelwerk. Stattdessen löst das Programm die Aufgaben, indem es eine diskrete Zeitreihenanalyse für die Abfolge der Töne, Harmonien und Motive in den Lernbeispielen durchführt und damit die neuronalen Netze trainiert. Aus dem Wissen, das dabei in den neuronalen Netzen gespeichert wird, werden dann Prognosen für neue Melodien und Themen abgeleitet. Die Anwendung dieses Wissens wird gezielt durch den Zufall gesteuert, was dazu führt, dass zu einer Eingabe verschiedene Lösungen erzeugt werden können.\n\ntonica fugata folgt damit der Arbeitsweise eines Komponisten, der sich im Prinzip an Regeln orientiert, aber in weit größerem Maße seiner musikalischen Intuition folgt und dabei aus seinem Erfahrungsschatz und den Werken der Musiktradition schöpft.\n\nDie Fuge ist eine der komplexesten und handwerklich anspruchsvollsten Formen in der Musik, mit der sich viele bedeutende Komponisten auseinandergesetzt haben. Die Kunst besteht darin, aus wenig Grundmaterial (Thema und Kontrapunkt) eine in sich geschlossene Komposition zu erschaffen. Dabei tritt das Thema immer wieder in veränderter Gestalt auf (z. B. als Umkehrung, Augmentation oder in Engführungen).\n\ntonica fugata legt einer Fuge ein Formschema zugrunde, das den groben Aufbau der Fuge festlegt. Basierend auf diesem Formschema und dem vorgegebenen Thema erzeugt das Programm das Themenmaterial, aus dem die Fuge schließlich zusammengesetzt wird. tonica fugata bietet verschiedene Formschemata zur Auswahl an, einige basierend auf den Fugen des Wohltemperierten Klaviers von J. S. Bach.\n\ntonica fugata verfolgt den Ansatz des Explorativen Lernens: Der Benutzer wird nicht schrittweise zum fertigen musikalischen Ergebnis geleitet, sondern soll anhand der Analyse eigener bzw. der vom Programm erzeugten Kompositionen selbständig Erfahrungen sammeln und komponieren lernen. Von Einsteigern und Laien wird daher eine gewisse Sicherheit hinsichtlich der ästhetischen Beurteilung der von tonica fugata erzeugten musikalischen Ergebnisse verlangt. Für erfahrene Benutzer und professionelle Komponisten, deren Kompositionen stilistisch der tonalen Musik oder dem Jazz zuzuordnen sind, eignet sich tonica fugata als Baukasten, aus dem sie Material für ihre eigenen Kompositionen schöpfen können.\n\ntonica fugata verwendet ein proprietäres Dateiformat, exportiert die Daten aber ins MIDI-Format, ins CapXML-Format und ins MusicXML-Format. Eine direkte Schnittstelle zum Notensatzprogramm capella des gleichen Herstellers erlaubt die Weiterbearbeitung der Kompositionen.\nDie Noteneingabe kann wahlweise über die Computertastatur, mit der Maus oder mit einem MIDI-Keyboard erfolgen. Die Software importiert Midi-Dateien und capella-Dateien.\n\n\n"}
{"id": "4012537", "url": "https://de.wikipedia.org/wiki?curid=4012537", "title": "SliTaz", "text": "SliTaz\n\nSliTaz ist eine unabhängig entwickelte, minimale Linux-Distribution mit einer kompletten grafischen Oberfläche und einem eigenen Paketsystem. SliTaz ist eine der kleinsten Linux-Distributionen und gehört zu den kleinsten modernen Betriebssystemen überhaupt mit einer vollständigen grafischen Oberfläche, das Iso-Image hat eine Größe von ca. 40 MB. Der Name ist ein Akronym für \"Simple Light Incredible Temporary Autonomous Zone\".\n\nSliTaz kann als Live-System gebootet werden oder wird auf der Festplatte installiert, beim Bootvorgang wird das gesamte System in das RAM geladen. Wurde von einer CD-ROM gebootet, kann die nach dem Booten entfernt werden, das Laufwerk steht dann wieder zur Verfügung. Da das System im RAM läuft, kann auch auf alten Rechnern flüssig gearbeitet werden, Programme starten auch auf PCs mit langsamen Festplatten rasch. SliTaz bietet eine vollständige und schnelle grafische Oberfläche bei minimalen Hardwareanforderungen.\n\nDie aktuelle Version benötigt mindestens 256 MB RAM. Installiert auf der Festplatte belegt das System ca. 100 MB. Für ältere Rechner, die insbesondere in Bezug auf die Kapazität des RAM den Mindestanforderungen nicht genügen, existiert eine verkleinerte Ausgabe, die Loram-Version. Es gibt die Loram-Version für Systeme ab 24 MB oder 128 MB RAM bei denen die CD-ROM nicht ausgeworfen werden kann.\n\nUm beim Betrieb von einer CD-ROM die volle Funktionalität zu erhalten, lassen sich persönliche Dokumente und Einstellungen auf externen Medien wie zum Beispiel einem USB-Stick oder einer Festplatte speichern.\n\nSliTaz kann über ein eigenes Paketsystem erweitert werden. Über den Paketmanager Tazpkg, der mittlerweile über eine GUI verfügt, lässt sich Software einfach verwalten. Mehr als 2300 Pakete in der Stable- und 2400 Pakete in der Cooking-Version stehen in den Archiven zur Verfügung.\n\nSliTaz wird in zwei Versionen veröffentlicht: Einer stabilen (momentan 4.0), sowie einer \"Cooking\"-Version. Beide sind voll funktionsfähig. SliTaz \"Cooking\" ist ein Snapshot der laufenden Entwicklung von SliTaz. Es ist vollständig benutzbar und erlaubt das Testen der neuesten eingebundenen Veränderungen und Anwendungen.\n\nZudem besteht durch das integrierte Tool Tazlito die Möglichkeit, eine eigene, modifizierte Live-CD zu erstellen, sogenannte \"Flavors\".\n\nSliTaz 1.0 ist das erste stabile Release, das veröffentlicht wurde. Es nutzt JWM (Joe's Window Manager) als Fenstermanager, EmelFM2 und das LXpanel. Der Kernel hat die Version 2.6.24.2.\n\nDer Linux-Kernel hat die Version 2.6.25.5. Es kommen binutils-2.17.50, glibc-2.7 und gcc-4.2.3 als Toolchain zum Einsatz. Weiters gibt es auch eine Unterstützung für das NTFS-Dateisystem.\n\nVerfügt über den Linux-Kernel-Version 2.6.30.6.\n\nVerfügt über den Linux-Kernel-Version 2.6.37\n\nDie aktuelle Cooking-Version wurde am 19. Mai 2014 veröffentlicht. Die Linux-Kernel-Version ist 3.2.53.\n\nMit dieser Version bieten die Entwickler eine Live-CD für Rechner an, die den Mindestanforderungen an den Arbeitsspeicher nicht genügen. Diese Version wurde mit TazLito erstellt.\n\n\n"}
{"id": "4016660", "url": "https://de.wikipedia.org/wiki?curid=4016660", "title": "Citavi", "text": "Citavi\n\nCitavi ist ein Programm zur Literaturverwaltung und Wissensorganisation für Microsoft Windows. Es wird an Universitäten und Hochschulen zum Schreiben von Hausarbeiten und Abschlussarbeiten eingesetzt, von Forschern beim Verfassen von wissenschaftlichen Publikationen, und in Firmen bei der Abfassung von Berichten und zur Wissensorganisation im Team. Citavi wird von der Firma \"Swiss Academic Software\" mit Sitz in Wädenswil bei Zürich entwickelt. Es basiert auf der Software-Plattform .NET.\n\nVorgänger war das an der Heinrich-Heine-Universität Düsseldorf entwickelte Programm \"LiteRat\", dessen Version 1.0 im Jahre 1995 erschien. 2006 wurde die neu entwickelte Version unter dem Namen Citavi 2.0 veröffentlicht. 2010 erschien die graphisch und technisch vollständig überarbeitete Version Citavi 3. Damit war das Programm auch mit englischsprachiger Bedienoberfläche nutzbar. Seit der Version 4, die im Jahr 2013 erschien, kann die Benutzeroberfläche zwischen Deutsch, Englisch, Französisch, Italienisch, Polnisch, Portugiesisch und Spanisch umgestellt werden, außerdem wurde ein Add-On für Word entwickelt. Citavi 5 ist im April 2015 mit Editionen für Einzelnutzer und für Teams erschienen. Mit Citavi 6 wurde im Februar 2018 die Möglichkeit ergänzt, die Daten in einer zentral bereitgestellten Cloud zu speichern und andere Nutzer mit differenzierten Nutzungsrechten zu Projekten einzuladen.\n\nEine eigenständige Entwicklung für OS X wurde 2011 eingestellt, eine betriebssystemunabhängige Webversion ist in Arbeit.\n\nCitavi integriert die Verwaltung von Literaturangaben und Volltexten mit Funktionen zur Wissensorganisation und einer Aufgabenplanung, die auf die Bedürfnisse der Arbeit mit Texten zugeschnitten ist. Die ins Programm integrierte, kontextualisierte Schnellhilfe wird ergänzt durch ein Online-Handbuch, einen einführenden E-Mail-Newsletter, Videos, ein Support-Forum sowie, für Inhaber einer Lizenz, persönliche Hilfe.\n\n\n\n\n\n\n"}
{"id": "4021626", "url": "https://de.wikipedia.org/wiki?curid=4021626", "title": "Ubuntu Privacy Remix", "text": "Ubuntu Privacy Remix\n\nUbuntu Privacy Remix (UPR) ist eine modifizierte Live-CD, die auf der Linux-Distribution Ubuntu basiert. UPR ist nicht für eine dauerhafte Installation auf der Festplatte gedacht. Ubuntu Privacy Remix soll eine abgeschottete Arbeitsumgebung bereitstellen, in der vertrauliche Daten sicher bearbeitet werden können. Das auf der Festplatte des dafür verwendeten Computers installierte System bleibt dabei unangetastet.\nDie Entwicklung und Aktualisierung wurde 2016 eingestellt, somit gilt UPR heute nicht mehr als sicher. Als Nachfolgerversion wird Discreete Linux betrachtet.\n\nDie erste stabile Version von UPR wurde am 4. Dezember 2008 veröffentlicht, sie basierte auf Ubuntu 8.04. Schon vor der ersten stabilen Version von UPR war Truecrypt essentieller Bestandteil des Systems.\nDie Entwicklung und Aktualisierung von TrueCrypt wurde 2014 eingestellt. Als Nachvolgerversion wird VeraCrypt betrachtet. Die letzten stabilen 7er-TrueCrypt-Versionen werden zwar heute noch empfohlen, jedoch finden Sicherheitsforscher immer wieder neue Sicherheitslücken in TrueCrypt.\n\nDie UPR-Entwickler vertreten die Ansicht, dass die Gefahr des Diebstahls vertraulicher Daten heute nicht mehr nur von gewöhnlichen Internet-Kriminellen und ihren Trojanischen Pferden, Rootkits und Keyloggern ausgehe. Vielmehr ergreife in vielen Ländern der Welt auch der Staat Maßnahmen, die Computer der Bürger mit solchen Mitteln zu bespitzeln und zu überwachen. Ubuntu Privacy Remix ist konzipiert als ein Werkzeug, das eigene Daten gegen unbefugte Zugriffe schützen soll.\n\nUbuntu Privacy Remix enthält die beiden bekannten Verschlüsselungsprogramme TrueCrypt und GnuPG. Die Autoren merken jedoch an, dass die Sicherheit von Verschlüsselung nicht isoliert aus dem Verschlüsselungsprogramm heraus abgeleitet werden könne. Betriebssysteme, Anwendungsprogramme, das persönliche Verhalten und natürlich Schadsoftware wie Trojanische Pferde, Rootkits und Keylogger, könnten die Sicherheit eines guten Verschlüsselungsprogramms wieder untergraben oder gar aufheben. Deshalb setzt UPR darauf, eine komplette, unveränderliche und abgeschottete Arbeitsumgebung für die Bearbeitung sowie Ver- und Entschlüsselungen sensibler Dokumente bereitzustellen.\n\nMit Datenträgern, Downloads, E-Mails, manipulierten Websites und harmlos aussehenden manipulierten Dokumenten, die Pufferüberlaufslücken in Programmen ausnutzen und anderen Angriffsmethoden bestehen viele Möglichkeiten, sich mit Schadsoftware zu infizieren, die dann die Vertraulichkeit der eigenen Daten gefährdet. Auch extra dafür durchgeführte Wohnungseinbrüche gehören zu den Planungen des Bundesinnenministeriums. UPR bietet einen Schutz davor, indem sich das System bei jedem Start in einem sauberen unveränderten Zustand befindet.\n\nUPR befindet sich auf einer nur-lesbaren CD, d. h. es kann nicht nachträglich verändert werden. Die Verwendung ausschließlich als Live-CD ist Teil des Konzepts und eine Installation von UPR auf der Festplatte ist bewusst nicht vorgesehen. Spionage- und andere Schadsoftware kann so nicht dauerhaft installiert werden. Es muss natürlich sichergestellt sein, dass die UPR-CD aus einer sicheren Quelle stammt und später nicht ausgetauscht wurde.\n\nTrueCrypt ist ein freies Open-Source-Programm zur Verschlüsselung von Festplatten, Teilen davon oder Wechseldatenträgern und ist in UPR installiert.\nIn UPR steht der volle Funktionsumfang der Linux-Version von TrueCrypt zur Verfügung. Als spezielle Anpassung an die Arbeit auf einem flüchtigen Live-System wurde in UPR die Funktionalität der „erweiterten TrueCrypt-Volumes“ entwickelt.\n\nZur Verschlüsselung einzelner Dateien und insbesondere beim Austausch solcher mit anderen Personen bietet sich dagegen wegen des asymmetrischen Verfahrens eher GnuPG an. GnuPG ist ebenfalls in UPR enthalten.\n\nZum eigentlichen Bearbeiten sind unter anderem enthalten:\n\nDer Unterbindung jeglicher Netzwerkverbindungen messen die UPR-Entwickler für die Sicherheit eine doppelte Bedeutung zu:\n\nViele Schädlinge nutzen Netzwerkverbindungen – vor allem solche ins Internet – um zusätzliche Komponenten nachzuladen. Danach versuchen sie beispielsweise sich der konkret vorgefundenen Konfiguration des Computers anzupassen oder sich selbst zu verändern, um Virenscannern zu entgehen.\n\nUm das Ziel eines abgeschotteten Inselsystems zu verwirklichen, verhindert Ubuntu Privacy Remix die Aktivierung vorhandener Netzwerk-Hardware. Dazu wurden dem angepassten Linux-Kernel die Unterstützungen für LAN-, WLAN, Bluetooth- und Infrarot-Hardware und vor allem die Datenfernübertragungsprotokolle entfernt.\n\nAuch der Unterbindung des Zugriffs auf lokale (und eventuell schon verseuchte) Festplatten messen die UPR-Entwickler für die Sicherheit eine doppelte Bedeutung zu:\n\nBei der nächsten Verwendung des lokal installierten Systems für Internet-Verbindungen könnten sie dann beispielsweise von einem lokal installierten Trojaner abtransportiert werden.\n\nDadurch dass dem Betriebssystem die Möglichkeit genommen wird, die Festplatten überhaupt zu aktivieren, wird auch verhindert, dass unverschlüsselte Swap-Partitionen auf den lokalen Festplatten automatisch gemountet werden, wie es bei einer normalen Ubuntu Live-CD passieren würde. Damit bestünde die Gefahr, dass sensible Informationen auf diesem Weg im Klartext auf die Festplatte ausgelagert würden.\n\nUm das Ziel eines abgeschotteten Inselsystems zu verwirklichen, verhindert Ubuntu Privacy Remix die Aktivierung lokaler Festplatten durch die Veränderung der Behandlung von ATA-Geräten im Quelltext des angepassten Linux-Kernels. Dies führt dazu, dass das System die (eventuell kompromittierten) lokalen S-/ATA-Festplatten vollständig ignoriert, ATA/ATAPI-Geräte wie DVD-Laufwerke aber normal erkennt, damit das System von CD überhaupt laufen kann.\n\nSeit Release 8.04_r2 werden alle Wechseldatenträger standardmäßig mit der mount-Option „noexec“ in das System eingehängt. Das bedeutet, dass Dateien auf diesen Datenträgern gelesen und geschrieben, aber nicht mehr als Code ausgeführt werden können. Dadurch kann Schadsoftware nicht mehr direkt von einem Wechseldatenträger innerhalb des laufenden Systems ausgeführt werden. Dies betrifft Wechseldatenträger mit den Dateisystemen (v)fat, ntfs, ext2/ext3 oder reiserfs.\n\nDas Arbeiten mit einer Live-CD bringt zwar die genannten Sicherheitsvorteile, aber auch Produktivitätsnachteile, weil bestimmte Konfigurations- und Nutzdaten nicht dauerhaft gespeichert werden können. Das bedeutet zum Beispiel:\n\n\"Erweiterte Truecrypt-Container\" sind ein Feature von Ubuntu Privacy Remix, das diese Probleme löst und die Arbeit mit dem System bequemer und effizienter machen soll. Ihre Hauptfunktionen sind:\n„Erweiterte TrueCrypt-Volumes“ bedeuten keinerlei Eingriff in die Funktion oder das Containerformat von TrueCrypt. Es werden lediglich beim Öffnen und Schließen ein paar zusätzliche Befehle ausgeführt, wie zum Beispiel das Setzen von symbolischen Verknüpfungen aus dem (flüchtigen) Home-Verzeichnis in das geöffnete TrueCrypt-Volume.\n\nAb der Version 9.04r1 sind die Rechte des Benutzers, unter dem die Live-CD läuft, anders als in anderen Live-CD-Systemen stark eingeschränkt. Es soll damit weiter erschwert werden, Schadsoftware in das laufende System einzuschleusen oder z. B. Kernelmodule nachzuladen. Das richtet sich nicht in erster Linie gegen absichtliche Maßnahmen des Anwenders, sondern gegen automatisierte Angriffe.\n\nDie UPR-Entwickler weisen auf der Website ausdrücklich darauf hin, dass es eine absolute Sicherheit nicht gibt, und das System vor einer Reihe – in der Regel für den Angreifer allerdings sehr aufwändiger – Angriffe nicht schützen kann.\n\nHierzu zählt zum Beispiel das Einziehen einer Virtualisierungsschicht zwischen Hardware und dem UPR-Betriebssystem. Das wäre durch den Einbau speziell dafür manipulierter Hardware in den Computer zu realisieren. Genauso müssen zu dieser Klasse spezielle Geräte wie Hardware-Keylogger gerechnet werden. Angriffe dieser Art erfordern in der Regel mehrfachen physischen Zugriff auf den Computer. Eine Ausnahme könnten unter bestimmten Bedingungen auf Hardware-Virtualisierung basierte Angriffe bilden (Virtual Machine Based Rootkit).\n\nDie sog. Cold-Boot-Attacke bezeichnet einen Angriff, bei dem ein Computer kalt neugestartet wird (Strom aus und wieder an ohne richtiges Herunterfahren) mit einem minimalen Betriebssystem. Weil dieses Mini-System nur wenig Speicher verbraucht, enthält der Rest des Speichers noch genau das, was vor dem Neustart im Speicher war. Das könnten auch die Schlüssel von TrueCrypt Containern oder GPG-Schlüssel sein. Je nach Computer können solche Reste auch noch nach mehreren Sekunden bis Minuten ohne Strom aufgefunden werden.\n\nAus dieser Methode lässt sich ein spezieller Angriff gegen Systeme, auf denen UPR verwendet wird, ableiten. Die Sicherheit von UPR basiert darauf, dass das lokal auf dem verwendeten PC installierte System, alle Festplatten und Netzwerkhardware komplett ignoriert werden, so dass auch darauf evtl. vorhandene Schadsoftware UPR selbst nichts anhaben kann.\n\nBei dem UPR-spezifischen Angriff muss es einem Trojaner, der den Speicher nach Informationen wie Schlüssel, Passphrasen usw. absucht, gelingen, sich selbst in das lokal installierte System einzunisten.\nWenn nun auf diesem System UPR zur Bearbeitung privater Daten eingesetzt wird – was eigentlich auch auf einem sonst unsicheren PC sicher sein sollte – und sofort danach in das lokale System rebootet wird, könnte dieser Trojaner nach dem Neustart noch Reste wie Schlüssel aus dem UPR-System im Speicher finden. Die Sicherheit von UPR könnte damit untergraben werden. Die Chance auf Erfolg ist etwas geringer als bei „richtigen“ Cold-Boot-Attacken, weil das lokale System vermutlich einen großen Teil des Speichers bereits überschrieben hat, den vorher UPR benutzt hatte.\n\nDieser Angriff setzt voraus, dass der Angreifer zumindest vermutet, dass UPR auf diesem Computer eingesetzt wird. Eine dagegen gerichtete Funktion in UPR 8.04r3 wurde in der aktuellen Version wieder entfernt, weil sie sich als nicht stabil erwiesen hat.\n\nDazu gehören zum Beispiel eine heimlich in der Wohnung installierte Kamera, die auf Tastatur und Bildschirm gerichtet ist und alle Inhalte mitfilmt. Oder ein schlechtes Passwort für die Verschlüsselung, das durch Social Engineering herausgefunden werden kann. Auch hier muss sich der Angreifer in vielen Fällen physischen Zugriff zu den Räumlichkeiten verschaffen können.\n\nAlle elektrischen Geräte, insbesondere Computerbildschirme, senden elektromagnetische Wellen aus. Diese sogenannte kompromittierende Abstrahlung kann mit geeigneten Empfangseinrichtungen auch über größere Entfernungen (bis über 100 Meter) hinweg aufgefangen werden, um den Datenverkehr abzuhören. Insbesondere kann ein Angreifer das Videosignal rekonstruieren und auf einem zweiten Bildschirm darstellen. Für die schnelle Visualisierung ist das Videosignal gut geeignet, jedoch können auch andere Komponenten und Signalleitungen abstrahlen und dadurch die verarbeiteten Informationen ungewollt senden. In einer Studie wiesen vor kurzem Vuagnoux/Pasini von der École polytechnique fédérale de Lausanne sogar die begrenzte „Abhörbarkeit“ von kabelgebundenen Tastaturen nach. Leicht abhörbare Funktastaturen mit primitiven XOR-Verschlüsselungsmechanismen sind für die Bearbeitung sensibler Daten ohnehin ungeeignet.\n\n"}
{"id": "4025364", "url": "https://de.wikipedia.org/wiki?curid=4025364", "title": "REYES (Computergrafik)", "text": "REYES (Computergrafik)\n\nDie REYES-Architektur ist ein Bildsynthese-Algorithmus, der in der 3D-Computergrafik verwendet wird, um fotorealistische Bilder zu berechnen. Er wurde Mitte der 1980er Jahre von der Lucasfilms Computer Graphics Research Group entwickelt, die nun unter dem Namen Pixar bekannt ist.\n\nZum ersten Mal wurde REYES für den Kinofilm \"\" eingesetzt, um Bilder der \"Genesis\"-Effektsequenz zu berechnen. Pixars PhotoRealistic RenderMan (kurz PRMan) ist eine Implementierung des REYES-Algorithmus. Nach der ursprünglichen Publikation, die den Algorithmus beschreibt, ist das REYES-Bildsynthesesystem: \"„Eine Architektur … für das schnelle, hochqualitative Synthetisieren komplexer Bilder.“\" Vorgeschlagen wurde REYES als eine Sammlung von Algorithmen und Datenverarbeitungsmethoden. Die Begriffe Algorithmus und Architektur jedoch werden in diesem Artikel als Synonyme verwendet und sind (in diesem Fall) austauschbar.\n\nREYES ist das Backronym für \"Renders Everything You Ever Saw\" (sinngemäß übersetzt: \"Berechnet alles, was du je gesehen hast\"), der Name ist zudem eine Anspielung auf den Ort Point Reyes, Kalifornien, in dessen Nähe Lucasfilm ansässig ist, und eine Andeutung auf die Verfahren, die mit optischen, bildgebenden Systemen verbunden sind (\"REYES\" wird gesprochen wie das englische Wort \"rays\", was \"Lichtstrahlen\" oder in der Optik, als Singular \"ray\", den Sehstrahl bezeichnet).\n\nDie Architektur wurde unter Berücksichtigung einer Anzahl Bedingungen entworfen:\n\n\nREYES erfüllt effizient mehrere Aufgaben für Effekte, die für die Berechnung in Kinofilmqualität nötig sind: Glatte, gekrümmte Flächen, Oberflächentexturierung, Bewegungsunschärfe und Tiefenunschärfe.\n\nREYES berechnet gekrümmte Oberflächen, wie solche, die durch parametrische Teilflächen (parametrische Patches) beschrieben werden, dadurch, dass es die Teilflächen in Mikropolygone teilt; dies sind kleine Quadrate, von denen jedes kleiner ist als die Größe eines Pixels. Obwohl viele Mikropolygone nötig sind, um die Approximation einer gekrümmten Fläche akkurat durchzuführen, können sie mit einfachen, parallelisierbaren Operationen verarbeitet werden. Ein REYES-Renderer tessliert die Primitiven höherer Ordnung bei Bedarf und teilt jede Primitive nur so fein auf, dass es im finalen Bild akkurat aussieht.\n\nAnschließend ordnet ein Shader jedem Mikropolygon Farbe und Transparenz zu. Die meisten REYES-Renderer erlauben es dem Benutzer, beliebige Beleuchtungs- und Texturierungs-Funktionen selbst hinzuzufügen, geschrieben in einer Shader-Sprache, die speziell für die Bedürfnisse der Verarbeitung von Daten aus der Bildsynthese geschaffen wird. Die Verarbeitung der Mikropolygone geschieht so, dass die Berechnung der Daten vektorisiert werden kann.\n\nGeshadete Mikropolygone werden auf die Bildebene abgebildet, um das Ausgabebild zu erzeugen. REYES benutzt einen innovativen Algorithmus für das Sichtbarkeitsproblem, englisch \"hider\" genannt, der die nötige Integration von Bewegungs- und Tiefenunschärfe durchführt, ohne mehr Geometrie oder Shaderaufrufe zu benötigen als die Berechnung eines scharfen Bildes bräuchte. Der \"hider\" sammelt alle Farben der Mikropolygone in einem Pixel über die Zeit und Linsenposition durch einen Monte-Carlo-Algorithmus, der \"Stochastic Sampling\" genannt wird.\n\nDie grundlegende REYES-Grafikpipeline basiert auf folgenden Schritten:\n\n\nBei diesem Design muss der Renderer den kompletten Bildspeicher im Hauptspeicher bereitstellen, da das endgültige Bild so lange nicht ausgegeben werden kann, bis alle Primitiven verarbeitet wurden. Zur Speicheroptimierung wird vor dem \"Dice\"-Schritt ein weiterer Schritt, genannt \"Bucketing\", eingefügt. Das Ausgabebild wird hierzu in ein Raster aus einzelnen Rechtecken eingeteilt, genannt \"Bucket\" (englisch für Eimer), jedes typischerweise 16 ×16 Pixel groß. Die Objekte werden dann grob entlang der Grenzen der \"Buckets\" geteilt und dann nach ihrem Ort dem jeweiligen \"Bucket\" zugeordnet. Jeder \"Bucket\" durchläuft einzeln für sich den \"Dice\" und die restlichen Schritte der Grafikpipeline, und Daten vom vorhergehenden \"Bucket\", die nicht mehr gebraucht werden, werden verworfen, bevor der Nächste berechnet wird. Auf diese Weise muss nur der Bildspeicher für den in Bearbeitung befindlichen Bucket und Speicher für die Szenenbeschreibung höherer Ordnung für alle geometrischen Primitiven bereitgestellt werden. Dies führt zu einer erheblichen Senkung des Speicherverbrauches für eine durchschnittliche Szene, verglichen mit dem nicht veränderten REYES-Algorithmus.\n\nDie folgenden Renderer verwenden den REYES-Algorithmus auf die eine oder andere Weise, oder erlauben dem Benutzer zumindest, ihn als Vorgabe zu verwenden:\n\n\n"}
{"id": "4030731", "url": "https://de.wikipedia.org/wiki?curid=4030731", "title": "Samsung SGH-i900 Omnia", "text": "Samsung SGH-i900 Omnia\n\nDas Samsung SGH-i900 Omnia (kurz: Samsung Omnia) ist ein Touchscreen-Handy im Barrenformat von Samsung. Es weist Ähnlichkeiten zu dem ebenfalls von Samsung entwickelten Samsung SGH-F480 auf. Unter dem Namen Omnia („Wunsch“ im Arabischen und „alles“ im Lateinischen) steht es aufgrund seiner Eigenschaften und Funktionen in direkter Konkurrenz zum Apple iPhone. Nachfolger ist das Samsung Omnia II.\n\nDas Omnia ist ein Smartphone mit Windows Mobile 6.1 als Betriebssystem. Es zählt zu den Konkurrenten des Apple iPhones.\n\nDas Samsung Omnia besitzt einen 3,2″ großen Touchscreen und eine virtuelle QWERTZ-Tastatur mit Vibrations-Feedback. Das WQVGA-Display kann 65.536 Farben darstellen und ist in der Lage, die Anzeige automatisch auf das Querformat umzustellen, wenn das Gerät quergelegt wird. Außerdem ist auf der Rückseite eine Kamera mit 5,0 Megapixeln angebracht, die Bilder mit einer Auflösung von 2560 × 1920 Pixeln und Videos mit einer Auflösung von bis zu 640 × 480 Pixeln aufnehmen kann. Zusätzlich befindet sich an der Vorderseite des Telefons eine kleine QCIF-Kamera für Videotelefonie. Das Samsung Omnia besitzt etwa 200 MB Hauptspeicher (davon jeweils 100 MB Programm- und Datenspeicher), sowie einen 8 bzw. 16 GB großen (je nach Ausstattungsversion) internen Speicher. Zusätzlich kann der Speicherplatz durch eine microSDHC-Speicherkarte um maximal 16 GB erweitert werden. Zu dem integrierten GPS-Empfänger gibt es je nach Anbieter bei den meisten Paketen eine kostenlose Route66-Navigationssoftware mit Kartenmaterial für Deutschland, Österreich und die Schweiz.\n\nSchnittstellen: Multifunktionsbuchse; WLAN 802.11b/g; Bluetooth 2.1 mit A2DP\nMobilfunk: EDGE und Quadband GSM; UMTS, HSDPA, GPRS.\n\nDas Omnia wird mit Opera Mobile als Web-Browser und Office Mobile ausgeliefert. Es unterstützt Push, DLNA und RSS-Web-Feeds.\n\n\nDas Omnia ist in drei Farben erhältlich:\n\nAbmessungen: 112,0 × 56,9 × 12,5 mm\nGewicht mit Akku: ca. 125 g\n\n\nDas Samsung Omnia unterstützt alle gängigen Übertragungsprotokolle, die ein problemloses Streamen von Audio- und Videodaten ermöglichen.\nVerbindungsmöglichkeiten:\n\nOft werden auf diesen Handys inoffizielle, durch Benutzer modifizierte Softwareversionen verwendet. Diese werden durch sogenanntes Flashen auf das Handy gebracht. Somit werden zum Beispiel auch inoffizielle Versionen von Windows Mobile 6.5 auf dem Omnia genutzt, obwohl sie nicht explizit für das Gerät entwickelt wurden. Ebenso kann der TouchFlo, der speziell für HTC-Handys entwickelt wurde, auch auf dem Samsung Omnia genutzt werden. Durch das Projekt Andromnia gibt es auch die Möglichkeit, Android-Betriebssystem auf dem Omnia zu installieren.\n\n"}
{"id": "4035094", "url": "https://de.wikipedia.org/wiki?curid=4035094", "title": "UIMA", "text": "UIMA\n\nUIMA (Unstructured Information Management Architecture, deutsch \"Architektur zur Verwaltung unstrukturierter Informationen\") ist ein Framework zur Programmierung von Data-Mining-Anwendungen, d. h. zur Wissensextraktion.\n\nDas Projekt UIMA wurde 2005 von IBM gestartet und wird seit Oktober 2006 von Apache betreut.\nZiel des Projekts ist es, ein standardisiertes Framework zum Erstellen von Anwendungen zur Verarbeitung unstrukturierter Informationen, insbesondere natürlicher Sprache (\"Natural Language Processing\", NLP) zu bieten.\nUnstrukturierte Informationen können in beliebigen Formaten vorliegen, z. B. Bild- oder Audio-Daten, jedoch sind Texte die gängigsten Informationen.\n\nDas Konzept von UIMA sieht vor, dass eine Pipeline implementiert wird, in der zunächst Daten eingelesen werden, diese dann verschiedene Analyse- und Verarbeitungsschritte durchlaufen und schließlich an einen oder mehrere sogenannte Verbraucher geliefert werden, die die Ergebnisse verarbeiten, z. B. in einer Datenbank speichern.\nIn jedem einzelnen Analyse-Schritt werden die Daten mit bestimmten Annotationen versehen, d. h. ein definierter Bereich der Datenmenge, also beispielsweise ein Teil des Texts, bekommt eine Anmerkung.\nDurch die starke Modularisierung in Pipelinestufen können die einzelnen Stufen leicht wiederverwendet werden.\n\nEin Beispiel für eine Pipeline ist eine simple Anwendung, die die durchschnittliche Anzahl von Wörtern pro Satz in einem Text berechnen soll.\nHierzu wird zunächst eine Pipelinestufe benötigt, die den Text einliest, z. B. aus einer Datei. Die zweite Stufe durchläuft den Text und markiert alle Wörter, indem alle Positionen von Leerzeichen im Text ermittelt werden. Die dritte Stufe führt analog dazu eine Satz-Erkennung durch, indem Markierungen von Satzzeichen zu Satzzeichen gesetzt werden. Diese beiden Schritte sind unabhängig voneinander und könnten demnach auch vertauscht werden. Die letzte Pipelinestufe muss nun nur noch die Anzahl markierter Wörter durch die Anzahl markierter Sätze teilen und ausgeben.\n\nEine Erweiterung könnte nun sein, die Anzahl der Verben pro Satz zu zählen, hierzu würde nach der dritten Stufe eine Wortart-Erkennung eingebaut, die jedes Wort mit einer Annotation wie „Verb“, „Nomen“ usw. versieht, und der Verbraucher würde statt der Wort-Annotationen die Wortart-Annotationen zählen, die „Verb“ entsprechen; alle anderen Teile der Pipeline können wiederverwendet werden.\nUIMA übernimmt in dieser Anwendung die Verwaltung der Pipeline und die interne Repräsentation der zu verarbeitenden Daten samt Annotationen, außerdem bietet es dem Entwickler alle nötigen Schnittstellen zum Einlesen und Auslesen der Informationen.\n\nUIMA wird insbesondere in der Forschung eingesetzt, entwickelt sich aber auch immer mehr zum Industrie-Standard. Eine der bekanntesten Anwendungen von UIMA ist der Einsatz im IBM Watson.\n\n"}
{"id": "4035865", "url": "https://de.wikipedia.org/wiki?curid=4035865", "title": "Trident (Software)", "text": "Trident (Software)\n\nTrident (auch bekannt als MSHTML) ist die HTML-Rendering-Engine der Windowsversionen des Webbrowsers Internet Explorer von Microsoft.\nDie erste Version von Trident wurde 1997 mit der Veröffentlichung von Internet Explorer 4 eingeführt und bis heute weiterentwickelt – sie ist ein Kernbestandteil aller Nachfolgeversionen von Internet Explorer.\n\nFrühere Versionen des Internet Explorers für Mac OS basierten auf der völlig eigenständigen Tasman-Engine.\n\nIn den Versionen 7, 8 und 9 dieses Browsers nahm Microsoft gravierende Änderungen an der Engine vor, um ihre Kompatibilität zu den etablierten Webstandards zu verbessern. Einen wesentlichen Fortschritt in dieser Richtung stellte der Internet Explorer 7 dar, weil Microsoft damit viele Fehler, teilweise auch schwerwiegende, beheben konnte. Die Nachfolgeversion, der Internet Explorer 8, bestand erstmals den Acid2-Test, der Internet Explorer 9 den Acid3-Test. Außerdem wurden mit dieser Version erneut zahlreiche Neuerungen des Webstandards implementiert. Die aktuelle Version des Browsers (Internet Explorer 11) setzt diese Entwicklung fort.\n\nTrident wurde entwickelt, um Entwicklern das einfache Integrieren in Eigenentwicklungen zu ermöglichen. Die Engine bietet eine COM-Schnittstelle an, um Websites in Umgebungen mit COM-Unterstützung anzuzeigen und zu bearbeiten; beispielsweise stellen die Programmiersprache C++ und die .NET-Laufzeitumgebung eine solche Unterstützung zur Verfügung. Dabei kann ein Browser-Steuerelement einem C++- oder VB-Programm hinzugefügt und verwendet werden, um die Inhalte einer im Browser geladenen Website abzurufen. Ebenso können Ereignisse der Browserbedienung abgefragt werden.\nDie Trident-Funktionalität wird durch Verknüpfen der Datei mshtml.dll mit dem Projekt erreicht.\n\n Die Versionsnummer bezieht sich auf die Version der \"MSHTML.dll\", diese ist jeweils auch mit der des Internet Explorers identisch.\n\nSämtliche Versionen von Internet Explorer ab Version 4 nutzen Trident als Rendering-Engine. In Windows Windows 98, Windows Me, und Windows 2000 greift der Windows-Explorer auf die Engine zurück, in Windows XP wird sie für die Systemsteuerungselemente \"Software\" und \"Benutzerkonten\" verwendet; die Module sind HTML-Anwendungen.\n\nDaneben machen zahlreiche Programmkomponenten und Anwendungen anderer Hersteller davon Gebrauch, darunter:\n\nObwohl jede Version von Internet Explorer Webstandards grundsätzlich unterstützt (in Internet Explorer 6 war ein \"Standards-Compliant Mode\" implementiert), wurden einige Kernstandards unvollständig gerendert. So gab es zum Beispiel keine Unterstützung für das Element <nowiki></nowiki>, das mit HTML 4.01 eingeführt wurde und bei der Darstellung umfließender Innenabstände von Containern, die in CSS 1 festgelegt wurden.\nFerner wurde die CSS-Angabe min-height nicht unterstützt, die eine Mindesthöhe dehnbarer Container zulassen soll.\n\nInternet Explorer 8 unterstützt CSS 2 vollständig.\n\nDie HTML-Rendering-Engine Tasman wurde in Internet Explorer 5 für Mac genutzt. Internet Explorer für Mac hat zuvor Trident verwendet.\nDie Entwicklung von Internet Explorer für Mac wurde 2003 eingestellt, Tasman wurde eingeschränkt weiterentwickelt und später in Microsoft Office 2004 for Mac eingesetzt.\n\nMicrosofts Webdesign-Projekt Expression Web verwendet statt Trident einen eigens dafür entwickelten Renderer, der laut Microsoft die Webstandards am konsequentesten umsetzen soll.\n\nEdgeHTML ist Microsofts neue HTML-Rendering-Engine, die erstmals unter Windows 10 im Webbrowser Microsoft Edge zum Einsatz kam.\n\n"}
{"id": "4036823", "url": "https://de.wikipedia.org/wiki?curid=4036823", "title": "Microsoft Windows", "text": "Microsoft Windows\n\nMicrosoft Windows (englische Aussprache []) bzw. Windows ist ursprünglich eine von Microsoft entwickelte grafische Benutzeroberfläche, aus der später eine Reihe von eigenständigen Betriebssystemen entstanden sind.\n\nDas ursprüngliche \"Windows\" war eine grafische Erweiterung des Betriebssystems MS-DOS, wie beispielsweise auch GEM oder PC/GEOS. Dieses System wurde ab Windows 95 um einen überarbeiteten Kernel, die 32-Bit-API \"Win32\" und Internetfähigkeit erweitert und mit den Versionen Windows 98, 98 SE sowie ME weitergeführt. Für diese Betriebssystemfamilie hat sich die Sammelbezeichnung Windows 9x etabliert. Parallel dazu wurde unter der Leitung von David N. Cutler seit 1988 das auf den Konzepten des Betriebssystems VMS basierende Windows NT entwickelt. Seit \"Windows XP\" vertreibt Microsoft für den Desktop nur noch Nachfolger von Windows NT, da Windows 9x aufgrund technischer Schwächen aufgegeben wurde. Seitdem heißt das Betriebssystem als Ganzes nur noch \"Windows\" (englisch für „Fenster“). Der Name rührt daher, dass die Benutzeroberflächen von Anwendungen auf dem Bildschirm als rechteckige Fenster dargestellt werden.\n\nWindows-Betriebssysteme sind vor allem auf Personal Computern und Servern verbreitet; daneben existieren Varianten für Geräte wie Smartphones oder PDAs sowie für spezielle \"Embedded Devices\" wie etwa vollelektronische Messgeräte und Einzelhandels-Kassensysteme oder für die Anwendung in Kraftfahrzeugen. Kevin Turner, der \"Chief Operating Officer\" von Microsoft, nannte auf der \"Worldwide Partner Conference 2014\" einen Gesamt-Marktanteil von 14 Prozent für alle Windows-Varianten.\n\nDer Begriff \"Window\" (englisch für „Fenster“) als Bezeichnung für ein Software-Oberflächenelement geht zurück auf das in den frühen 1970er Jahren im Xerox PARC entwickelte WIMP-Paradigma (Window, Icon, Menu, Pointing-Device) für den Aufbau von Benutzerschnittstellen. \"Microsoft Windows\" ist ein System, das dieses Paradigma umsetzt.\n\nUrsprünglich entwickelte das US-amerikanische Unternehmen Microsoft keine Betriebssysteme, sondern seit Mitte der 1970er-Jahre BASIC für verschiedene Computerplattformen. Erst mit MS-DOS stieg Microsoft 1981 in das Betriebssystemgeschäft ein und lieferte dafür unter anderem BASICA für PC DOS und GW-BASIC für MS-DOS, weitere Anwendungen folgten.\n\nAuch für Apple steuerte Microsoft bereits einen BASIC-Interpreter bei, sowie die Anwendungsprogramme Word und Multiplan (später Excel), die ebenfalls für den Macintosh entwickelt wurden. Inspiriert vom grafischen Betriebssystem von Apple, der \"Macintosh System Software\", das wiederum vom Computer mit dem ersten grafischen Betriebssystem überhaupt – dem Xerox Alto – inspiriert war, entwickelte Microsoft daraufhin die eigene, auf DOS basierte grafische Oberfläche „Interface Manager“, die kurz vor der Veröffentlichung der Version 1.0 1985 in „Windows“ umbenannt wurde.\n\nSchon im Vorfeld drängte Microsoft Apple dazu, die grafische Benutzeroberfläche der Apple Lisa an andere PC-Hersteller zu lizenzieren. Doch Apple wollte nicht auf die eigenen Hardware-Verkäufe verzichten, da rund 55 % der Einnahmen aus den Verkäufen von Apple-Computern stammten.\n\nAls der damalige CEO von Apple, John Sculley, Windows 1.0 sah, wollte er Microsoft sofort verklagen. Doch Bill Gates setzte Sculley unter Druck, indem er damit drohte, die weit verbreiteten Macintosh-Anwendungen Word und Excel sofort einzustellen, wenn Apple nicht zu Zugeständnissen bereit sei. Obwohl sowohl Apple als auch Microsoft die grafische Benutzeroberfläche, (GUI), von Xerox lizenziert hatten, sah Windows 1.0 in einigen Details dem Macintosh-Betriebssystem nur allzu ähnlich – bis hin zum „Spezial“-Menü, das der Macintosh zuerst hatte. Da Microsoft zur Entwicklung von Anwendungsprogrammen Einblicke in den Quelltext der „System Software“ hatte, war der Verdacht einer absichtlichen Kopie naheliegend. Microsoft hingegen argumentierte, dass mit der Entwicklung des „Interface Manager“ bereits 1983 begonnen wurde – also noch bevor man den Prototyp des Macintosh zur Verfügung hatte. John Sculley, dem es wichtig war, Anwendungen von Drittherstellern auf der Macintosh-Plattform zu etablieren und der daher auf Word und Excel nicht verzichten wollte, ließ sich am 22. November 1985 schließlich auf einen Vertrag ein, der es Microsoft erlaubte, Windows 1.0 trotz aller Ähnlichkeit zur „System Software“ des Macintosh zu veröffentlichen. Als fast drei Jahre später Windows 2.03 vorgestellt wurde, entschloss sich Apple am 17. März 1988 dennoch, Microsoft wegen Copyright-Verletzung zu verklagen. Über fünf Jahre später, am 24. August 1993, wurde die Klage zugunsten von Microsoft abgewiesen. Da war Microsoft mit Windows 3.0 und 3.1 der Durchbruch bereits gelungen.\n\nDas Schaubild stellt einzelne Hauptversionen des Betriebssystems \"Microsoft Windows\" anhand der Erscheinungsdaten und aufgegliedert in die Produktlinien auf einer Zeittafel angeordnet dar, die einen besseren Überblick ermöglicht.\n\nMicrosoft Windows wurde als grafische Benutzeroberfläche für DOS-Rechner vorgestellt. DOS wurde für Systemzugriffe verwendet. Das änderte sich mit Windows 3.x allmählich, insbesondere durch die Einführung der \"32-Bit-Zugriffe\". Zwischen Windows-Programmen war nur kooperatives Multitasking verfügbar. Mit \"Windows for Workgroups\" wurde eine Version mit integrierter Netzwerkfähigkeit veröffentlicht. Der Namenszusatz wurde ab Windows 95 und in allen Windows-NT-Versionen wieder fallen gelassen.\n\nAm 10. November 1983 wurde von Microsoft auf der COMDEX/Fall '83 ein Prototyp mit der Bezeichnung „Interface Manager“ vorgestellt, welcher die erste grafische Benutzeroberfläche von Microsoft für DOS darstellte. Microsofts Marketing-Abteilung entschied jedoch später, nachdem die Entwickler immer von Fenstern sprachen, das System in „Windows“ umzubenennen. \"Microsoft Windows 1.0x\" wurde am 20. November 1985 veröffentlicht. Windows 1.0x wurde für 99 US-$ verkauft, war aber kein großer Erfolg, da es so gut wie keine Anwendungen dafür gab. Als Oberfläche dient ein Dateimanager. Die einzelnen Programme müssen anhand ihres Dateinamens ausgewählt werden.\n\"Windows 2.0\" wurde im November 1987 veröffentlicht und beinhaltete unter anderem Verbesserungen an der grafischen Benutzeroberfläche. Erstmals wurde auch eine Windows-Version von Microsoft Word und Microsoft Excel veröffentlicht.\n\n\"Windows 3.0\" wurde am 22. Mai 1990 veröffentlicht. \"Windows 3.1\", welches am 1. März 1992 folgte, war die erste kommerziell erfolgreiche Version eines Windows-Systems. Windows 3.1x war sehr beliebt, alleine in den ersten 2 Monaten wurden etwa 3 Millionen Lizenzen verkauft. Mit \"Windows for Workgroups 3.1\", Codename „Sparta“, erschien im Oktober 1992 das erste netzwerkfähige Windows. Durch die Nachinstallation des \"TCP/IP Netzwerkprotokoll-Stack\" unterstützte \"Windows for Workgroups 3.11\" das Internetprotokoll. Windows 3.2 ist ein Update der Windows-3.1-Version für China, laut Microsoft Knowledge Base wurden neue bzw. verbesserte Eingabeeditoren für die Schrift eingesetzt. Die \"Win32s-Schnittstelle für Windows 3.1x\", die nachinstalliert werden musste, stellte einen Teilsatz der Win32-API aus Windows NT bereit, wodurch einige für Windows NT entwickelte Programme funktionierten.\n\nDie \"Windows-9x\"-Linie basiert auf MS-DOS, besitzt aber einen eigenen 32-Bit-Kernel, der nach dem Systemstart die Systemzugriffe mittels sogenannter VxDs (\"Virtual Device Driver\", virtuelle Gerätetreiber) steuert. Das Win32-API war gegenüber Windows NT 3.1 und Windows NT 3.5 nicht völlig neu, Windows 95 war aber die erste Windows-Version für Heimanwender, die diese benutzte.\n\nWindows 9x erhielt ein angepasstes DOS, während frühere DOS-basierte Windows-Versionen ein vorinstalliertes DOS voraussetzten. Der Start von Windows war (außer in Windows Me) durch einen Eintrag in der Datei MSDOS.SYS verhinderbar, so dass Windows wie in früheren Versionen manuell durch das DOS-Programm WIN.COM startete. Microsoft wollte suggerieren, MS-DOS existiere nicht eigenständig, jedoch war das MS-DOS-Betriebssystem aus jedem Windows 9x ohne Windows lauffähig. Windows 9x unterstützt für 32-Bit-Anwendungen präemptives Multitasking und für Kompatibilität mit Windows-3.x-16-Bit-Anwendungen kooperatives Multitasking. 32-Bit-Prozesse besitzen jeweils eigene virtuelle Adressräume (Speicherschutz), konsequenter Speicherschutz ist jedoch für die Kompatibilität zu Anwendungen, die Hardware direkt ansprechen, nicht gewährleistet.\n\nDer Begriff „Windows 9x“ leitet sich aus den nun spezielleren Namen (anstatt nur Versionsnummern) her, die in den meisten DOS-basierten Windows-Versionen 4.x mit einer Neun beginnen. Von diesem Schema weicht Windows Me ab, obwohl es mit den verwandten anderen Versionen zusammengefasst wird.\n\n\"Microsoft Windows 95\" galt als revolutionär, als es 1995 erschien. In dem 16-/32-Bit-System wurde DOS zum Unterbau degradiert. Auch verwaltete es lange Dateinamen. Innerhalb von vier Tagen wurden die ersten Millionen Exemplare verkauft. In den beiden Folgejahren erschienen Windows 95b und Windows 95c, welche USB und FAT32 zur Adressierung von Festplatten von über 2 Gigabyte unterstützen.\n\n\"Microsoft Windows 98\" erschien 1998 als Evolutionsstufe von \"Windows 95\" mit dem Internet Explorer 4.0 und Multimedia-Erweiterungen. Nach Windows 95 ist Windows 98 das zweiterfolgreichste System, die offizielle Unterstützung endete im Jahr 2006. Am 5. Mai 1999 erschien \"Windows 98 Second Edition\" (englisch für \"zweite Ausgabe\") welche in überarbeiteter Version den Internet Explorer 5 sowie Verbesserungen in den Bereichen Multimedia und USB enthielt.\n\n\"Microsoft Windows Millennium Edition\" erschien 2000 mit Multimedia-Verbesserungen und einer Systemwiederherstellung, um das System auf einen automatischen oder vom Benutzer ausgewählten Zeitpunkt zurückzusetzen. Ferner wurde der Schutz von Systemdateien verbessert. Die Funktion \"System File Protection\" (SFP) unterbindet deren Löschung. Durch das Programm „Komprimierte Ordner“ wird das Erstellen und Entpacken von ZIP-Archiven direkt unterstützt. Zudem enthält Windows Me einige von Windows 2000 übernommene Funktionen.\n\nDie Windows-NT-Serie besitzt einen neuen Kernel. Das Kürzel \"NT\" stand ursprünglich für \"N-Ten\" (N10), einen Emulator, auf welchem das System von den NT-Entwicklern in der Anfangsphase betrieben wurde.\nMicrosoft zufolge ist die Vermutung, NT stehe für New Technology, falsch. Der Name stand zunächst für das von Microsoft und IBM gemeinsam entwickelte Betriebssystem OS/2, welches nur in den 16-bit-Versionen (Versionsnummern 1.x) gemeinsam entwickelt, aber getrennt vermarktet wurde. Mit dem Entwicklungsschritt zu den 32-bit-Versionen (2.x) trennte sich Microsoft von IBM, um das Betriebssystem als \"Windows NT\" in Konkurrenz zu OS/2 von IBM weiterzuentwickeln.\n\nWindows NT ist für verschiedene Prozessorarchitekturen konzipiert. Die erste Version wurde für Intel-386-, MIPS- und Alpha-Prozessoren angeboten. Mit NT 3.51 kam PowerPC hinzu. Während die frühere Entwicklung von Versionen von NT für die Alpha-Architektur von DEC und weiteren nach und nach eingestellt worden ist, so dass Windows 2000 nur noch x86 unterstützte, wurde mit Windows XP die Unterstützung der AMD64- und der IA-64-Architektur eingeführt. Mit Windows RT wurde die ARM-Architektur unterstützt. Heute unterstützt Microsoft x86, AMD64 und ARM.\n\n\"MS-DOS\" wird für Intel-386, MIPS, PowerPC und Alpha emuliert, weitgehend namens- und funktionsgleiche Befehle sind als Laufzeitumgebung verfügbar, DOS-Programme können weiter verwendet werden, solange sie keinen direkten Zugriff auf die Hardware erfordern. Spiele laufen daher meist gar nicht oder nur ohne Ton- und Joystick-Unterstützung. 16-Bit-Windows-Programme wurden für MIPS, PowerPC und Alpha ebenfalls emuliert. Auf AMD64, IA-64 und ARM fehlt der MS-DOS-Emulator und die Unterstützung für 16-Bit-Windows-Programme.\n\nNT besitzt präemptives Multitasking mit Speicherschutz. Direkte Hardwarezugriffe von Programmen sind (im Gegensatz zur DOS-Linie) durch die strikte Durchsetzung eines Schichtenmodells nicht erlaubt. Beginnend mit Microsoft Windows 2000 wurden die NT-Linie und die Konsumentenvariante von Microsoft Windows vereinheitlicht und in eine gemeinsame Produktlinie überführt. Mit der XP-Version kann die Zusammenführung „unsicherer Multimedia-Versionen, DOS-basiert“ und „sicherer NT-Versionen ohne Multimedia“ als abgeschlossen betrachtet werden.\n\n\"Microsoft Windows NT 3.1\" war die erste Windows-NT-Ausgabe. Es erschien am 26. Juli 1993 in einer Workstation- und einer Servervariante. Windows NT 3.1 unterstützte x86-Prozessoren ab Intel 80386, MIPS-Prozessoren der Serien R4000 und R4400, später auch Alpha-AXP-Prozessoren. Die Oberfläche entspricht der von Windows 3.1.\n\n\"Microsoft Windows NT 3.5\", Codename „Daytona“, ist im September 1994 erschienen und stellt eine Weiterentwicklung von Windows NT 3.1 dar. Die überarbeitete Version \"Microsoft Windows NT 3.51\" kam im Juni 1995. Windows NT 3.51 unterstützte erstmals PowerPC-Prozessoren, was die hauptsächliche Neuerung war. Dazu wurde vor allem die Win32-API erweitert. Die Workstation- und die Server-Variante wurden klarer voneinander abgegrenzt, als das noch bei NT 3.1 der Fall war.\n\"Microsoft Windows NT 4.0\", Codename „Cairo“, ist am 29. August 1996 erschienen. Anders als Windows NT 3.x hat Windows NT 4.0 fast die gleiche Benutzeroberfläche wie Windows 95. Zunächst wurde Windows NT 4.0, wie auch Windows NT 3.x, in zwei Varianten veröffentlicht, in den Jahren 1996 und 1998 wurden schließlich zwei weitere Server-Varianten des Betriebssystems veröffentlicht.\n\n\"Microsoft Windows 2000\" ist am 17. Februar 2000 erschienen und trägt die NT-Version 5.0. Es bietet eine verbesserte Unterstützung von Geräten und existiert in insgesamt vier unterschiedlichen Varianten. Die Server-Varianten wurden speziell auf unterschiedliche Unternehmensgrößen hin konzipiert und bieten erstmals Active Directory. Windows 2000 unterstützte im Gegensatz zum Vorgänger nur noch x86-Prozessoren.\n\"Microsoft Windows XP\", Codename „Whistler“, ist am 25. Oktober 2001 erschienen und trägt die NT-Version 5.1. Es war das bis dato am besten verkaufte Windows-Betriebssystem und vereint erstmals die großen Produktschienen „Privat“ und „Unternehmen“. Vorangetrieben wurden insbesondere die Erneuerung der Benutzerführung und die Integration von Multimedia und Internet. Es führte die Produktaktivierung ein, die das Verwenden von Schwarzkopien verhindern sollte. Windows XP gibt es in verschiedenen Ausgaben. Die x86-Versionen wurden bei Einführung in drei Editionen vertrieben, der \"Home Edition\" und der \"Professional Edition\" für die x86-Architektur sowie die \"Windows XP 64-Bit Edition\" für die IA-64-Architektur. Später wurden weitere Editionen eingeführt. Microsoft Windows XP Embedded erschien am 28. November 2001. Am 28. März 2003 folgte, basierend auf Microsoft Windows Server 2003 x64, eine weitere 64-Bit-Edition, dieses Mal für Itanium-2-Prozessoren. Am 25. April 2005 erschien auf gleicher Basis die \"Windows XP Professional x64 Edition\" für AMD64-Prozessoren. Die \"Media Center Edition\" und die \"Tablet PC Edition\" erschienen für x86-Prozessoren. 2008 erschien das auf XP aufbauende \"Windows Embedded Standard 2009\".\n\n\"Microsoft Windows Server 2003\" ist im Jahr 2003 als Server-Variante von Windows XP erschienen und trägt die NT-Version 5.2. Die \"x64-Varianten\" sind speziell auf die 64-Bit-Architektur zugeschnittene Varianten des Betriebssystems. Des Weiteren erschienen \"Enterprise\" und \"Datacenter\" auch für Itanium-Prozessoren. Von Windows Server 2003 sind verschiedene Varianten erschienen.\n\"Microsoft Windows Vista\", Codename „Longhorn“, ist am 30. November 2006 für Unternehmen und am 30. Januar 2007 für Privatanwender erschienen. \"Microsoft Windows Server 2008\" erschien am 27. Februar 2008. Vista und Server 2008 tragen die NT-Version 6.0. Nach einer ersten Alphaversion Anfang 2002 wurde der Veröffentlichungstermin mehrfach verschoben, dennoch konnten nicht alle ursprünglich geplanten Funktionen verwirklicht werden (beispielsweise die Dateisystemerweiterung WinFS). Die grafische Benutzerschnittstelle wurde überarbeitet und bietet die Modi Aero Basic und Aero Glass. Alle Produktvarianten sind sowohl als Versionen für die 32-Bit-Architektur als auch als Versionen für die 64-Bit-Architektur (Windows Vista für AMD64, Windows Server 2008 auch für IA-64) erschienen. Windows Vista enthält neben den von Windows XP bekannten \"N-Versionen\" ohne vorinstallierten Windows Media Player für die Europäische Union auch \"K-Versionen\", die ohne vorinstallierten Windows Media Player und Windows Messenger ausgeliefert werden. Die \"K-Versionen\" sind für den südkoreanischen Markt bestimmt. Die \"Starter-Variante\" von Windows Vista wurde in Europa nicht veröffentlicht. Insgesamt sind von Vista zehn Varianten erschienen. Von Windows Server 2008 erschienen mehrere Editionen. Windows Server 2008 ist die letzte Server-Version, die x86-Prozessoren unterstützt.\n\"Microsoft Windows 7\" wurde im Jahr 2000 unter dem Codenamen \"Blackcomb\" angekündigt und sollte \"Windows XP\" ablösen. 2001 entschied sich Microsoft jedoch, zwischen \"Windows XP\" und \"Blackcomb\" eine weitere Version von Windows zu veröffentlichen, die später \"Windows Vista\" genannt wurde. 2006 änderte man außerdem den Codenamen von \"Blackcomb\" zu \"Vienna\", später entschied man sich dann für den Namen Windows 7.\nWindows 7 und die auf dem gleichen Kernel aufbauende Server-Variante, \"Microsoft Windows Server 2008 R2\", wurden seit dem 22. Oktober 2009 verkauft. Wichtige Veränderungen in Windows Server 2008 R2 zu Windows Server 2008, der Servervariante von Windows Vista, sind z. B. die Unterstützung von 256 logischen CPU-Kernen, der Verzicht auf die Version für 32-bittige x86-Prozessoren und die Einführung von DirectAccess; Windows Server 2008 R2 ist somit nur noch für x86-Systeme mit 64-Bit-Unterstützung und die IA-64-Architektur verfügbar. Das erste Service Pack wurde am 22. Februar 2011 freigegeben.\n\"Microsoft Windows Server 2012\", das seit dem 4. September 2012 verfügbar ist, ist der Nachfolger von \"Windows Server 2008 R2\". Am 26. Oktober folgte die Veröffentlichung des auf dem gleichen Kernel basierende \"Microsoft Windows 8\". Im Jahr 2007 wurden für die Entwicklung die Codenamen „Mystic“ und „Orient“ verwendet. Im Januar 2011 wurde auf der Consumer Electronics Show in Las Vegas angekündigt, dass Windows 8 nicht nur die herkömmliche x86-, sondern auch die ARM-Architektur der drei größten Produzenten Nvidia, Qualcomm und Texas Instruments unterstützen werde. Diese Version wird unter dem Namen \"Microsoft Windows RT\" vermarktet. Windows Server 2012 dagegen unterstützt nur noch AMD64/Intel 64, die Itanium-Unterstützung entfiel. \"Microsoft Windows Phone 8\" basiert ebenfalls auf dem Windows-NT-6.2-Kernel, während frühere Windows-Phone-Versionen auf dem CE-Kernel basieren. Microsoft verkaufte Windows 8 in den ersten drei Monaten zu einem stark reduzierten Preis.\n\"Microsoft Windows 10\" wurde am 30. September 2014 angekündigt. Eine \"Technical Preview\" folgte einen Tag später. Während früher für jede Plattform ein eigenes Windows entwickelt wurde (Windows 8 für Desktop und Tablets, Windows RT für ARM-Tablets, Windows Phone 8 für Smartphones), deckt Windows 10 alle Plattformen ab, die grafische Benutzeroberfläche passt sich der Displaygröße sowie dem Gerätezweck an. Das Startmenü kommt in einer überarbeiteten Form zurück, bei mobilen Geräten ist es bildschirmfüllend. Es ersetzt somit wieder den aus Windows 8 und Windows Server 2012 bekannten Startbildschirm. Der Marktstart für Windows 10 war am 29. Juli 2015.\n\nDie \"CE-Linie\" wurde für mobile Geräte wie Pocket PCs, Smartphones und PDAs konzipiert und stellt eine eigene Produktlinie dar. Neben den hier aufgelisteten Versionen existieren weitere Versionen für einzelne Gerätetypen und Hauptprozessoren.\n\n\n\n\n\nDer Nachfolger Windows Phone 8 zählt nicht mehr zur CE-Linie, sondern zur NT-Linie durch die Verwendung des Windows-NT-Kernels.\n\n\nDas Konzept von Microsoft Windows ist es, eine möglichst binärkompatible Plattform für Anwendungsprogramme zu sein. Realisiert wird dies über die Win32-API und deren Erweiterungen. Windows ist seit den Anfängen eine \"offene\" Plattform, das heißt, jeder kann uneingeschränkt Anwendungen für Windows schreiben und vertreiben, da keine Erlaubnis seitens Microsoft oder der Erwerb kostenpflichtiger Dokumentationen dafür notwendig ist. Microsoft ist auch deshalb Marktführer, weil die Abwärtskompatibilität der Windows-Plattform lange Zeit höchste Priorität bei der Weiterentwicklung von Windows hat. Eine solche stabile binärkompatible Plattform hat für Anwendungsprogrammanbieter den Vorteil, dass Anwendungen nicht für jede Windowsvariante (vergangene oder zukünftige) angepasst werden müssen, sondern der Plattform-Anbieter für die Kompatibilität verantwortlich ist. Microsoft hat deswegen eine stetig wachsende Menge anwendungsspezifischer Workarounds, sogenannte , in jede neue Windowsversion integrieren müssen.\nFür den Nutzer ergibt sich der Vorteil, dass er seine gewohnte Software in der gewünschten Version auch nach einem Windows-Upgrade weiterverwenden kann und er nicht auf eine Anpassung warten oder auf eine andere Software umsteigen muss.\nNachteil dieser stabilen aber proprietären Windows-Plattform ist der Lock-in-Effekt, der einen Betriebssystemwechsel aufgrund mangelnder Interoperabilität unattraktiv macht.\n\nDie Familie der Windows-Betriebssysteme besitzt die Marktführerschaft auf dem PC-Markt mit einem Anteil von 80 bis 90 % laut verschiedener WebStat-Counter, gefolgt von Apple-Betriebssystemen.\n\nDa zur Zeit der Entwicklung von Windows NT die heutige Relevanz des Internets von Microsoft falsch eingeschätzt wurde, vernachlässigte der Konzern zunächst die Internetsicherheit. Darüber hinaus wurde Windows bis Windows Server 2003 standardmäßig mit deaktivierten Sicherheitsoptionen ausgeliefert, und angreifbare (wenn auch nützliche) Dienste waren aktiviert. Im Juni 2005 hat Bruce Schneiers \"Counterpane Internet Security\" berichtet, dass sie mehr als 1000 neue Viren und Würmer für Windows-Systeme in den letzten sechs Monaten gesichtet hätten.\n\nUm die Jahrtausendwende gab Microsoft bekannt, dass dem Thema Sicherheit fortan höchste Bedeutung zugemessen werde. Das dringend benötigte System für automatische Updates wurde erstmals mit Windows 2000 eingeführt. Das Ergebnis war, dass das Service Pack 2 für Windows XP und das Service Pack 1 für Windows Server 2003 sehr viel schneller von den Kunden installiert wurden, als das bei früheren Aktualisierungen der Fall war. Microsoft verteilt Sicherheitspatches über sein \"Windows Update System\" normalerweise einmal im Monat im Rahmen des sogenannten Patchdays.\n\nSpätestens seit dem in den Medien sehr präsenten Computerwurm W32.Blaster und seinen Derivaten im Jahr 2003 hat Microsoft bei Windows Vista den zentralen Fokus auf die Internetsicherheit gelegt. Laut Eigenaussage von Microsoft hatte dies zur Folge, dass bei Vista nach seiner Einführung deutlich weniger Sicherheitslücken aktiv ausgenutzt worden seien als bei Windows XP und Vista deutlich seltener von Schadsoftware befallen worden sei. Die zusätzliche Sicherheit wird vor allem auf neu entwickelte Sicherheitsmaßnahmen wie die Benutzerkontensteuerung zurückgeführt, die jedoch auch Einschränkungen beim Bedienkomfort nach sich zieht.\n\nMit Windows 7 sind die Sicherheitsprobleme weiter zurückgegangen. Wegen der deutlichen Weiterentwicklung der Sicherheitsmaßnahmen weichen Angreifer inzwischen verstärkt auf Schwachstellen in Drittprogrammen aus.\n\nSicherheitsanalysen von Drittparteien\n\nEine Studie von Kevin Mitnick und der Agentur Avantgarde aus dem Jahre 2004 führte zu dem Ergebnis, dass eine ungepatchte Installation von Windows XP mit Service Pack 1 bereits nach vier Minuten von einer Infektion befallen wurde, wenn diese direkt mit dem Internet verbunden war (also auch aus dem Netz erreichbar war). Diese sehr kurze Zeit kommt unter anderem durch die fehlende Windows-Firewall in Kombination mit der Sicherheitslücke im Windows-RPC-Dienst zustande, aber auch durch ungünstige Standardeinstellungen. Seit Windows XP Service Pack 2 ist eine Firewall bereits integriert und standardmäßig aktiviert, sodass die Studie hier zu deutlich besseren Ergebnissen kommt.\n\nSicherheitsexperten von iSec kommen in einer auf der Black Hat 2011 präsentierten Studie zu dem Schluss, dass Windows 7 sicherer sei als Mac OS X. Letzteres weise im Netzwerk einige Schwächen auf. Die Autoren haben zwar hauptsächlich die Sicherheit von Mac OS X Leopard bis Lion getestet, kommen aber auch zu dem Ergebnis, dass Windows 7 sicherer sei als Mac OS X 10.7.\n\nDas Betriebssystem unterstützt Programme für die Windows-Plattform sowie seit Windows Vista auch .NET-Programme. Windows-Laufzeitumgebungen für Java sind von Microsoft, Oracle und anderen Anbietern erhältlich. Einige Ausgaben der Windows-Versionen mit NT-Kernel enthalten auch das POSIX-Subsystem. Dieses ist aber in der Praxis bedeutungslos, da POSIX-Elemente wie symbolische Links und eine POSIX-kompatible Shell fehlen. POSIX-kompatible Anwendungen wären nur sehr mühsam auf Windows-Betriebssysteme portierbar. Mit Cygwin und Windows Subsystem for Linux existiert jedoch eine Abstraktionsschicht, womit Unix- und Linux-Programme unter Windows ausführbar sind.\n\nKernel-Mode-Treiber für Windows Vista in der 64-Bit-Version müssen signiert sein, um beim Systemstart automatisch geladen werden zu können. Diese Signierung ist für Hardwarehersteller kostenpflichtig.\n\nDie enge Kopplung der Anwendungen Webbrowser (Internet Explorer), Media Player und Windows-Explorer zur Dateiverwaltung war Gegenstand langjähriger Rechtsstreitigkeiten mit Anbietern alternativer Anwendungsprogramme. Diese Anbieter sahen in der engen Verzahnung der Anwendungsprogramme mit dem Betriebssystem einen ungerechtfertigten Wettbewerbsvorteil Microsofts. Mit Windows XP sowie mit dem Service Pack 3 für Windows 2000 ermöglichte Microsoft die Kopplung des Betriebssystems an alternative Anwendungsprogramme.\n\nDie Kryptografiebibliothek von Windows enthält zwei öffentliche Schlüssel; während der erste Schlüssel Microsoft gehört, war die Bedeutung des zweiten Schlüssels, der in allen Windows-Versionen seit Windows 95 OSR2 enthalten ist, zunächst unbekannt. Als das Service Pack 5 von Windows NT 4.0 veröffentlicht wurde und Microsoft vergaß, die Debugsymbole zu entfernen, fielen Entwicklern die Namen der zwei Schlüssel auf. Der erste Schlüssel hieß \"_KEY\", der zweite \"_NSAKEY\". Dies löste Spekulationen aus, dass der zweite Key der National Security Agency (NSA) gehöre, die damit eigene Anwendungen signieren und Windows-Systeme kompromittieren könne. Microsoft veröffentlichte später eine Pressemitteilung, in der sie jeden Bezug des NSAKEY-Schlüssels zur Behörde NSA dementierten.\n\nEntwicklern fiel in der Beta-Version von Windows 2000 ein dritter Schlüssel auf, was selbst die Windows-Entwickler überraschte. Microsoft betonte in einer Pressemitteilung, dieser Schlüssel signiere Cryptographic Service Provider zu Testzwecken.\n\n\n"}
{"id": "4037876", "url": "https://de.wikipedia.org/wiki?curid=4037876", "title": "California 3000", "text": "California 3000\n\nCalifornia 3000 ist eine für das Bauwesen entwickelte Software für Microsoft Windows der G & W Software Entwicklung GmbH aus München. \n\nDie Software kann zur Kostenplanung, Ausschreibung, Vergabe und Abrechnung (AVA) im Bauwesen und zum Baucontrolling während der Planung von Bauprojekten und Überwachung eingesetzt werden. Verwendung findet sie bei Unternehmen aus den Bereichen Architektur- und Ingenieurbüros, Bauämter, Projektsteuerer, Bauabteilungen von Banken, Versicherungen und Industriekonzernen, Ver- und Entsorgungsunternehmen, Wohnungsbau- und Immobiliengesellschaften und Bauträgern.\n\nDie Software unterstützt die Anwender vom Kostenrahmen durch die HOAI-Phasen 1–9 bis zur Kostenfeststellung und Dokumentation der abgeschlossenen Baumaßnahme. \n\n1983 gründeten die Geschäftsführer und Gesellschafter Erwin Grütter und Dr. Achim Warkotsch die G & W Software Entwicklung GmbH in München.\nDie ersten Entwicklungen wurden auf Olivetti-Rechnern durchgeführt. Über MS-DOS erfolgte der Umstieg zu Windows.\n\nNach Angaben des Herstellers haben 6.000 Unternehmer California im Einsatz.\n\n"}
{"id": "4037960", "url": "https://de.wikipedia.org/wiki?curid=4037960", "title": "COSIMIR", "text": "COSIMIR\n\nCOSIMIR (\"Cell Oriented Simulation of Industrial Robots\") ist ein 3-D-Simulationssystem und wurde in der Zeit von 1992 bis 2005 am Institut für Roboterforschung der Technischen Universität Dortmund entwickelt. Ein kompatibles Produkt mit dem Namen CIROS wird seit 2008 bei RIF e. V. – Institut für Forschung und Transfer in Dortmund entwickelt.\n\nCOSIMIR wurde ursprünglich zur Offline-Programmierung und Simulation einfacher Roboterarbeitszellen eingesetzt und war das erste in Deutschland entwickelte 3-D-Simulationssystem. Durch die Fähigkeit zur Simulation von Sensoren, Transportprozessen, SPSen und anderen Steuerungen (Hardware-in-the-Loop) wurde der Einsatzbereich auf vollständige industrielle Fertigungsanlagen ausgedehnt. COSIMIR wird als Werkzeug im Rahmen der Digitalen Fabrik angewendet.\n\nCOSIMIR wurde in diversen nationalen und internationalen Forschungsprojekten eingesetzt:\n\n\n"}
{"id": "4038945", "url": "https://de.wikipedia.org/wiki?curid=4038945", "title": "Computer History Museum", "text": "Computer History Museum\n\nDas Computer History Museum ist ein 1996 gegründetes Museum in Mountain View im Santa Clara County im US-Bundesstaat Kalifornien, das sich der Bewahrung und Darstellung der IT-Geschichte widmet. Es beherbergt eine der weltweit größten Sammlungen an Computern.\n\nGordon Bell gründete 1979 mit finanzieller Unterstützung durch DEC das \"Digital Computer Museum\". Im September 1979 wurde in Camden das Museum eröffnet.\n\n1983 wurde der Name in \"The Computer Museum\" geändert und ein Umzug nach Boston geplant. Zum 13. November 1984 bezog das Museum ein ehemaliges Lagerhaus, das gemeinsam mit dem \"The Children’s Museum of Boston\" genutzt wurde. Die Abkürzung \"TCM\" wurde für beide Institutionen gemeinsam geprägt.\n\n\"The Computer Museum History Center\" wurde 1996 als Außenstelle in Mountain View im Silicon Valley gegründet und die Baracken des ehemaligen Möbellagers des Marinestützpunkts \"Moffett Field\" dienten als erster Standort. Als Arbeitsauftrag galt das Sammeln von Artefakten und Geschichtsdaten vor Ort für das Museum.\n\nDas Computermuseum in Boston wurde 1999 geschlossen und das Children’s Museum übernahm die Räumlichkeiten. Die dort verbliebenen Ausstellungsstücke übernahm das Science Museum in Boston.\n\nMit Ausnahme einer Vielzahl von Ausstellungsstücken um die Robotersammlung übergab das Science Museum im Jahr 2000 seine Computersammlung an das Computer Museum History Center. Sie wurde übergangsweise in einem großen Luftschiffhangar des Moffett Airfield gelagert.\n\n2001 wurde der Name auf \"Computer History Museum\" (CHM) geändert.\n\nDurch Spenden konnte im Jahr 2002 das ehemalige Verwaltungsgebäude von SGI erworben werden. Im Juni 2003 wurde der Betrieb im neuen Gebäude mit einer provisorischen Ausstellung aufgenommen. Aus Platzgründen konnten dort nur wenige Prozent des gesamten Fundus gezeigt werden; der Rest wurde in Lagerhäusern in der Umgebung eingelagert.\n\nNach einer 19 Millionen Dollar teuren und zwei Jahre dauernden Renovierung konnte das Museum mit der erweiterten Dauerausstellung \"Revolution: The First 2000 Years of Computing\" im Jahr 2011 wiedereröffnet werden.\n\nEtwa im Jahr 2010 begann das Museum mit der Archivierung von Quelltext wichtiger Software, beginnend mit MacPaint 1.3, bestehend aus Assembler und Pascal-Code, welcher von Apple Inc. freigegeben wurde. 2012 folgte der Quelltext der Programmiersprache APL. Im Februar 2013 stiftete Adobe Systems den Photoshop-1.0.1-Quelltext für die Sammlung, im November 2013 die Apple Inc. den Apple DOS-Quelltext des Apple II. Am 25. März 2014 folgten MS-DOS 1.1, MS-DOS 2.0 sowie Word for Windows 1.1a, gestiftet durch Microsoft. Am 21. Oktober 2014 folgte der Xerox Alto-Quelltext und andere Ressourcen.\n\nDie Sammlung umfasst ungefähr 90.000 Objekte, Filme und Fotografien, sowie eine Vielzahl von Dokumenten und digitalen Daten.\nZu den Ausstellungsstücken zählen Raritäten wie der 1970 erbaute \"Mulby M\" der Firma Krantz Computer, der Apple I, der Cray-1, der Cray-2, der Cray-3 und der originale Utah Teapot, der Martin Newell als Vorbild für sein 3D-Modell diente, das auch heute noch ein Standard-Referenz-Modell für Computergrafik ist.\n\nDas in Deutschland beheimatete Heinz Nixdorf MuseumsForum warb genauso wie das Computer History Museum damit, das größte Computermuseum der Welt zu sein.\nIm Vergleich zum Heinz Nixdorf MuseumsForum, das über eine Dauerausstellungsfläche von 6000 Quadratmetern verfügt, stellt das Computer History Museum seine Exponate auf einer wesentlich kleineren Fläche von 2300 Quadratmetern aus. Bei der Zahl der Exponate liegt das deutsche Museum allerdings deutlich hinter dem amerikanischen zurück.\n\nSeit 1987 vergibt das Museum den \"Computer History Museum Fellow Award\". Die Preisträger sind Menschen, die bedeutende Beiträge zur Weiterentwicklung der Computertechnologie geleistet haben. In jedem Jahr werden von der Öffentlichkeit Vorschläge für mögliche Kandidaten entgegengenommen. Dadurch soll gewährleistet werden, dass die verschiedensten Leistungen für die Auszeichnung berücksichtigt werden können. Die endgültige Auswahl trifft ein Gremium aus Historikern, Wissenschaftlern, Vertretern der Industrie sowie Museumsmitarbeitern und früheren Preisträgern. Die erste Preisträgerin war die Computerpionierin Grace Murray Hopper. Zu den weiteren Preisträgern zählen unter anderen Konrad Zuse, der Erfinder des ersten Computers, Steve Wozniak, der den Apple I baute, und Linus Torvalds, der den Linux-Kernel schrieb.\n\n"}
{"id": "4044105", "url": "https://de.wikipedia.org/wiki?curid=4044105", "title": "Zenith minisPORT", "text": "Zenith minisPORT\n\nDer Zenith minisPORT war ein Laptop, den Zenith Data Systems 1989 auf den Markt brachte. Der Computer verfügte über eine Intel 80C88 CPU, die – per Software umschaltbar – mit 4,77 oder 8 MHz betrieben werden konnte. Des Weiteren hatte er 1 MB RAM, der auf 2 MB aufgerüstet werden konnte. Der minisPORT wurde mit MS-DOS betrieben, das aus dem ROM geladen wurde. Sein LCD hatte eine Auflösung von 640 × 200 Pixel. Er war einer der ersten sehr kleinen Laptops und kann als Vorläufer heutiger Subnotebooks angesehen werden.\n\n\n\n"}
{"id": "4045808", "url": "https://de.wikipedia.org/wiki?curid=4045808", "title": "Tasman (Software)", "text": "Tasman (Software)\n\nTasman ist eine HTML-Rendering-Engine von Microsoft und wurde erstmals mit der Macintosh-Version von Internet Explorer 5 eingeführt. Tasman sollte die vom World Wide Web Consortium geschaffenen Web-Standards besser unterstützen, als die bisher verwendete Trident-Engine. Zum Zeitpunkt der Veröffentlichung galt Tasman als die Engine mit der besten Unterstützung für Web-Standards wie HTML und CSS. Die Entwicklung des Internet Explorer for Mac wurde inzwischen eingestellt, jedoch werden neuere Versionen der Tasman-Engine noch immer in verschiedenen Microsoft-Produkten verwendet, so unter anderem in Microsoft Office 2004/2008 oder MSN for Mac OS X.\n\nDie Tasman-Engine wurde unter Leitung von Tantek Çelik entwickelt.\n\nDie erste Version der Tasman-Engine (als „v0“ bezeichnet) wurde am 27. März 2000 mit dem \"Internet Explorer 5 Macintosh Edition\" veröffentlicht. Mit Erscheinen des \"Internet Explorer 5.1 for Mac\" folgte dann Version 0.1 der Engine.\n\nAm 15. Mai 2003 stellte Microsoft den abonnement-pflichtigen Browser MSN for Mac OS X vor, der mit der Tasman-Version 0.9 aufwartete. In einer Nachricht an die Newsgroup \"Mac Internet Explorer Talk\" listete der damalige Manager des Internet Explorer for Mac Jimmy Grewal einige Verbesserungen auf:\n\nAm 11. Mai 2004 wurde Microsoft Office 2004 for Mac veröffentlicht, welches eine Version der Tasman-Engine in seiner E-Mail-Software Entourage verwendete.\n\nDie Eingabe codice_1 in der Mac-Version des Internet Explorers zeigt eine Liste der Mitarbeiter, die an der Unterstützung des Acid1-Tests Anteil hatten.\n"}
{"id": "4046811", "url": "https://de.wikipedia.org/wiki?curid=4046811", "title": "LANline", "text": "LANline\n\nDie LANline ist eine deutsche IT-Fachzeitschrift.\n\nDie LANline erscheint als Print-Ausgabe monatlich im DIN A4-Format. Zusätzlich gibt es eine Online-Berichterstattung, einen E-Mail-Newsletter und weitere digitale Publikationen.\n\nAls B2B-Zeitschrift wurde sie zunächst von 1988 an vom AWi Verlag und dann bis 2009 von der Konradin IT-Verlag GmbH in Grasbrunn herausgegeben, die zum Konradin Verlag gehörte. Dieser verkaufte die LANline 2009 an den ITP Verlag in Grasbrunn. Seit Ausgabe 3/2018 erscheint die LANline bei WEKA Fachmedien GmbH in Haar. Chefredakteur ist Jörg Schröper.\n\nZu den Themengebieten gehören neben der Netzwerktechnik unter anderem alle Formen der IT-Verkabelung, der RZ-Infrastruktur sowie IT-Management und -Sicherheit. Im Umfeld dieser Themen richtet die LANline an mehreren Standorten in Deutschland, in Österreich und in der Schweiz Veranstaltungen als Tech Forum und als Datacenter Symposium aus.\n\nDie LANline erscheint monatlich im DIN A4-Format, zusätzlich zu den Stammheften werden Sonderhefte und E-Books produziert. Nach IVW hat die Zeitschrift eine verbreitete Auflage von Exemplaren. Eine Besonderheit und das Erkennungsmerkmal der Publikation sind die Titelbilder, die der Künstler Wolfgang Traub gestaltet.\n\n"}
{"id": "4048036", "url": "https://de.wikipedia.org/wiki?curid=4048036", "title": "Midori (Browser)", "text": "Midori (Browser)\n\nMidori (, japanisch für „grün“) ist ein freier und plattformübergreifender Webbrowser. Er ist Teil der Xfce Umgebung. Seit 2019 ist Midori Mitglied im Astian Foundation Projekt.\n\nMidori ist in Vala geschrieben, die Benutzeroberfläche ist unter GTK+ realisiert. Für das HTML-Rendern kommt die GTK+-Variante von WebKit zum Einsatz. \n\nMit Hilfe von Nutzerskripten und -styles können, ähnlich wie mit der Greasemonkey Erweiterung, Webseiten nach eigenen Bedürfnissen verändert werden.\n\nDer Browser enthält einen Adblocker, der externe Filter unterstützt. Außerdem können Cookies verwaltet werden.\n\nAufgrund der minimalistischen Oberfläche ist der Browser schnell und ressourcenschonend.\n\n"}
{"id": "4059891", "url": "https://de.wikipedia.org/wiki?curid=4059891", "title": "Kempston Micro Electronics", "text": "Kempston Micro Electronics\n\nKempston Micro Electronics war ein britisches Elektronikunternehmen, das sich in den 1980er Jahren auf den Vertrieb von Joysticks und Peripheriegeräte für Heimcomputer spezialisiert hatte; einige Geräte stellte die Firma auch selbst her. Das Unternehmen hatte seinen Hauptsitz in Kempston, Bedfordshire, England.\n\nUm einen Markt für die von der Firma angebotenen Joysticks zu erschließen, produzierte Kempston verschiedene Joystick-Interfaces für den in Großbritannien meistverbreiteten Heimcomputer, den Sinclair ZX Spectrum, der anders als viele andere Heimcomputer ab Werk keine Buchsen für den Anschluss von Joysticks mitbrachte. Mit den Kempston-Interfaces, die in den universalen Erweiterungsport auf der Rückseite des Spectrum eingesteckt wurden, wurde es möglich, die damals als De-facto-Standard anzusehenden Atari 2600-kompatiblen Joysticks mit DE-9-Stecker an diesem Rechner zu verwenden. Mit dem Verkauf von diesen Joystick-Interfaces etablierte Kempston auch einen Standard für das Auslesen der Joystick-Position durch Spectrum-Software; ein Bitmuster der derzeit geschlossenen Kontakte erscheint aus Sicht des Z80-Prozessors auf dem E/A-Port 31 und kann daher z. B. mit dem BASIC-Befehl \"LET j=IN 31\" gelesen werden. Diese Ansteuerlogik wurde später auch von anderen Unternehmen übernommen. Sinclair selbst übernahm sie jedoch nicht, als die Firma schließlich mit dem \"ZX Interface 2\" ein eigenes Joystick-Interface für den ZX Spectrum anbot. Viele spätere Spectrum-Spiele erlauben dem Benutzer daher die Auswahl zwischen mehreren verschiedenen Joystick-Interfaces oder auch der Verwendung der Tastatur als Ersatz, wenn gar kein Interface vorhanden ist.\n\nJoysticks, die von Kempston vertrieben wurden und mit dem Interface funktionierten, waren u. a. Competition Pro, Competition Pro Plus, Score Board und Formula 1 und 2.\n"}
{"id": "4064054", "url": "https://de.wikipedia.org/wiki?curid=4064054", "title": "Heathkit H89", "text": "Heathkit H89\n\nDer H89 war einer der ersten Personal Computer für den Heimanwender. 1979 wurde er von der Firma Heathkit unter der Bezeichnung H89 (als Bausatz) beziehungsweise WH89 (als Fertiggerät, W für Wired) auf den Markt gebracht. Nach der Übernahme von Heathkit durch Zenith Data Systems vertrieb Zenith ihn unter der Bezeichnung Z-89.\n\nDer Rechner verfügte über zwei Zilog Z80 Prozessoren mit einer Taktfrequenz von je 2 MHz und arbeitete mit den Betriebssystemen HDOS und CP/M. Einer der Prozessoren war für die Terminalfunktionen zuständig. Er vereinigte einen 12\"-CRT-Monitor, Tastatur und Diskettenlaufwerke in einem Gehäuse. Die Diskettenlaufwerke waren an einem Controller für hartsektorierte Disketten angeschlossen und verarbeiteten hartsektorierte 5,25\"-Disketten mit einer Kapazität von 100 kB.\n\nAls Zubehör wurden verschiedene Diskettenstationen angeboten:\n\n\nNeben dem Grundmodell waren verschiedene Variationen erhältlich. Ohne Diskettenlaufwerke, jedoch mit einem Datasetteninterface wurde er als Version H88 verkauft. Unter der Bezeichnung H19 verkaufte Heathkit ein Terminal, das durch ein Upgradekit zu einem H89 ausgebaut werden konnte. Das Modell H90 verfügte statt der im H89 verbauten Diskettenlaufwerke über Laufwerke, die softsektorierte Disketten mit einer Kapazität von 640 kB verarbeiten konnten sowie über den dafür notwendigen Controller.\n\nHeathkit bot für den H89 neben dem Betriebssystem einen Texteditor, einen Assembler und eine Version seines Benton Harbor BASIC Interpreters an.\n\n"}
{"id": "4069683", "url": "https://de.wikipedia.org/wiki?curid=4069683", "title": "IBM 5550", "text": "IBM 5550\n\nDer IBM 5550 ist eine Serie von Mikrocomputern, welche in den 1980ern und 1990ern in Ostasien vermarktet wurde. Sie wurde damit vermarktet, dass sie die drei Rollen PC, Schreibmaschine und Terminal in einem Rechner vereinte.\n\nDer IBM-PC, welcher seit 1981 auf dem Markt war, konnte nicht mit ostasiatischen Sprachen umgehen, da der Intel 8088-Prozessor, der im IBM-PC verwendet wurde, nicht leistungsfähig genug war und die Auflösung des Bildschirms zu niedrig für die chinesischen Schriftzeichen war.\n\nDaraufhin wurde der IBM 5550 veröffentlicht, der den leistungsfähigeren Intel 8086-Prozessor und einen hochauflösenden Bildschirm mit 1024×768 Pixeln (einige Varianten auch nur 720×512) verwendete. Zunächst verwendete der IBM 5550 eine eigene Architektur, später wurde die Micro Channel Architecture des Personal System/2-Rechners übernommen. Der Rechner wurde 1983 erstmals in Japan vermarktet, später folgten Versionen für Korea, VR China und Taiwan. Als Betriebssystem kamen MS-DOS und OS/2 zum Einsatz.\n\n"}
{"id": "4070123", "url": "https://de.wikipedia.org/wiki?curid=4070123", "title": "Mac OS X Snow Leopard", "text": "Mac OS X Snow Leopard\n\nSnow Leopard (), vollständig Mac OS X Snow Leopard 10.6, ist die siebte Hauptversion von macOS, dem Desktop-Betriebssystem von Apple, das seinerzeit unter dem Namen Mac OS X eingeführt wurde. Es folgte auf Mac OS X Leopard und war seit dem 28. August 2009 verfügbar. Diese Ausgabe beinhaltet in erster Linie Verbesserungen der bestehenden Technik, zahlreiche Detailänderungen in den Arbeitsabläufen und Änderungen an der grafischen Benutzeroberfläche. Ein Apple-Rechner mit Intel-Prozessor ist Voraussetzung, ältere Macs mit PowerPC-Prozessor werden nicht mehr unterstützt. Die nachfolgende Version ist Mac OS X Lion und wurde am 20. Juli 2011 veröffentlicht.\n\nNeben Verbesserungen der Leistung, Effizienz und Stabilität des Systems bietet Snow Leopard u. a. folgende neue Funktionen:\n\n\nApple gibt für Snow Leopard folgende Voraussetzungen an, damit alle Anwendungen funktionieren:\n\nAnmerkung: Beim MacBook Air ist das DVD-Laufwerk keine Systemvoraussetzung, da für die Installation auch das DVD-Laufwerk eines anderen Macs (\"Remote Disk\") bzw. bei den Modellen der dritten Hardwaregeneration (\"MacBookAir3,1\", ab Oktober 2010) der beiliegende USB-Stick genutzt werden kann.\n\nZum Zeitpunkt der ersten \"Developer Preview\" war OpenGL 2.1 spezifiziert. OpenGL 3.0 wurde etwa ein Jahr vor dem Erscheinen von Snow Leopard veröffentlicht und wurde trotz mehrerer Systemaktualisierungen nur zu 91 % implementiert.\n\n"}
{"id": "4079690", "url": "https://de.wikipedia.org/wiki?curid=4079690", "title": "Meizu M8", "text": "Meizu M8\n\nDas Meizu M8 ist ein internet- und multimediafähiges Multi-Touch-Smartphone des chinesischen MP3-Player-Herstellers Meizu. Schon lange vor dessen Erscheinen regte das Handy aufgrund seiner starken Ähnlichkeit zu Apples iPhone zu kontroversen Diskussionen an. Das Betriebssystem des Meizu M8 oder auch \"minione\", ist eine von Meizu stark veränderte Version von Windows CE 6.0, die der GUI von Apple iOS ähnelt.\n\nEine erste Version des M8 wurde am 4. März 2008 auf der Cebit vorgestellt. Zu sehen war ein nicht-funktionaler Plastikprototyp und ein Entwicklungsboard mit einer ersten Version des Meizu OS inklusive Telefonfunktion.\nDer Release des Handy verzögerte sich seit Ankündigung um fast 2 Jahre. Ab dem 8. Dezember 2008 war erstmals eine geringe Stückzahl auf dem chinesischen Markt erhältlich. Dieser „Testversion“ fehlten allerdings die für China wichtigen Mobilfunklizenzsticker. Seit dem 14. Januar 2009 hat Meizu jedoch diese Lizenz. Der offizielle Releasetermin der schwarzen 8-GB-Version des Meizu M8 war der 18. Februar 2009. Später erschien eine 16-GB-Version sowie am 28. April 2009 schließlich die weiße Version des M8.\n\nMit \"M8Android\" gab es einen Versuch, das Betriebssystem Android auf das Gerät zu portieren.\n\nMeizu entwickelte für die Firmware-Version 1.0 die komplette Oberfläche neu und erweiterte das Handy um einige Funktionen, wie zum Beispiel direkten Download aus dem integrierten Opera-Browser oder Programme wie den Messenger QQ. Dazu wurde das Betriebssystem, auf dem die Meizu Oberfläche aufbaut, auf WinCE 6.0 Release 3 aktualisiert. Dieser Entwicklungsprozess dauert noch immer an. Die neue Oberfläche wurde erstmals offiziell am 6. Februar 2010 mit der Firmware-Version 0.9.6.9 veröffentlicht.\n\nAm 5. März wurde der Meizu-Stand auf der CeBit 2008 von der Polizei geschlossen. Das M8 war entgegen allen Gerüchten allerdings nicht der Grund, sondern angeblich unlizenzierte Unterstützung des MP3-Formates auf den Meizu-MP3-Playern.\n\nMit dem Marktstart des iPhone auf dem chinesischen Markt strengte Apple zunächst eine Übereinkunft mit dem chinesischen Patentamt und Meizu für einen Produktionsstopp an, verlangte dann aber einen sofortigen Verkaufsstopp, der auch durchgesetzt wurde.\n\nDa WLAN von der chinesischen Regierung ohne WAPI nicht freigegeben ist, ist die Funktion standardmäßig nicht verfügbar. Die erforderliche Hardware ist in einigen Geräten allerdings vorhanden, so dass man mit einem einfachen Programm WLAN auf dem M8 freischalten kann. Ebenso kann statt GPRS EDGE genutzt werden.\nEine internationale Version des M8 mit WLAN wurde nie öffentlich angekündigt. Dennoch werden auch weiterhin im Internet WLAN-Versionen des Handys angeboten.\n\nAuf der offiziellen Meizu-Website wurde das Handy als Dualband-Handy angepriesen. Dennoch sollen zumindest Handys der ersten Produktionscharge, die sogenannten \"Testgeräte\", auch mit den Frequenzen 850 und 1900 MHz funktionieren. Dies bestätigten Benutzer in Meizu-Community-Foren.\n\nAm 22. September 2009 erschien die zweite Version des M8, das M8SE (Second Edition). Die Produktion des M8 wurde zeitgleich eingestellt. Neu an dieser Version ist die offizielle Unterstützung von WLAN bzw. WAPI sowie EDGE. Weiter wurde die SIM-Abdeckung leicht verändert, einige Chips ausgetauscht sowie der Musik-Button an der rechten Seite des Handys weggelassen.\n\nSchon lange vor dem Erreichen der Firmware 1.0 kündigte Unternehmenschef Jack Wong bereits Nachfolger des M8 an.\nDas M9 ist 3G-fähig und ausgestattet mit einem 1-GHz-Prozessor, einem 3,6-Zoll-multitouch-Display mit einer Auflösung von 960 × 640 Pixeln, 512 MB RAM, WLAN und GPS sowie einer 5-Megapixel-Kamera. Auf internen Speicher wurde verzichtet und dafür auf einen microSD-Kartenslot gesetzt. Als Betriebssystem wählte man diesmal Android 2.2 mit einer Meizu-eigenen Oberfläche.\nDas Meizu M9 ist seit dem 25. Dezember 2010 erhältlich.\n\n"}
{"id": "4080176", "url": "https://de.wikipedia.org/wiki?curid=4080176", "title": "Heathkit H8", "text": "Heathkit H8\n\nDer H8 war ein Computer der Firma Heathkit in Bausatzform und wurde ab 1977 verkauft. Er war der erste Computer für den Heimanwender<ref name=\"PM 9/77\">Anthony R. Curtis: \"Everyman's home computer is here.\" Popular Mechanics, Oktober 1977, S. 92.</ref>. Der Grundbausatz kostete zur Markteinführung 375 US-$.\n\nDer Rechner basierte auf einem Intel 8080A. Er verfügte im Grundausbau über eine Tastatur mit 16 Tasten für die Eingabe von oktalen Daten. Zur Anzeige von Daten war ein LED-Display aus Siebensegment-Anzeigen verbaut. Im Grundbausatz war ein ROM mit einer Größe von 8 kB enthalten. Zusätzlich waren Speicherkarten mit einer Größe von je 4 kB verfügbar, mit denen der Hauptspeicher bis auf 32 kB ausgebaut werden konnte. Zum Anschluss von externen Geräten hatte das System einen proprietären 50-Pin Bus. Betrieben wurde das System mit dem Betriebssystem HDOS.\n\nDer Bausatz alleine war jedoch noch nicht einsatzfähig. Es wurde mindestens eine H8-1 Speichererweiterungkarte für 140 US-$ benötigt. Zusätzlich konnte das System um die folgenden Komponenten erweitert werden:\n\n\nAb 1978 war eine Diskettenstation mit zwei Laufwerken zum Preis von 675 US-$ erhältlich.\n\n"}
{"id": "4088751", "url": "https://de.wikipedia.org/wiki?curid=4088751", "title": "Bildpunktgeometrie", "text": "Bildpunktgeometrie\n\nBildpunktgeometrie oder Pixelgeometrie bezeichnet das Verhältnis von Breite zur Höhe eines Bildpunktes auf einem Fernseh- oder Computerbildschirm.\n\nBei den in Europa gültigen SDTV-Fernsehnormen beträgt das Seitenverhältnis 1,064:1. LCDs hingegen besitzen und HD-Sender senden jedoch quadratische Bildpunkte. Bei Computerbildschirmen ist ebenso ein Seitenverhältnis von 1:1 üblich. Um eine Bildverzerrung zu vermeiden, muss das gesendete SD-Fernsehbild für den LC- oder Computermonitor bzw. das HD-Signal für ein SD-Gerät umgerechnet werden.\n"}
{"id": "4089037", "url": "https://de.wikipedia.org/wiki?curid=4089037", "title": "Parted Magic", "text": "Parted Magic\n\nParted Magic ist eine Live-Linux-Distribution mit dem Ziel, für beschreibbare Datenspeicher wie Festplattenlaufwerke oder Solid-State-Drives (SSD) eine Management-Lösung zu bieten. Dabei können sowohl bestehende Datenbestände im Rahmen der Datenwiederherstellung versucht werden wiederherzustellen oder auch diese davon unabhängig zu manipulieren. Weiters können bestehende Daten in Form kompletter Partitionsabbildung gesichert und wiederhergestellt werden, ferner wird die Möglichkeit einer sicheren Datenvernichtung angeboten, so dass gelöschte Daten auch nicht mehr mit forensischen Spezialwerkzeugen zurückgeholt werden können.\n\nDazu bietet das System neben einer kompletten Linux-Umgebung eine Zusammenstellung verschiedener Programme an. Unter anderem das Partitionierungsprogramm GNU Parted zur Partitionsmanipulation, Clonezilla zur Sicherung und Wiederherstellung von kompletten Datenspeichern, und Tools wie \"Erase Disk\" zur sicheren Datenlöschung.\n\nParted Magic ist als Direktstartsystem für die Speichermedien CD und USB-Stick, sowie im PXE-Format erhältlich und lädt sich beim Startvorgang komplett in den Arbeitsspeicher. Für den Betrieb werden mindestens 1 GB Arbeitsspeicher benötigt (als Direktsystem 512 MB).\n\nNeben GNU Parted und dem darauf aufbauenden GParted bietet Parted Magic unter anderem folgende Programme an, um Festplatten auf Fehler zu untersuchen und diese wenn möglich zu beheben:\n\nDie Distribution verwendet die Desktop-Umgebung LXDE und bietet die in vielen heutigen Betriebssystemen üblichen Programme wie PDF-Betrachter (Evince), Bildbetrachter (Mirage), Packprogramme (File Roller), Musik-Player (Audacious), Brennprogramme (Xfburn), CD-Ripper (Asunder) und das Drucksystem CUPS an.\nZudem sind einige Internet-Programme wie ein IRC-Client (XChat) und der Webbrowser Firefox mit nachladbarem Flash und Java als auch der freie Virenscanner ClamAV mit der grafischen Benutzeroberfläche ClamTk vorhanden. Eine simple GUI ermöglicht das Speichern von getätigten Einstellungen für zukünftige Sitzungen. Auch die Installation auf einer Festplatte ist möglich.\n\nParted Magic wird unter der GNU General Public License veröffentlicht und ist damit Freie Software.\n\nFrüher konnte Parted Magic kostenfrei von der Webseite des Projektes heruntergeladen werden. Seit Anfang August 2013 jedoch (ab Version 2013_08_01) wird für den Bezug von Parted Magic über eine Paywall der Projekt-Webseite eine Gebühr erhoben. Das Preismodell sieht verschiedene Tarife vor und reicht von 9,99 USD für einen Einmal-Download des CD-Images, über 12,99 USD für den Erhalt von Parted Magic auf CD bzw. ab 17,99 USD auf USB-Stick, bis zu 49,99 USD für ein Jahresabonnement, welches zu unbeschränkten Downloads aller erscheinenden Versionen incl. der wöchentlichen Builds legitimiert.\n\nDer Sourcecode von Parted Magic ist weiterhin gratis herunterladbar. Der Hintergrund für das Einrichten der Paywall war die Arbeitslosigkeit des Programmierers und seiner Frau.\n\nDer Autor spendet nach Eigenangaben einen gewissen Betrag an andere Freie-Software- und Open-Source-Projekte, die in Parted Magic enthalten sind. Die Spenden betrugen 2013 3.000 USD und 2.500 USD im Jahr 2014.\n\n\n"}
{"id": "4101431", "url": "https://de.wikipedia.org/wiki?curid=4101431", "title": "IBM PS/1", "text": "IBM PS/1\n\nMit dem IBM PS/1 Personal Computer unternahm IBM im Jahr 1990, fünf Jahre nach dem IBM PCjr einen erneuten Versuch, sich im Heimcomputer-Markt zu etablieren. Im September 1994 wurde der PS/1 durch den IBM Aptiva ersetzt. Alle PS/1- und Aptiva-Modelle hatten ein eingebautes Modem zum Zugriff auf Online-Hilfe und vorinstallierte Software.\n\nDie Bezeichnung „PS/1“ sollte eine gegenüber der professionellen Produktlinie PS/2 eingeschränkt leistungsfähige Maschine andeuten. Im Gegensatz zum PCjr hatte der PS/1 jedoch einen höheren Grad von Kompatibilität mit den PS/2-Systemen.\nDie PS/1-Familie war für Einsteiger gedacht und wurde in ganz normalen Elektronik-Geschäften zusammen mit vergleichbaren Angeboten von Compaq, Hewlett-Packard, Packard Bell und anderen verkauft, ein Abrücken von der bisher üblichen Strategie, dass IBM-Produkte nur bei IBM selbst zu kaufen waren.\nAlle PS/1-Modelle enthielten ein Modem, so dass die Benutzer online bei IBM Hilfe anfordern konnten.\nEine Besonderheit an allen PS/1-Modellen war es, dass man den ursprünglichen Zustand des BIOS wiederherstellen konnte, wenn man beim Einschalten des Rechners beide Mausknöpfe gedrückt hielt und diese erst nach Beendigung des Power on Self-test losließ. Bei den Modellen 2011 und 2121 führte dieses Verfahren auch dazu, dass das PC-DOS von einem eingebauten ROM gebootet wurde anstatt von der Festplatte. Auf diese Weise hatten die PS/1-Systeme einen elementaren Schutz vor Viren, da sich immer der Zustand der Werksauslieferung wiederherstellen ließ.\n\nIm Lauf der Produktion der PS/1-Systeme gab es unterschiedliche Gehäuse:\n\nDie Gehäuse 2133 und 2155 wurden in verschiedenen Modelljahren eingesetzt; das Tower-Gerät 2168 erschien erst später.\n\nDer ursprüngliche PS/1 enthielt eine 10 MHz Intel 80286 CPU und wurde mit dem Ziel entwickelt, leicht einzurichten und leicht zu benutzen zu sein. IBM traf die eher ungewöhnliche Entscheidung, die Hauptplatine und weitere Elektronik im Monitor unterzubringen. Die frühen Modelle 2011 und 2121 starteten das DOS aus einem ROM und zeigten zu Beginn einen viergeteilten graphischen Bildschirm, der direkten Zugang zu Hilfe-Informationen, Microsoft Works, eigener Software und dem DOS-Kommandoprozessor bot. Sie hatten 1 MB Hauptspeicher, ein eingebautes 2400-Baud-Modem, eine optionale 30 MB-Festplatte und eine optionale Soundkarte.\nIBM vertrieb außerdem ein 5.25\"-Diskettenlaufwerk und eine \"Adapter Card Unit (ACU)\", um Erweiterungskarten von Drittanbietern aufzunehmen. Das Modell 2121 verwendete das gleiche Gehäuse wie die 2011, aber hatte einen ISA-Erweiterungssteckplatz im Inneren des Gehäuses. Der Hauptspeicher konnte mit einem proprietären 4-MB-Speichermodul von 2 MB auf 6 MB erweitert werden.\n\nDie ersten PS/1-Modelle (2011, 2121, 2123) boten zu wenig Erweiterungsmöglichkeiten, vor allem wegen fehlender ISA-Erweiterungssteckplätze.\n\nEin Nachfolgemodell hatte später einen Intel 80386SX Prozessor mit 16 MHz, eine Version mit dem Intel 80486SX lief mit 20 MHz.\n\nDie Modelle 2133 können in die folgenden Hardware-Kategorien eingeteilt werden:\n\nIm Mai 1993 führte IBM eine „neue Generation“ der PS/1-Produktlinie ein. Diese Systeme enthielten eine normale LPX Hauptplatine. Merkwürdigerweise wurden viele dieser späteren PS/1-Systeme vorinstalliert mit MS-DOS und Microsoft Windows ausgeliefert anstatt mit IBM’s PC-DOS oder OS/2. Dies lag daran, dass IBM den Markt für OS/2 in teureren Maschinen mit mehr Rechenleistung sah. Allerdings hatte eins der ersten 2133-Modelle ein vorinstalliertes OS/2 2.1.\n\nDie Produktlinie PS/1 wurde 1994 eingestellt und durch die Aptiva-Linie ersetzt, die von der Architektur her den letzten PS/1-Modellen ähnelte, aber einen marktgängigeren Namen hatte. Aptivas wurden in den Vereinigten Staaten bis zum Frühjahr 2000 verkauft; damals machte der Preisdruck die Produktlinie unprofitabel und IBM zog sich aus dem PC-Markt für Endverbraucher völlig zurück.\n\n\n"}
{"id": "4106433", "url": "https://de.wikipedia.org/wiki?curid=4106433", "title": "Celtx", "text": "Celtx\n\nCeltx ist eine kommerzielle Software für die Vorproduktion von Medienprojekten, wie Filme, Videos, Hörspiele, Theaterstücke, Dokumentationen und Spiele. Es bietet umfangreiche Funktionen zum Kreieren, Planen und Organisieren eines Projekts.\n\nDie Basisfunktion von Celtx ist das Editieren von Drehbüchern, die dem internationalen Industriestandard entsprechen. Ein Client-Server-System erlaubt die Zusammenarbeit innerhalb von Projekten über das Internet und fördert die integrierte und non-lineare Umsetzung.\n\nCeltx ist auf freien, nicht-proprietären Standards (z. B.: HTML, XML, RDF und TXT) aufgebaut und ist unter der Celtx Public License lizenziert. Die Entwicklung und Übersetzung der Software wird durch freiwillige Mitglieder der internationalen Celtx-Community bewerkstelligt. Es liegen Versionen für Windows, Mac OS X und Linux vor.\n\nCeltx ist das englische Akronym für Crew, Equipment, Location, Talent und XML. Die Aussprache lautet „keltix“.\n\nIm Gegensatz zu anderer Drehbuchsoftware (Final Draft etc.) steht bei Celtx das All-In-One-Konzept im Vordergrund. Die komplette Vorbereitungsphase eines Projekts kann mit nur einem Programm realisiert werden und erleichtert somit die Umsetzung. Das Erstellen eines Drehbuchs, Notizen, Charakteren, Storyboard-Sequenzen, Aufgabenverteilung und detaillierte Berichte für Cast und Crew können in einer Software verwirklicht werden.\nDa Projekte auf einem allgemein zugänglichen Server gespeichert werden können, ist das gemeinsame Arbeiten mehrerer Teammitglieder möglich. Dateien können so organisiert werden, dass sie nur für bestimmte Mitglieder zugänglich sind oder ausgetauscht und veröffentlicht werden können. Auf diese Weise entfällt das häufige Ausdrucken von Drehbüchern, und das Erarbeiten eines Projekts kann unter Umständen deutlich schneller geschehen.\n\nCeltx enthält verschiedene Editoren, die den jeweiligen Industriestandards entsprechen.\n\nUmfangreiche Formatierungsfunktionen, automatisches Vervollständigen von Text und eine integrierte Rechtschreibprüfung erleichtern das Schreiben. Zusätzliche Formatierungsmöglichkeiten sind Seitenumbrüche und Dual-Dialog. Textzeilen oder einzelne Elemente können farbig markiert und mit anderen Funktionen von Celtx verknüpft werden. Im weiteren Planungsverlauf ist es dadurch möglich, Berichte zu erstellen auf denen beispielsweise die einzelnen Charaktere oder Requisiten aufgelistet werden.\n\nCeltx-Skripten können entweder direkt ausgedruckt oder aber auch in HTML bzw. PDF exportiert werden.\n\nDie Anwendung benutzt das umfangreiche LaTeX-Typesetting, um die Skripten ohne Abweichung in die anerkannten Industriestandards zu formatieren. Dies geschieht auf dem Celtx-Server, d. h., die Datei muss zuerst auf diesen übertragen werden. Auch die PDF-Funktion kann nur auf diese Weise erfolgen. Szenen- bzw. Dialogfortsetzungen (Continueds und Mores) und Seitennummerierungen werden automatisch generiert und sind optional. Um die Sicherheit zwischen der Desktopanwendung und dem Celtx-Server zu gewährleisten, wird eine Verschlüsselungstechnologie genutzt.\n\nDie Funktion Karteikarten erlaubt einen besseren Überblick bei besonders großen Projekten. Diese werden ebenfalls automatisch zu jeder neuen Szene erstellt, können eingefärbt und beschrieben werden. Außerdem kann man die Reihenfolge der einzelnen Szenen per Drag and Drop verändern.\n\nStoryboards können in das Medienprojekt eingefügt und mit Beschreibungen versehen werden. Diese Funktion stellt ein weiterer Bestandteil des Planungsverlaufs dar und hilft das Projekt vor der Produktion zu visualisieren. Mit der Slideshow können die Bilder abgespielt werden.\n\nCeltx enthält des Weiteren die Funktion um digitale Fotos, gescannte Dokumente, Videoclips und Audiodateien in das Projekt einzubinden. Das Celtx-Format ist so angelegt, dass sich alle Dateien in einer Art Container befinden (genau genommen in einem ZIP Archiv). Somit besteht ein Medienprojekt aus nur einer Datei.\n\nEine neue Art der Organisation in Celtx sind Kataloge, auf denen detaillierte Informationen des Projekts aufgelistet und ausgegeben werden. Jede neue Celtx-Datei enthält einen Hauptkatalog, welcher die wichtigsten Elemente der erstellten Geschichte bereithält. Zusätzlich können Nebenkataloge erstellt werden, die spezifische Produktionsinformationen von verschiedenen Kategorien enthalten.\n\nCeltx enthält eine Ablaufplanung in Form eines Terminkalenders. In diesen werden in Verbindung mit dem Skript Informationen für die Realisierungstermine eingetragen. Die enge Verknüpfung mit dem Projekt bietet den Vorteil, dass spezifische Kalenderblätter für die einzelnen Teammitglieder leichter fällt. Die Kalenderblätter können ausgedruckt werden, sind aber durch den Celtx-Client-Server auch für andere Teammitglieder einsehbar.\n\nDer Celtx-Client-Server wird oftmals auch als Backup-System verwendet. Um ein unbeabsichtigtes Überschreiben zu verhindern oder um auf ältere Versionen zurückzugreifen, können Snapshots erstellt werden, die ebenfalls auf dem Server gespeichert bleiben.\n\nErst durch das Anlegen eines Accounts sind alle Funktionen von Celtx freigegeben. Dieses Konto ist passwortgeschützt und durch die Verschlüsselung ist es nicht möglich, ein Projekt von Außenstehenden einzusehen, es sei denn, es ist so gewollt. Der Uploader kann sein Projekt mit anderen Mitgliedern teilen und eine barrierefreie Zusammenarbeit ist möglich. Nachdem dem Erscheinen der Version 1.0 wurde das Account-System in Celtx-Studio umbenannt und nach der Umstellung auch kostenpflichtig. Die Mitgliedschaft kostet derzeit (2010) $4,99 im Monat oder $49 im Jahr und kann jeweils von fünf Personen gleichzeitig benutzt werden. Das Herunterladen der Software bleibt weiterhin kostenlos.\n\nDie Software arbeitet offline nur eingeschränkt. So erstellt das Programm beispielsweise PDF-Dateien nur mit Online-Zugang zu den Servern des Herstellers, es sei denn, man nutzt die Druckfunktion in Verbindung mit unabhängiger Zusatzsoftware wie Ghostscript (und dessen GUIs).\nÜber den Hersteller selbst, die Grayfirst Corporation, finden sich im Internet kaum Informationen.\nAlle Bezahlvorgänge einschließlich der Kreditkartendatenübermittlung laufen über eigene Server des Herstellers.\nEine sehr knappe Privacy Policy von Celtx ist im Internet veröffentlicht. Die Grayfirst Corporation, deren Firmierung unter gleicher Anschrift in Kanada wie Celtx ist, hat keine veröffentlicht.\n\nDie Website weist die Desktop-Software unter der Mozilla Public License aus. Der Desktop-Software selbst liegt keine Lizenz und keine Copyright-Angabe bei.\n\n"}
{"id": "4109680", "url": "https://de.wikipedia.org/wiki?curid=4109680", "title": "Sycat", "text": "Sycat\n\nsycat (Systematische CIM Analyse Tools) ist eine Prozessmanagementsoftware zur Unternehmensgestaltung und -entwicklung. Sie bedient sich der swimlane-Darstellung, oftmals auch als Organisationsprozessdarstellung (OPD) bezeichnet, für die Aufbau- und Ablauforganisation des Unternehmens.\nProzessmanagement, speziell zur Visualisierung und Beschreibung von Prozessen (Geschäftsprozessmodellierung) im Rahmen der Einführung eines Qualitätsmanagementsystems nach DIN EN ISO 9001. Weltweit sind 16.500 sycat-Lizenzen in 1.650 Unternehmen im Einsatz.\n\nDie von Hartmut F. Binner entwickelte Software wurde kontinuierlich durch verschiedene Komponenten ergänzt. Viele der Komponenten und Funktionen wurden direkt aus dem Beratungsgeschäft heraus als notwendig erachtet und in das Tool integriert.\n\nBereits 1988 wurde der Prototyp des Tools sycat anlässlich der CeBIT als erste Prozessmodellierungssoftware am Markt vorgestellt. 1999 erlangte das Modul sycat-mobile den vom Technologie-Centrum Hannover ausgeschriebenen Technologiepreis für den Einsatz innovativer Multimedia-Technologien und -Anwendungen. Unter der Schirmherrschaft des niedersächsischen Wirtschaftsministers Walter Hirche und des Wirtschaftsdezernenten der Region Hannover Hans-Georg Martensen bekam die in sycat integrierte Workflow-Lösung den Innovationspreis 2003 für herausragende Leistungen auf dem Gebiet der Informationstechnologie. Aus dem im Jahr 1994 von Binner gegründeten Unternehmen Dr. Binner CIM-House GmbH entwickelten sich 2006 das Bildungsinstitut Prof. Binner Akademie und im Jahr 2007 die binner IMS GmbH. Das Nachfolgeunternehmen binner IMS ist seit dem 1. Februar 2007 Ansprechpartner für die Software sycat und zugeordnete Dienstleistungen. Der Gründer Hartmut F. Binner ist inzwischen nicht mehr in der binner IMS GmbH tätig.\n\nAm 1. März 2012 wurde die binner IMS GmbH in die sycat IMS GmbH umbenannt, welche die Software sycat entwickelt und verkauft. Das seit Firmengründung 1994 aktive Geschäftsfeld Consulting wurde im Zuge der Umfirmierung aus dem Unternehmen herausgenommen und ist nun Gegenstand des neu gegründeten Unternehmens proWert GmbH.\n\nDas Prozessmanagementwerkzeug sycat ist eine modular aufgebaute Standard Software für BPM. Es ermöglicht die\nvon Geschäfts- und Betriebsprozessen. \n\nGeschäftsprozesse beziehen sich auf die dispositiven, planenden und steuernden Abläufe im Unternehmen. Bei den Betriebsprozessen werden die operativen Abläufe, das heißt die wertschöpfenden Aktivitäten beschrieben. Sowohl Geschäfts- als auch Betriebsprozesse können in unterschiedlicher Darstellungsweise grafisch abgebildet werden. Mit den visualisierten Prozessen wird die Grundlage für einen prozessorientierten Gestaltungsansatz der Kernprozesse gebildet. Die Arbeitsabläufe lassen sich, unabhängig von Branche und Größe eines Unternehmens, durch Betrachtungs- und Dokumentationsvarianten abbilden.\n\nsycat ist in einer Modulstruktur aufgebaut und umfasst die Bereiche: \n\nSämtliche sycat-Module greifen auf die gleiche Datenbank zu, sodass einmal aufgenommene Daten durch die einzelnen Module weiter bearbeitet werden können. Dadurch lassen sich Prozesse aus den verschiedenen Managementsichten betrachten.\n\n\n\n\n"}
{"id": "4110856", "url": "https://de.wikipedia.org/wiki?curid=4110856", "title": "7-Zip", "text": "7-Zip\n\n7-Zip ist ein freies Packprogramm, das unter den Bedingungen der LGPL lizenziert ist. Die Entwicklung erfolgt durch den russischen Softwareentwickler Igor Wiktorowitsch Pavlov, der die erste Version von 7-Zip im Jahr 1999 veröffentlichte und bis heute aktiv entwickelt (Stand Frühjahr 2018). 7-Zip stellt die Referenzimplementierung des von ihm entwickelten Lempel-Ziv-Markow-Algorithmus (LZMA) dar.\n7-Zip wurde im Juli 2007 von SourceForge als bestes Open-Source-Projekt ausgezeichnet.\n\nStandardmäßig erzeugt \"7-Zip\" Archive im 7z-Format mit der Dateinamenserweiterung codice_1. Jedes Archiv kann viele Verzeichnisse und Dateien enthalten. Bei 7z handelt es sich grundsätzlich um ein reines Containerformat, das offen und modular aufgebaut ist. So werden Sicherheits- und Datenkompressions-Funktionen durch eine Reihe von hintereinander geschalteten Filtern realisiert. Diese implementieren dann zum Beispiel Präprozessoren, Kompressionsalgorithmen oder Verschlüsselungs-Filter.\n\nAlle Dateinamen werden in Unicode-Kodierung abgelegt.\n\n7-Zip unterstützt außerdem eine Reihe von komprimierten und nicht-komprimierten Archiv-Formaten:\n\nFolgende Formate können sowohl ge- als auch entpackt werden:\n\nAuf zip bzw. gzip basierende Formate:\n\nZur Kompression von zip- und gzip-Dateien verwendet 7-Zip einen selbst entwickelten Deflate-Encoder, der oft bessere Ergebnisse liefert als die weitaus häufiger verwendete Deflate-Implementation von zlib, was allerdings zu Lasten der Kompressionsgeschwindigkeit geht. Diese Version des Deflate-Encoders ist auch unabhängig von 7-Zip als Teil der AdvanceCOMP-Suite erhältlich.\n\nDarüber hinaus lassen sich Archivinhalte beziehungsweise gepackte Daten aus Dateien der folgenden Formate extrahieren:\n\n\n\n7-Zip kann manche Windows-Installer-Dateien (codice_22) öffnen und bietet Zugang zu deren gesamten Inhalt, inklusive der Meta-Dateien. Ebenso lassen sich mit LZX komprimierte Microsoft-Cabinet-Dateien (codice_21) und mit LZMA komprimierte NSIS-Installationsprogramme öffnen. Gleiches gilt für manch andere ausführbare Programme (Endung z. B. codice_23). Daher kann \"7-Zip\" auch dazu genutzt werden, zu überprüfen, ob es sich bei einer unbekannten Binärdatei um ein Archiv handelt, und dessen Inhalt gegebenenfalls zu extrahieren.\n\n\n\n\n\nDie native 7z-Kompression bedient sich einer Vielzahl von Algorithmen.\nNach einer Sortierung der zu packenden Dateien in eine günstige Reihenfolge und (bei progressiver Komprimierung) eventueller Verkettung zu einem fortlaufenden Datenstrom können bestimmte Dateitypen mit verschiedenen Vorfiltern bzw. Präprozessoren aufbereitet werden, um bei der anschließenden Kompression bessere Ergebnisse zu erzielen. Hierzu werden für ausführbare Dateien in einer Reihe unterstützter Formate entsprechende Filter wie BCJ und BCJ2 für Windows-Programmdateien mit x86-Maschinencode verwendet. Zur Dekorrelation von zum Beispiel PCM- und Rastergraphikdaten steht ein Delta-Filter zur Verfügung.\n\nFür den folgenden Hauptkompressionsschritt werden beim nativen 7z-Format die Verfahren LZMA, PPMd und bzip2 unterstützt.\n\nDann kann ein Verschlüsselungsschritt mit AES nachgeschaltet werden.\n\nMit dem in Voreinstellung verwendeten LZMA und Vorfiltern werden überwiegend bessere Kompressionsraten als mit anderen verbreiteten Formaten erzielt. Besonders bei großen Datenmengen übertrifft es das verbreitete RAR oft deutlich.\n\nLZMA ist ein relativ neues, von Igor Pavlov selbst entwickeltes und mit 7-Zip eingeführtes Verfahren. Es nutzt eine verbesserte Variante des LZ77-Algorithmus, Markow-Ketten und einen Bereichskodierer (eine Umsetzung arithmetischen Kodierens) zur Entropiekodierung.\n\n\"7-Zip\" unterstützt die als AES standardisierte 256-Bit-Rijndael-Verschlüsselung. Die Verschlüsselung kann getrennt sowohl für die gepackten Dateien als auch für die 7z-Verzeichnisstruktur aktiviert werden. Falls die Verzeichnisstruktur verschlüsselt ist, muss schon für die Anzeige der Namen der im Archiv enthaltenen Dateien das richtige Passwort angegeben werden.\n\nDer WinZip-AES-Verschlüsselungsstandard wird ebenso unterstützt, so dass ZIP-Archive mit 256 Bit AES verschlüsselt werden können. Dabei ist jedoch keine Verschlüsselung der Verzeichnisstruktur wie bei 7z-Archiven möglich.\n\n\"7-Zip\" bietet viele Funktionen, unter anderem:\n\nDie grafische Benutzeroberfläche fügt sich via „Drag and Drop“ und Kontextmenüs nahtlos in Windows ein. Eine Kommandozeilenversion und ein Plugin für den FAR Manager werden mitgeliefert.\n\nBeim Auspacken von Archivdateien überträgt 7-Zip (anders als der Windows-Explorer sowie WinZip und WinRAR) einen vorhandenen Zone.Identifier nicht auf die extrahierten Dateien.\n\n7-Zip wird für Windows entwickelt, es existiert jedoch mit p7zip auch eine portable Kommandozeilenversion des Programms, welche plattformübergreifend für die BSDs, Linux, Unix, OS/2, BeOS und AmigaOS 4 kompiliert werden kann.\n\nEs gibt viele Frontends für \"p7zip\", die z. B. eine Einbindung in grafische Benutzeroberflächen ermöglichen.\n\nWeiterhin existiert mit den XZ Utils eine Sammlung von auf dem LZMA-Code des LZMA SDK basierenden Werkzeugen zur reinen Datenkompression (also ohne Archivierungs-Funktion), mit denen sich der LZMA analog zu Programmen wie gzip, bzip2 oder der zlib (zum Beispiel auch in Kombination mit tar) nutzen lässt.\n\n7zX ist eine Implementierung für macOS, welche aber mit der Version für Windows nur das Kompressionsformat gemeinsam hat. Die Anwendung selbst baut auf einer separaten Codebasis auf.\n\nEs existieren Java-, .NET- und Python-Bindings.\n\nDas Installationsprogramm von 7-Zip sowie alle mit 7-Zip erstellten selbst-entpackenden Archive hatten bis zum Erscheinen der Programmversion 16.00 Sicherheitslücken.\n\nDie Sicherheitslücken, die u. a. das Einschleusen und Ausführen von Schadcode in archivierten UDF-Dateien ermöglichten, wurden in Version 16.00 geschlossen.\n\n\n"}
{"id": "4112289", "url": "https://de.wikipedia.org/wiki?curid=4112289", "title": "Windows Automated Installation Kit", "text": "Windows Automated Installation Kit\n\nMit dem Windows Automated Installation Kit (Windows AIK oder WAIK) bietet Microsoft eine Möglichkeit, Windows automatisch installieren zu lassen. Angewendet wird diese Technik seit Windows Vista (November 2006). Die automatische Installation überspringt sämtliche Anfragen, die üblicherweise während einer Betriebssysteminstallation auftreten. Der Lizenzschlüssel, Computername und weitere Einstellungen können einmalig festgelegt und dann auf mehreren Computern installiert werden.\n\nDer Windows System Image Manager (Windows SIM), mit dem sich codice_1-Dateien für den unbeaufsichtigten Installationsvorgang erstellen lassen, ist im Windows Automated Installation Kit enthalten.\n\nWAIK wurde durch das Windows Assessment and Deployment Kit (ADK) ersetzt und unterstützt nun auch Windows 8.\n\n\n"}
{"id": "4115972", "url": "https://de.wikipedia.org/wiki?curid=4115972", "title": "Emerge Desktop", "text": "Emerge Desktop\n\nEmerge Desktop ist eine alternative Desktop-Umgebung für Windows 2000, Windows XP, Windows Vista und Windows Server 2003. Es ist in C++ geschrieben, vorwiegend mit dem MinGW-Compiler entwickelt und unter der GPL3 veröffentlicht.\n\nEmerge Desktop besteht aus Modulen, die „Applets“ genannt werden. Applets für Emerge Desktop sind eigenständige Programme, die jedoch untereinander Daten austauschen können. Zu Emerge Desktop gehören diese Applets:\n\nJedes Applet ist dazu gedacht Funktionen der Standard-Windowsoberfläche (des Windows Explorers) zu ersetzen bzw. zu erweitern und bietet verschiedene Einstellungen und Anzeigemöglichkeiten an. \nDas Applet emergeCore ermöglicht allen Emerge Desktop Applets untereinander Daten auszutauschen. Ohne emergeCore laufen die Applets als gewöhnliche Anwendungsprogramme. So können sie z. B. auch auf anderen Windowsoberflächen benutzt werden.\n\nMit emergeTray erhält der Benutzer ein Benachrichtigungsfeld, das alle Benachrichtigungssymbole anzeigt, die normalerweise in der Taskbar des Windows Explorers unten rechts neben der Uhrzeit erscheinen.\n\nVirtuelle Arbeitsbereiche sind unter Unix-artigen Systemen schon lange bekannt. Es gibt auch einige Virtual Window Manager (VWM) für Microsoft Windows; und Emerge Desktop bringt mit diesem Applet seinen eigenen VWM mit.\n\n"}
{"id": "4117872", "url": "https://de.wikipedia.org/wiki?curid=4117872", "title": "Go-oo", "text": "Go-oo\n\nGo-oo (Go OpenOffice) war ein auf dem plattformunabhängigen OpenOffice.org basierendes Office-Softwarepaket von Novell. \n\nOb es sich aber bei Go-oo um eine richtige Abspaltung handelte, ist nicht feststellbar. Die Ziele waren, einen besseren Support von Microsoft-Formaten bereitzustellen, deren Excel-VBA-Makros zu unterstützen und eine bessere Interoperabilität. Darüber hinaus wurden ostasiatische Sprachen besser unterstützt.\n\nGo-oo wird seit September 2010 nicht mehr weiterentwickelt, da die Entwickler sich der Document Foundation und ihrem Projekt LibreOffice angeschlossen haben.\n\nAuf Go-oo basierend entstand auch das Programmpaket OxygenOffice.\n\n\nFür die verschiedenen Betriebssysteme sind jeweils unterschiedliche Versionen verfügbar. In der Regel erscheinen neue Versionen einige Tage, nachdem OpenOffice.org eine neue Version veröffentlicht.\n\n\n"}
{"id": "4121429", "url": "https://de.wikipedia.org/wiki?curid=4121429", "title": "PSPP (Software)", "text": "PSPP (Software)\n\nPSPP ist eine Software zur Analyse von statistischen Daten. Das Programm ist als freier und völlig kompatibler Ersatz für das proprietäre Programm SPSS konzipiert, deckt allerdings bisher nur einen begrenzten Umfang von dessen Funktionen ab. Es verfügt über eine grafische Benutzeroberfläche sowie die Möglichkeit der Steuerung über eine Kommandozeile.\n\nPSPP kann Excel-, Gnumeric-, OpenDocument-Spreadsheet-, CSV-, ASCII- und Postgres-Daten importieren und in das SPSS-Portable-, das SPSS-System- und das ASCII-Format exportieren. Mit PSPP lassen sich t-Tests, Varianzanalysen, lineare Regressionen und eine Vielzahl anderer statistischer Operationen durchführen. Das Programm verfügt über vielfältige Möglichkeiten, um Daten zu rekodieren, umzuordnen und zu verändern.\n\nDas PSPP-Projekt wurde initiiert, um Benutzern die Möglichkeit zu geben, die Einschränkungen von SPSS zu vermeiden. Unter den Nutzungsbedingungen von SPSS ist es Kunden verboten, das Programm zu kopieren oder zu verändern. SPSS kann gekauft oder gemietet werden. Es steht auch eine kostenlose Vollversion von SPSS, dem Original, zum Download bereit. Diese Testversion ist nach der Installation 14 Tage lauffähig. In neueren Versionen von SPSS ist ein digitales Rechtemanagement eingebaut, das dazu dient, die Nutzung der Software nach Ablauf der Mietfrist zu verhindern. Der Nutzer der Mietversion muss nach einem Jahr die Lizenz erneuern.\nDie Kauflizenz ist uneingeschränkt und unbefristet nutzbar.\nEine Einschränkung von Laufzeit und Variablenanzahl besteht nur bei der über den Buchhandel erwerbbaren Studentenlizenz von SPSS. Diese wird aber nicht vom Hersteller von SPSS selbst vertrieben, sondern von einem eigenständigen Unternehmen.\n\nDer Autor von PSPP entschloss sich daher, eine freie, zu SPSS vollständig kompatible Alternative zu schaffen, welche weder einen Ablauf der Lizenz noch eine künstliche Einschränkung der Variablenzahl kennt und bei der es jedem gestattet ist, sie zu kopieren, zu modifizieren und zu tauschen.\n\nDer Name PSPP ist eine Anspielung auf das proprietäre Pendant SPSS, besitzt im Gegensatz zu diesem aber keine offizielle Bedeutung. Stattdessen unterbreitet der Autor mehrere Vorschläge wie Perfect Statistics Professionally Presented oder People Should Prefer PSPP.\n\nPSPP ist in C geschrieben und benutzt die GNU Scientific Library für seine mathematischen Routinen und plotutils, um Diagramme zu generieren.\n\nIm Buch „SPSS für Dummies“ erwähnt der Autor PSPP unter der Überschrift „Zehn nützliche Dinge, die man im Internet finden kann“.\n\n"}
{"id": "4122409", "url": "https://de.wikipedia.org/wiki?curid=4122409", "title": "BestCrypt", "text": "BestCrypt\n\nBestCrypt ist eine kommerzielle Software zur partitionsweisen Verschlüsselung von Festplatten.\n\n\n"}
{"id": "4124363", "url": "https://de.wikipedia.org/wiki?curid=4124363", "title": "BSI CRM", "text": "BSI CRM\n\nBSI CRM ist eine Customer-Relationship-Management Software. Sie enthält Module für Sales, Marketing, Service, Contact Center und Reporting. BSI CRM ist sowohl On Premise als auch On Demand verfügbar und läuft auf den üblichen (Java-)Applikationsservern und Datenbanken. BSI CRM gilt als die meistverbreitete Schweizer CRM-Lösung und wird beispielsweise von Unternehmen wie der Schweizerischen Post, ABB, M-net oder UBS eingesetzt.\n\nBSI CRM bietet ein Contact Center Modul an. So können eingehende Anrufe über die Daten im CRM identifiziert werden und etwaige Prozesse mit den jeweiligen Kunden im CRM verknüpft werden. Diese Daten stehen dann wiederum für Marketing-Zwecke zur Verfügung.\n\nEine Besonderheit bezüglich Datenschutz wurde für einen deutschen Kunden eingebaut (Lidl): Personenbezogene Daten werden kaum erhoben; werden sie doch benötigt, werden diese nach einer gewissen Zeit wieder gelöscht.\n\nDie erste Version von BSI CRM kam 1996 auf dem Markt, damals auf Basis von SQLWindows. Im Jahre 2000 wurde die Software in Java neu programmiert und im 2008 auf SOA und Eclipse RCP umgestellt.\n2009 wurde das Kampagnen Management stark erweitert (SMS-Anbindung und E-Mail-Integration, Budgetkontrolle, Messung der Zielerreichung). \n2010 wurde das Framework, auf dem BSI CRM aufbaut, unter dem Namen Eclipse Scout unter der Eclipse Public License veröffentlicht.\nMit Eclipse Neon wurde 2015 das Eclipse Scout Framework auf ein HTML \"web frontent\" geändert\nund entsprechend ist BSI CRM Indigo nun auch keine Swing Applikation mehr, sondern verwendet ein HTML User Interface.\n"}
{"id": "4126821", "url": "https://de.wikipedia.org/wiki?curid=4126821", "title": "Indirect Hard Modeling", "text": "Indirect Hard Modeling\n\nIndirect Hard Modeling (IHM) ist ein nichtlineares multivariates Analyseverfahren zur quantitativen Untersuchung von Spektren bzw. allgemein von Peak-förmigen Daten, zum Beispiel auch Chromatogrammen.\n\nIHM kann eingesetzt werden, um aus den Messdaten einer Probe eine gesuchte Messgröße zu bestimmen, i. d. R. die Konzentration der Bestandteile der Probe. Im Gegensatz zu den klassischen datengetriebenen linearen Verfahren verwendet IHM ein deterministisches nichtlineares Modell, welches einer rigorosen mathematischen Formulierung der physikalischen Zusammenhänge zwischen Messgröße und Messdaten entspricht. Es besteht aus zwei Teilmodellen. Das Modell des Spektrums (oder Chromatogramms) ergibt sich dabei als gewichtete Summe von so genannten \"Hard Models\" der Spektren der einzelnen Bestandteile der Probe. Jedes Hard Model entspricht wiederum einer gewichteten Summe von parametrisierten Peak-Funktionen. Das zweite Teilmodell ist ein funktionaler Zusammenhang zwischen den im ersten Teilmodell ermittelten Gewichten und der gesuchten Messgröße, die oft physikalisch begründet als linear angenommen werden kann. Der Namen der Methode erklärt sich daraus, dass das Messsignal eines Gemisches nicht direkt durch Anpassen von Banden modelliert wird, sondern ausgehend von den Hard Models der Bestandteile.\n\nDas IHM Verfahren wurde an der RWTH Aachen entwickelt und erstmals 2004 publiziert. Erweiterungen zur Identifikation unbekannter Teilmodelle (Complemental Hard Modeling, CHM, sowie Hard Modeling Factor Analysis, HMFA, welches Ideen aus der Hauptkomponentenanalyse aufgreift) erfolgten 2008.\n\nBis 2010 wurden IHM und verwandte Hard Modeling Methoden an der RWTH in einer Software names PEAXACT weiterentwickelt. Ende 2011 lizenzierte die RWTH die PEAXACT-Verwertungsrechte an das Aachener Unternehmen S-PACT GmbH, das seitdem die Entwicklungen weiterverfolgt.\n\nAufgrund der Verwendung eines deterministischen Modells ist die Anwendung von IHM nicht universell für multivariate statistische Fragestellungen geeignet, sondern speziell auf die Analyse von Peak-förmigen Daten ausgerichtet. Ihr Einsatzgebiet liegt somit innerhalb der instrumentellen analytischen Chemie, vor allem der Spektroskopie und der Chromatographie.\n\nDerzeitig existieren wissenschaftliche Veröffentlichungen, welche die Anwendung von IHM auf Infrarot- und Raman-Spektren behandeln.\n\n\n"}
{"id": "4132939", "url": "https://de.wikipedia.org/wiki?curid=4132939", "title": "Bolt – Ein Hund für alle Fälle", "text": "Bolt – Ein Hund für alle Fälle\n\nBolt – Ein Hund für alle Fälle (Originaltitel: \"Bolt\") ist ein US-amerikanischer Computeranimationsfilm und der 48. abendfüllende Trickfilm der Walt Disney Animation Studios. Der Film lief am 21. November 2008 in den Vereinigten Staaten und Kanada in den Kinos an. In Deutschland startete der Film am 22. Januar 2009. Der Film wurde zusätzlich in einer 3D-Version (Disney Digital 3D, Shuttertechnik) sowie im Polarisationsverfahren für alle unterstützenden Kinos vertrieben.\n\nBolt ist der Hauptdarsteller einer gleichnamigen, populären Fernsehserie. In dieser Serie ist er ein Superhund mit unglaublichen Kräften, mit denen er regelmäßig Penny, ein 13-jähriges Mädchen, das sowohl in der Serie als auch im wahren Leben Bolts liebende Besitzerin ist, rettet. Gegenspieler sind Dr. Calico, der „grünäugige Mann“, und dessen Organisation. Bolt spielt schon sein ganzes Leben seine Rolle in der Serie, sodass er glaubt, die Filmsets, Abenteuer und seine Kräfte seien echt. Um die Einschaltquote der Serie zu verbessern, lässt man Penny am Ende der Staffel entführen – Bolt hält dies natürlich auch für die Realität.\n\nDurch einen unglücklichen Zufall entkommt Bolt kurz darauf vom Set, fällt in ein Paket und wird in diesem von Hollywood quer durch die USA nach New York City verschickt. Dort stellt er fest, dass seine ganzen Kräfte nicht mehr funktionieren, schiebt dies aber auf Sabotage. In der Stadt trifft er auf Mittens, eine Straßenkatze, die er für eine Komplizin der bösen Organisation hält, da in der Serie seine Gegenspieler stets von Katzen umgeben sind. Er nimmt sie gefangen und macht sich auf die Reise zurück, um seine Besitzerin zu retten. Unterwegs erkennt Mittens, dass das seltsame Verhalten Bolts auf dessen Irrglauben beruht und versucht ihn aufzuklären, was zunächst scheitert. Zusammen treffen sie auf Dino (im Original Rhino), einen fernsehbesessenen Hamster, der sein Leben in einem Joggingball verbringt und Bolts Serie vergöttert; er hält sie ebenfalls für real. Nach einigen turbulenten Abenteuern sieht Bolt ein, dass sein vorheriges Leben nur Illusion war, und beginnt mit Mittens Hilfe, die Freuden eines normalen Hundedaseins kennenzulernen und zu genießen – nur seine Freundschaft mit Penny bleibt für ihn real.\n\nDarum setzt er seine Suche fort und landet wieder in Hollywood, wo er feststellt, dass er durch ein Double ersetzt wurde. Kurz vor dem Aufgeben überzeugen seine neu gewonnenen Freunde ihn davon, dass Penny ihn vermisst und er nur die Aufnahme einer Filmszene mit dem Double gesehen hat. Durch einen Unfall mit dem unerfahrenen Ersatzhund gerät das Set in Flammen und Penny ist im Inferno gefangen. Bolt schafft es unter Einsatz seines Lebens, sie zu retten. Bolt und seine Familie kündigen daraufhin den Job im Filmgeschäft. Sie ziehen zusammen mit den neu gewonnen Freunden Mittens und Dino an den Rand der Stadt und verbringen ein schönes Leben miteinander.\n\n\nZum Kinostart gab der Deutsche Tierschutzbund eine Pressemitteilung mit dem Titel „Disney-Film zeigt gefährliches Hamster-Spielzeug“ heraus, aus der hervorgeht, dass Hamsterbälle, wie sie der Hamster „Dino“ verwendet, kein tiergerechtes Spielzeug darstellen und tierschutzwidrig sind. Auch die Tierärztliche Vereinigung für Tierschutz bezeichnet Hamsterbälle als tierschutzwidrig und der Zentralverband Zoologischer Fachverbände Deutschlands e. V. schreibt in seiner Publikation \"Gefährliches Zubehör für Heimtiere\": „Die Produkte erfüllen nicht die Anforderungen von § 2 TSchG“. Zudem haben sich als direkte Reaktion auf den Film verschiedene Tierschutzverbände ähnlich geäußert. Die unter Tierhaltern bekannte Internetplattform „DieBrain“ führte, um der Verbreitung der Hamsterbälle durch den Disney Film entgegenzuwirken, die bundesweite Aktion „Aktion gegen Hamsterbälle“ durch. Auch der WDR berichtete in der Sendung Tiere suchen ein Zuhause über die mögliche negative Wirkung des Films und führt die Gefahren von Hamsterbällen aus.\n\nDer Film wurde 2009 gemeinsam mit \"Kung Fu Panda\" und \"WALL·E – Der Letzte räumt die Erde auf\" für den Oscar nominiert. \n\nWeiterhin gewann \"Bolt\" fünf Nominierungen für den Annie Award und zwei für den Golden Globe Award (Animationsfilm und Filmsong „I Thought I Lost You“).\n\nDie Deutsche Film- und Medienbewertung FBW in Wiesbaden verlieh dem Film das Prädikat wertvoll.\n\nDer Song \"Barking at the Moon\" von Rilo Kiley, im Original gesungen von Jenny Lewis, wird in der deutschen Version des Films von der Sängerin Omnitah gesungen.\n\n\n\n"}
{"id": "4133791", "url": "https://de.wikipedia.org/wiki?curid=4133791", "title": "Windows Essential Business Server", "text": "Windows Essential Business Server\n\nWindows Essential Business Server war eine Serversuite von Microsoft für Kleine und mittlere Unternehmen.\n\nDas Betriebssystem wurde von Microsoft am 16. September 2008 fertiggestellt und zwei Monate später veröffentlicht. Die Einstellung des Verkaufes zum 30. Juni 2010 wurde am 5. März desselben Jahres bekannt.\n\nMicrosoft bot die zwei Editionen Standard und Premium an, die beide auf Windows Server 2008 Standard basierten. Die Standard Edition enthielt Lizenzen für drei Windows Server 2008 x64 Standard-Server-Instanzen. Auf diesen drei Server-Instanzen (physikalisch oder virtuell) wurden folgende Serverprodukte integriert, die durch eine gemeinsame Installationsroutine installiert werden:\nMicrosoft Exchange 2007, Microsoft System Center Essentials, Microsoft Forefront Security für Exchange Server und Forefront Threat Management Gateway (TMG) Medium Business Edition.\n\nDie Premium Version enthielt zusätzlich zu allen Bestandteilen der Standard Edition eine weitere Lizenz für eine Instanz des Windows Server 2008 in der Standard Edition und die Datenbank-Software Microsoft SQL Server 2008.\n\nDer Essential Business Server konnte durch eine zentralisierte und erweiterbare Administrationskonsole verwaltet werden. Er bot erfahrenen Administratoren aber auch den direkten Zugriff auf alle Standard-Verwaltungswerkzeuge wie die Exchange 2007 Verwaltungskonsole, die auch über die EBS-Administrationskonsole aufgerufen werden konnten. Dabei wurden die Möglichkeiten der Windows Server 2008 Terminaltechnologien eingesetzt, indem die Verwaltungswerkzeuge von entfernten Computern zum Teil als sogenannte gestreamte Applikationen aus der EBS-Administratorkonsole heraus aufgerufen wurden. Außerdem bot die Konsole die Möglichkeit, die Computer des Netzwerkes zu überwachen. Hierfür nutzte die Software die Monitoringdaten aus Microsoft System Center Essentials.\n\nDie Konsole konnte von Drittanbietern über ein von Microsoft zur Verfügung gestelltes SDK erweitert werden.\nEssential Business Server beinhaltete ebenfalls den Remote-Web-Arbeitsplatz, einer webbasierten Lösung zum entfernten Zugriff auf E-Mails über Outlook Web Access, Sharepoint sowie der Möglichkeit, auf die Computer des internen Netzwerkes über eine sichere, authentifizierte Verbindung per Remote Desktop Protocol (RDP) zuzugreifen. Diese RDP-Verbindungen werden über den Terminal Services Gateway ausgeführt und benötigen daher nur den SSL Standard-Port TCP/443 um eine Verbindung herstellen zu können. Für alle anderen Dienste des Remote-Web-Arbeitsplatz war ebenfalls ausschließlich dieser Port notwendig.\n\nAm 5. März 2010 gab Microsoft bekannt, dass aufgrund der geringen Nachfrage des Produkts der Windows Essential Business Server ab dem 30. Juni 2010 eingestellt wird. Microsoft empfahl Benutzern den Umstieg auf die individuellen Produkte Windows Server 2008 R2, Exchange Server 2010, System Center Essentials 2010, Forefront Security für Exchange Server 2010 und Forefront Threat Management Gateway 2010. Vom 30. Juni 2010 bis zum 31. Dezember 2010 bot Microsoft Standalone-Produkte von Windows Server 2008 Standard, System Center Essentials 2007 und Exchange Server 2007 kostenlos an aktuelle Essential Business Server-Kunden an.\n\n"}
{"id": "4144310", "url": "https://de.wikipedia.org/wiki?curid=4144310", "title": "Dropzone", "text": "Dropzone\n\nAls Dropzone bezeichnet man einen geheimen Speicherort im Internet, an dem ein Hacker seine automatisch gesammelten gestohlenen Informationen (Passwörter, Kontodaten etc.) sammelt. Solche Dropzones sind meist durch Passwörter geschützt. Häufig befinden sich die Dropzones in Standard-Serververzeichnissen.\n\n\n"}
{"id": "4144521", "url": "https://de.wikipedia.org/wiki?curid=4144521", "title": "TeamViewer", "text": "TeamViewer\n\nTeamViewer ist eine Fernwartungssoftware für Screen-Sharing, Videokonferenzen, Dateitransfer und VPN. Die Software arbeitet als Online-Dienst durch Firewalls und NAT sowie Proxy-Server hindurch. TeamViewer funktioniert nur mit der Internetprotokoll-Version 4 (IPv4). Im Gegensatz zu VNC nutzt es ein proprietäres Protokoll. Für Privatnutzer ist TeamViewer als Freeware verfügbar.\n\nDie Software wurde erstmals 2006 durch die Göppinger TeamViewer GmbH vorgestellt.\n\nTeamViewer unterstützt Windows, macOS, bestimmte Linux-Distributionen vollständig sowie Android und iOS, Blackberry und Windows Phone von denen aus nur ferngesteuert werden kann. Windows 10 Mobile kann mit der „TeamViewer QuickSupport“ App auch auf Mobiltelefonen ferngesteuert werden.\n\nFür die private, nichtkommerzielle Nutzung ist TeamViewer offiziell kostenfrei, jedoch gibt es Reports von Anwendern, dass TeamViewer nur mit zeitlicher Beschränkung einer Sitzung genutzt werden kann. Zusätzliche Funktionen wie eine optionale Verwaltungsdatenbank stehen nur in der kostenpflichtigen Version zur Verfügung.\n\nEine kostenpflichtige Lizenz bezieht sich jeweils auf den Arbeitsplatz, von dem aus Fernsteuerungen oder Meetings durchgeführt werden, nicht auf die ferngesteuerten Rechner oder eingeladenen Teilnehmer eines Meetings. Dabei kann ein einzelner Arbeitsplatz oder eine maximale Anzahl parallel möglicher Sitzungen lizenziert werden.\n\nDie Vollversion von TeamViewer bietet die gesamte Funktionalität der Software. Eingehende und ausgehende Verbindung via Internet und LAN sind möglich, außerdem VPN-Verbindungen und Dateiübertragung. Die Vollversion kann als Windows-Systemdienst laufen.\n\nAb Version 5 ermöglicht das Programm eine Audio- und Videoübertragung über IP-Telefonie, eine Anwendungsauswahl, bei der nur spezielle Anwendungen präsentiert werden, und einen lokalen \"Rückmonitor\", das heißt die exakte Darstellung des übertragenen Bildschirminhalts.\n\nDer TeamViewer Portable enthält den gleichen Funktionsumfang wie die Vollversion. Es werden aber dabei keinerlei Daten auf dem Computer gespeichert. TeamViewer Portable ist damit für den Einsatz auf einem USB-Stick gedacht. TeamViewer Portable ist nicht für OS X erhältlich.\n\nBeim Aufbau einer Verbindung überprüft TeamViewer zunächst, welche Verbindungsarten bei der bestehenden Konfiguration von Sicherheitsmaßnahmen (wie Firewalls und Proxyserver) hergestellt werden können. In 70 % der Fälle kann nach der Aushandlung der Verschlüsselung über die TeamViewer-Server eine direkte Verbindung (über TCP oder UDP) zwischen beiden Endpunkten der Verbindung hergestellt werden; die für diese Sitzung benötigte Datenübertragung wird dann nicht mehr über die TeamViewer-Server abgewickelt.\nDer Hersteller stellt in der Dokumentation eine Beschreibung des Sitzungsaufbaus zur Verfügung. Daraus geht hervor, wann und wie Daten über die TeamViewer-Server übermittelt werden.\n\nBei der portablen Version stimmt der Anwender zu, dass „nicht personenbezogene Daten“ gesammelt werden dürfen.\n\nTeamViewer nutzt eine Verschlüsselung auf der Basis des RSA-Kryptosystems und einer AES-Verschlüsselung mit 256 Bit. Bei direkten Verbindungen zwischen zwei Rechnern im LAN wird symmetrische Verschlüsselung genutzt.\n\nNach Angaben des Herstellers sind Man-in-the-Middle-Angriffe nicht möglich. Dies soll durch den signierten Schlüsselaustausch von zwei Schlüsselpaaren gewährleistet werden. Dabei wird der öffentliche Schlüssel (Public Key) direkt ausgetauscht und damit der symmetrische Schlüssel (AES 256 Bit) generiert. Die öffentlichen Schlüssel werden über den Hauptserver ausgetauscht, der die Echtheit per Signatur über das Stammzertifikat bestätigt. Der öffentliche Schlüssel des Stammzertifikats ist den Clients bekannt, die damit die Echtheit der Signatur prüfen. Somit gibt es eine sichere Zertifikatskette, die den Clients bestätigt, dass ein öffentlicher Schlüssel zu einer bestimmten ID gehört. Das Abgreifen von Daten erfordert somit administrativen Zugriff auf den Hauptserver (nur Meta- und Verbindungsdaten) oder das Ausnutzen von etwaigen Sicherheitslücken des Serversystems.\n\nTeamViewer und ähnlich Programme werden regelmäßig zur Durchsetzung von Technical Support Scams genutzt. Der Betrüger erlangt durch TeamViewer Zugriff auf den Computer des Opfers. In seltenen Fällen wird eine ältere Version von TeamViewer benutzt (in der Regel Version 9), um das Bildschirmbild vor dem Opfer zu verbergen. Der Betrüger ist somit in der Lage, auf dem Computer des Opfers zu agieren, während dieses lediglich einen schwarzen Bildschirm sehen kann.\n\n"}
{"id": "4144666", "url": "https://de.wikipedia.org/wiki?curid=4144666", "title": "Stillinger-Weber-Potential", "text": "Stillinger-Weber-Potential\n\nDas Stillinger-Weber-Potential ist ein klassisches physikalisches Potential zur Darstellung von speziellen Kristallgittern. Der Hauptanwendungsbereich ist die Simulation der Gitterdynamik von Silizium sowie siliziumähnlichen Elementen und deren Legierungen untereinander.\n\nWie bei allen klassischen Potentialen kann keine Aussage zu quantenmechanischen Effekten getroffen werden. Trotzdem sind sie sinnvoll, wenn Systeme betrachtet werden, die aus vielen Atomen oder Molekülen bestehen und der quantenmechanische Aspekt in den Hintergrund tritt. Der Vorteil gegenüber anderen Potentialen, wie dem Lennard-Jones-Potential oder dem Tersoff-Potential liegt in dem guten Verhältnis von Genauigkeit zu Rechenaufwand.\n\nDie Genauigkeit bezieht sich auf Silizium und ähnliche im Diamantgitter kristallisierende Halbleiter und entspringt der Formulierung des Potentials, welche eine tetraedrische Basis bevorzugt. Gleichzeitig ist das aber auch ein Nachteil, da andere Konfigurationen, die beispielsweise unter Druck entstehen können, sowie Effekte an Ober- und Grenzflächen nicht realistisch wiedergegeben werden. Durch den moderaten Rechenaufwand bei der Simulation der Gitterkräfte lohnt sich dennoch der Einsatz bei großen, evtl. periodischen Strukturen, bei denen viele Zeitschritte simuliert werden sollen. So kann mit einem modernen Computercluster innerhalb einiger Tage ein System mit bis zu einer Million Atome und mehreren Millionen Zeitschritten untersucht werden.\n\nF. Stillinger and T. A. Weber, Phys. Rev. B 31, 5262 (1985).\n\nhttp://www.fisica.uniud.it/~ercolessi/md/md/node50.html\n"}
{"id": "4148239", "url": "https://de.wikipedia.org/wiki?curid=4148239", "title": "Pootle", "text": "Pootle\n\nPootle () ist ein in Python geschriebenes freies Online-Übersetzungs- und Verwaltungswerkzeug. Pootle wurde 2004 von Translate.org.za und Partnern entwickelt und veröffentlicht. Später wurde es als Teil des WordForge-Projekts weiterentwickelt und wird heute auf SourceForge verwaltet.\n\nPootle ist eine webbasierte Server-Lösung zur Unterstützung von Übersetzungen für die Softwareentwicklung. Sein Fokus liegt primär auf der Lokalisierung von Software. Es nutzt das Translate Toolkit zur Bearbeitung der zu übersetzenden Dateien. Seine wesentliche Teilfunktionalitäten sind ein Übersetzungsspeicher, eine Glossarverwaltung, sowie Module zur Zielsetzung und zur Benutzerverwaltung.\n\nPootle kann unterschiedlichen Zwecken im Übersetzungsprozess dienen. Zum Beispiel kann es einfach nur die Übersetzungsstatistik auf einem Server anzeigen. Mit Hilfe seiner Vorschlags-Funktionalität können Benutzer Übersetzungs- und Korrekturvorschläge einreichen, so dass es als eine Art Fehlerverfolgungssystem für die Übersetzung verwendet werden kann.\n\nPootle erlaubt die Zuweisung einzelner Übersetzungsprojekte und/oder Sprachdateien eines Projektes zu spezifischen Benutzern. Das Offline-Arbeiten wird durch Export-, Import- und Abgleich-Funktionen unterstützt, so dass mit anderen Tools (z. B. Poedit, Lokalize etc.) gleichzeitig gearbeitet werden kann.\n\nDie erste Veröffentlichung erschien im Dezember 2004, nachdem es von David Fraser von der Translate.org.za-Community im Rahmen eines durch CATIA und St James Software unterstützten Projekts entwickelt worden war. Schon zuvor kam Pootle in diversen Übersetzungsteams von Translate.org.za in Südafrika zum Einsatz.\n\nDer Name \"Pootle\" ist ein (einer Figur der BBC-Kinderserie „The Flumps“ entlehntes) Acronym für \"PO-based Online Translation / Localization Engine\" (deutsch: PO-basiertes Online Übersetzungs- / Lokalisierungs-Werkzeug).\n\nAb 2006 wurde Pootle als Teil des WordForge-Projekts weiterentwickelt, welches wiederum vom Open Society Institute und vom kanadischen International Development Research Centre unterstützt wurde. Hierbei wurde das XLIFF-Dateimanagement sowie die Infrastruktur zur Abbildung von Arbeitsabläufen für Übersetzungen hinzugefügt.\n\nAb Version 2 setzt Pootle auf dem Django-Framework auf.\n\nPootle wird zum Beispiel von OpenOffice.org, TYPO3 und vielen anderen Projekten verwendet. Pootle ist die Basis des Verbatim-Projekts, welches die Infrastruktur für die Lokalisierung der Mozilla-Projekte entwickelt.\n\nDer Pootle Server greift zum Bearbeiten der zu übersetzenden Daten direkt auf Dateien im gettext-PO- bzw. XLIFF-Format zu. Die Texte können für eine schnelle Suche indiziert werden. Bearbeitete Dateien können direkt an ein Versionskontrollsystem übergeben werden. Mit Hilfe des Translate Toolkits können Statistiken, Worthäufigkeiten und Fehler dargestellt werden.\n\nDer webbasierte Pootle Editor ermöglicht das Übersetzen über eine Internetverbindung. Der Editor ist in 55 Sprachen erhältlich, sowohl die Benutzeroberfläche als auch die Bearbeitungsfenster können mit bidirektionalen Texten umgehen. Diverse Filter und Fehlerchecks helfen bei der Qualitätssicherung. Projektspezifische Glossare können benutzt werden. Vorschläge aus dem Übersetzungsspeicher können eingeblendet werden.\n\n\n"}
{"id": "4149152", "url": "https://de.wikipedia.org/wiki?curid=4149152", "title": "GlusterFS", "text": "GlusterFS\n\nGlusterFS ist ein verteiltes Dateisystem, das Speicherelemente von mehreren Servern als einheitliches Dateisystem präsentiert. Die verschiedenen Server, auch Cluster-Nodes ( ‚Knoten‘) genannt, bilden eine Client-Server-Architektur über TCP/IP. Als Besonderheit können NAS-Systeme über Infiniband direkt in den Cluster eingebunden werden, auch eine redundante Anbindung von Speichermedien über TCP/IP, Infiniband Verbs oder InfiniBand SDP (Socket Direct Protocol) ist möglich. Die Daten auf allen Cluster-Nodes können gleichzeitig gelesen und geschrieben werden, wobei alle Änderungen an Dateien auf allen Servern augenblicklich umgesetzt werden. Das Dateisystem wird über ein FUSE-Kernel-Modul eingebunden und wird von POSIX-fähigen Betriebssystemen unterstützt, zum Beispiel Linux, FreeBSD, OpenSolaris und Mac OS X. Um einen GlusterFS-Server zu starten, wird kein Kernel-Modul benötigt. Ein Server kann sowohl Client als auch Server gleichzeitig sein. Ein Client für Windows-Systeme ist in Planung, wird aber von den Entwicklern erst umgesetzt, sobald das WinFUSE-Projekt stabil läuft.\n\nDie Entwicklung von GlusterFS begann Mitte 2005 durch das GlusterOS-Entwicklerteam von Z Research Inc., ein erstes Release des Dateisystems wurde im Juli 2006 veröffentlicht. GlusterFS ist unter der GPL in Version 3 lizenziert. Die Entwickler bieten kostenpflichtigen Support an. Ende 2011 wurde GlusterFS von Red Hat für 136 Millionen US-Dollar gekauft.\n\nGlusterFS ist modular gestaltet und unterstützt mehrere Betriebsmodi:\n\n\n\n\n\n\n\n\n\nGlusterFS bedient sich eines modularen Aufbaus, so dass mit wenigen Komponenten alle beschriebenen Funktionen abgebildet werden können. Die Komponenten sind Datenpartitionen (volumes), Transportgruppen (transport groups) und Übersetzer (translators). Die Übersetzer bieten die Möglichkeit, das Dateisystem um Funktionen zu erweitern, insbesondere ob dieses über Infiniband, TCP oder einer Mischung aus beidem kommunizieren soll, welche physischen Partitionen oder Verzeichnisse eingebunden werden sollen und welche RAID-Level verwendet werden. Zur Veranschaulichung des modularen Konzepts existiert ein ROT13-Übersetzer zum „Verschlüsseln“ des Dateisystems.\n\nMit GlusterFS lässt sich eine Art Netzwerk-RAID erstellen, von welchem aus mehrere Rechner gleichzeitig auf ein gemeinsames Dateisystem zugreifen können. Es unterliegt hierbei nicht Limitierungen wie der, maximal zwei Server nutzen zu können. GlusterFS ist fehlertolerant, da bei GlusterFS Nutzdaten, Metadaten und Namespace verteilt gespeichert werden können. Durch jeden weiteren GlusterFS-Server erhöht sich der maximale Datendurchsatz des Dateisystems, so dass hier I/O-Bandbreiten von einigen Gigabyte pro Sekunde erreicht werden können.\n\nBei Prozessoren (CPUs) gilt das Mooresche Gesetz, was jedoch bei Speichermedien und Storage-Lösungen nicht zutrifft, obwohl hier ebenfalls ein Bedarf nach größeren und schnelleren Speichern besteht. Oftmals ist nicht die CPU-Leistung eines Servers der Flaschenhals, sondern immer öfter die zu langsamen Datenspeicher des Systems. GlusterFS schafft hier Abhilfe durch die Möglichkeit, beliebig zu skalieren.\n\n\n"}
{"id": "4150390", "url": "https://de.wikipedia.org/wiki?curid=4150390", "title": "Windows SteadyState", "text": "Windows SteadyState\n\nWindows „Steady State“ war ein kostenloses englischsprachiges Sicherheits-Tool der Firma Microsoft, das früher unter dem Namen „Microsoft Shared Computer Toolkit“ bekannt war. Das Programm wird von Microsoft seit dem 31. Dezember 2010 nicht mehr angeboten, jedoch bieten einige Anbieter das Programm noch immer zum Download an. Der Support endete mit dem 1. Juli 2011.\n\nSteady State war für den Einsatz an Rechnern mit Windows-XP-Betriebssystem gedacht, die sich an öffentlichen Orten befinden und vielen Personen zugänglich sind, z. B. in Bibliotheken oder Internetcafés. In der aktuellen Version 2.5 war es möglich, Steady State auch unter Windows Vista zu nutzen.\n\nDas Programm konnte jedoch im Privatgebrauch ebenfalls verwendet werden, so dass z. B. verhindert werden konnte, dass andere Nutzer bestimmte Einstellungen ändern oder Dateien von der Festplatte löschen, indem das entsprechende Profil gesperrt wurde. Der betreffende Nutzer konnte dann scheinbar normal Änderungen an seinem Konto vornehmen, diese wurden jedoch bei der Abmeldung wieder gelöscht.\n\nEs war ebenfalls möglich, das Ausführen unbekannter Software zu verhindern oder den Zugriff auf bestimmte Programme zu verbieten. Alle Änderungen, die während einer Sitzung vorgenommen wurden, d. h. wenn z. B. ein Programm ausprobiert oder versehentlich eine wichtige Datei, z. B. ein Foto, gelöscht wurde, konnte dies mit der Option „Protect the Hard Disk“ im Hauptdialog von Steady State wieder rückgängig gemacht werden. Das galt auch für den Fall, dass versehentlich ein Computervirus installiert wurde. Dies war möglich, da das Programm die Systempartition in einer Cache-Datei speichern konnte (Cache.wdp), die mindestens 2 GB bis maximal 40 GB umfasste.\n\nSollte nicht nur das XP-System selbst, sondern auch andere Dateien und Anwendungen geschützt werden, mussten sich diese auf der Systempartition (meist Laufwerk C:) befinden. Beim Arbeiten am Rechner schrieb \"Steady State\" mit Hilfe eines Virtualisierungtreibers alle Änderungen und Daten in die erstellte Cache-Datei. Sobald der Computer neu gestartet wurde, wurden diese Dateien gelöscht und damit sämtliche Änderungen am System.\n\n"}
{"id": "4158650", "url": "https://de.wikipedia.org/wiki?curid=4158650", "title": "Orca (Screenreader)", "text": "Orca (Screenreader)\n\nOrca ist ein Open-Source-Screenreader für die Desktop-Umgebung Gnome. Er ist in den meisten GNU/Linux-Distributionen wie beispielsweise openSUSE, Ubuntu, Fedora, aber auch unter openSolaris enthalten. Auch wenn Orca nicht der erste Screenreader für eine der grafischen Oberflächen unter Linux ist, so hat er sich als einziger etabliert und erscheint seit 2005 mit jeder Freigabe von GNOME.\n\nOrca arbeitet mit dem Braillezeilensteuerungsprogramm BrlTTY zusammen, um serielle, USB- und Bluetooth-Braillezeilen ansteuern zu können. Orca bezieht seine Daten aus der Assistive Technology Service Provider Interface (AT-SPI). Es interagiert daher grundsätzlich mit allen Anwendungen, die ihre Oberfläche über AT-SPI zugänglich machen wie bspw. Adobe Reader, Eclipse, Mozilla Thunderbird oder OpenOffice.org. Die aktuelle stabile Version von Orca hängt immer von der aktuellen GNOME-Version ab.\n\nDer Name von Orca nimmt Bezug auf das populäre proprietäre Konkurrenzprodukt JAWS: \"Jaws\" ist der Originaltitel des Films \"Der weiße Hai\"; dort ist \"Orca\" der Name des Boots, mit dem Jagd auf den Hai gemacht wird, benannt nach dem einzigen natürlichen Feind des weißen Hais, dem Großen Schwertwal. Auch andere bedeutende Screenreader bzw. deren Hersteller sind ähnlich nach Meeresgetier benannt wie Flipper oder Dolphin.\n\nDie erste Version von Orca wurde offiziell im November 2005 vorgestellt. Zuvor hatte Sun Microsystems beschlossen, das Engagement im Gnome-Projekt zu erweitern und einen freien Screenreader für GNOME zu entwickeln. Marc Mulcahy, ein blinder Programmierer bei Sun, erarbeitete den ersten lauffähigen Prototyp. Im Januar 2010 wurde Sun Microsystems durch Oracle übernommen. Im Zuge der firmeninternen Umstrukturierung wurde auch die Entwicklung von Orca eingestellt. Offene Briefe aus der Gemeinschaft wurden von Oracle nicht beantwortet. Orca wurde schließlich zu einem Gemeinschaftsprojekt, welches von Freiwilligen verwaltet, entwickelt und übersetzt wird.\n\nOrca nutzt standardmäßig eine freie Sprachausgabe, eSpeak. Sprachausgaben werden unter Orca mit Sprachmodulen angesteuert, sogenannten Backends.\n\nOrca nutzt zur Ansteuerung für Sprachausgaben drei verschiedene Module:\n\nMan kann mit Orca auch kostenpflichtige Synthesizer verwenden, wie zum Beispiel IBM ViaVoice. Solche externen Sprachausgaben werden dann meistens mit dem Speech Dispatcher angesteuert.\n\nOrca kann über den Bildschirmleser BrlTTY angesteuert werden. Dabei fungiert BrlTTY als Ansteuerungsprogramm für die Braillezeilen, d. h. es stellt die Treiber zur Verfügung. Angesteuert wird BrlTTY von Orca über die BRLapi, ein Ansteuerungsprotokoll für BrlTTY.\n\nOrca gibt die Inhalte des Bildschirms auf der Braillezeile mit Kontext aus. So findet man in der Menüleiste neben dem Wort \"Datei\" auch \"menu\" bzw. \"mnu\" (je nach Ausführlichkeitseinstellung). Dies erlaubt eine bessere Übersicht über den Bildschirminhalt. Weiterhin kann Orca Text auch in Kurzschrift anzeigen und in neueren Versionen auch gesprochene Meldungen auf der Braillezeile ausgeben.\n\nOrca kann auch eine Bildschirmvergrößerung für stark Sehgeschädigte bereitstellen. Dort kann der Nutzer den Vergrößerungsgrad, den Kontrast, die Farbeinstellungen und einiges mehr festlegen. Hierfür sollte ein Rechner mit mehr als 512 MB RAM bereitstehen, da es sonst zum Ruckeln des Bildes kommen könnte.\n\n"}
{"id": "4164870", "url": "https://de.wikipedia.org/wiki?curid=4164870", "title": "Rollygeddon", "text": "Rollygeddon\n\nRollygeddon ist ein 31-minütiger Amateur-Animationsfilm von Thomas Zeug.\n\nEin riesiger Asteroid steuert geradewegs auf die Erde zu und droht die Menschheit zu vernichten. Durch einen unglücklichen Zufall ist Rolly derjenige, den die Nasa nun mit einem Raumschiff und einem Sprengkörper bewaffnet zu dem Felsbrocken aus dem All fliegen, ihn in die Luft sprengen und somit die Menschheit retten soll. Doch während seiner Mission geht leider etwas gewaltig schief und Rolly begeht einen folgenschweren Fehler.\n\nDer Film wurde innerhalb von knapp zwei Jahren von Thomas Zeug zu Hause am Computer erstellt. Das Projekt war ursprünglich als 5-minütiger Clip geplant, da es ein Remake eines älteren Kurzfilmes von Thomas Zeug ist. In einer Szene findet eine Schlägerei mit einer animierten Version von Bud Spencer statt. Diese wurde Bild für Bild aus einem Teil des Films Zwei wie Pech und Schwefel nachanimiert. Die Hauptfigur Rolly wurde von Thomas Zeug selbst gesprochen. \n\nNach der Premiere am 10. August 2007 im Regensburger Regina Kino wurde Rollygeddon acht Tage später auch im Internet veröffentlicht und ist seitdem zum kostenlosen Download verfügbar. Des Weiteren vertreibt Thomas Zeug den Film auch als DVD, auf welcher sich zusätzliche Extras (z. B. Audiokommentar, Gag-Lexikon, geschnittene Szenen, …) befinden. Mehr als zwei Drittel der Produktionskosten konnten durch freiwillige Spenden refinanziert werden.\n\nDer Film lief bereits auf mehreren Festivals und konnte auch schon einige Preise gewinnen, darunter den mit 10.000 € dotierten Rookie Award von TV-Spielfilm, der auf der Jupiter Preisverleihung 2008 in Berlin vergeben wurde. Der Film wurde auch mehrmals in der Sendung on3-südwild im Bayerischen Fernsehen und auf dem Aus- und Fortbildungskanal ausgestrahlt.\n\n\n"}
{"id": "4165531", "url": "https://de.wikipedia.org/wiki?curid=4165531", "title": "Windows Media Center", "text": "Windows Media Center\n\nDas Windows Media Center (WMC) ist eine Software von Microsoft, mit der Musik, Bilder und Filme wiedergegeben werden können. Bei der \"Windows XP Media Center Edition\" ist das Media Center schon integriert; es wird mit \"MCE\" abgekürzt.\n\nMCE wurde erstmals in Form der Windows XP Media Center Edition separat verkauft. Es ist in Vista und Windows 7 in den Anwender-Betriebssystemen von Microsoft enthalten; in Windows 8 muss es nachgerüstet werden. Die Benutzeroberfläche des WMC lässt sich mit der Fernbedienung, aber auch mit Maus und Tastatur steuern.\nDurch optimierte Schriftgrößen und Formatierungen können WMC-PCs in der Unterhaltungselektronik, insbesondere Fernseher, verwendet werden oder diese ersetzen. Anwendungsgebiete sind daher Wohnzimmer-PCs oder auch IDTV-Systeme. Digitales Kabelfernsehen nach dem (in Europa üblichen) DVB-C-Standard wird ohne Kauf von Drittanbietersoftware nicht unterstützt.\n\nAm 2. Mai 2015 kündigte Microsoft an, das Windows Media Center einzustellen und nicht mehr für Windows 10 anzubieten.\n\nDas Programm kann Bilder, Videos und Musik von lokalen Laufwerken und aus dem Netzwerk wiedergeben. Das Programm ordnet die Dateien nach Name, Datum, Tags und vielen weiteren Dateiattributen. Die Navigation ist für alle Medieninhalte gleich und überschaubar organisiert.\n\nDas Programm arbeitet eng mit dem Windows Media Player (WMP) zusammen, der die erweiterte Bearbeitung und Verwaltung organisiert.\nAlle Medieninhalte, die das WMC wiedergeben kann, können über das Netzwerk an Fernseher mit Windows Media Center Extender oder mit der Xbox 360 gesendet werden.\n\nDas WMC unterstützt tragbare Geräte, die über USB als Datenträger erkannt werden. Dateien können aus der Medienbibliothek heraus gespeichert bzw. synchronisiert werden. Allgemein werden Windows-Mobile-Geräte, Pocket PCs, Smartphones, Portable Media Player und auch USB-Sticks erkannt.\nSollten Videoaufnahmen im DVR-MS-Format vorliegen, werden diese bei der Übertragung in das Format Windows Media Video mit niedrigerer Bitrate umgewandelt, um die Dateigröße zu reduzieren. Optional kann auch Musik während der Synchronisation neu-encodiert werden.\n\nBilder können in einer Einzelansicht oder als Diashow mit oder ohne Musik abgespielt werden. Diashows können gespeichert werden und als Bildschirmschoner verwendet werden. Einfache Bearbeitungen wie Beschneiden, Drehen oder Rote-Augen-entfernen sind vorhanden und können auch mit der Fernbedienung gesteuert werden.\nWird eine Kamera oder ein anderes Speichermedium angeschlossen, können die Bilder über das WMC direkt importiert werden.\n\nIst im Computer eine TV-Karte eingebaut, kann das WMC diese für den Empfang benutzen, Programme zeitversetzt wiedergeben oder aufnehmen. Als Format werden die analogen Formate PAL und NTSC und bis zu vier TV-Karten im Computer unterstützt.\nMit Windows 7 werden die Digital TV Formate ATSC, DVB, ISDB und QAM unterstützt. Digitales Kabelfernsehen nach europäischem Standard wird weiterhin, aufgrund fehlender passender Frequenzlisten, nicht unterstützt. Allerdings bieten wenige TV-Karten-Hersteller hier treiberseitige Lösungen an.\n\nAuch wird die Wiedergabe und Aufnahme über Composite- und S-Video-Eingänge unterstützt.\n\nAls Aufnahmeformat dient dabei das hauseigene DVR-MS (Digital Video Recording through Microsoft) das sich aus einem MPEG2-Stream in einem ASF-Container zusammensetzt. Die aufgenommenen Filme können dabei über das Windows Media Center oder über den Windows DVD Maker als Video-DVD gespeichert werden.\n\nÜber viele verschiedene Quellen besteht die Möglichkeit Musik aufzunehmen. So können bei entsprechender Hardware und Treiberkonfiguration auf verschiedene Eingänge der Soundkarte zugegriffen werden, und damit auch ältere analoge oder digitale Formate aufzunehmen.\nDas Windows Media Center organisiert und speichert die Daten dabei in der Grundeinstellung im persönlichen Musikordner. Diese werden in einer gemeinsamen Medienbibliothek, die das WMC und der Windows Media Player besitzen, aufgeführt. Der Benutzer kann in beiden Programmen Wiedergabelisten erstellen und Medieninformationen bearbeiten. Alle Änderungen stehen in beiden Programmen direkt zur Verfügung.\n\nEs sind alle üblichen Steuerelemente vorhanden. zusätzlich kann bei längerem drücken der Sprungtasten auch gespult werden soweit das Dateiformat dies auch unterstützt. Als Modi stehen die \"Zufällige Wiedergabe\" oder \"Wiederholen\" zur Verfügung. Auch können die Visualisierungen des WMP verwendet werden.\n\nSollte die Fernsehkarte auch Radioempfang unterstützen, kann das WMC diese benutzen um Sender zu suchen, wiederzugeben, zu speichern und zu benennen.\n\nPraktisch alle Formate, die der Windows Media Player wiedergeben kann, können ebenfalls im Windows Media Center abgespielt werden. Probleme gibt es mit der Integration von zusätzlichen Formaten, z. B. Matroska, Ogg, FLAC, deren Dateiformate bisher nur eingeschränkt abgespielt werden können.\n\nVon vorhandenen Filmen können DVD-Videos erstellt werden, solange diese als Windows Media Video, AVI oder MPEG-2 vorliegen.\n\nDas WMC kann über Plug-Ins mit verschiedenen Funktionen ausgestattet werden, die vergleichsweise ungewöhnlich für Windows Standardanwendungen sind. Von Communities wurden bereits zahlreiche freie Tools entwickelt, die die Funktionen des Programms weiter ausbauen. Aber auch kommerzielle Anbieter bieten Plugins zu ihrer Plattform an, wie auch vereinzelte Fernsehsender, um ihr Programm mit zusätzlichen Informationen auszustatten.\n\nErweiterungen werden meistens über das Menü Online-Medien zur Verfügung gestellt.\n\nSpiele waren schon in Windows Media Center 2003 enthalten wurden aber in den späteren Versionen entfernt und erst wieder mit Windows Vista hinzugefügt. Bereits integriert sind die mitgelieferten Windows Spiele, die sich mit Hilfe einer Fernbedienung spielen lassen. Weitere Spiele werden auch von den Communities entwickelt.\n\nEs besteht die Möglichkeit Medieninhalte über Internetadressen aufzurufen, sowie diese in Wiedergabelisten auf dem Computer oder Servern zu speichern. In älteren Versionen muss dafür ein Plug-In nachinstalliert werden. Allerdings besteht derzeit keine Möglichkeit die Streams auf der Festplatte zu speichern.\n\nDas Windows Media Center wurde mit Windows XP Media Center Edition 2003 eingeführt. Jede Version erhielt in der Entwicklung von Microsoft einen Codenamen, um zukünftige und bestehende „Wohnzimmerwelten“ multimedial mit dem Betriebssystem Windows zu betreiben.\n\nRechner, die von Herstellern mit der Windows XP Media Center Edition ausgestattet wurden, wurden mit einer Fernbedienung ausgeliefert. Auf dieser befand sich ein grüner Knopf mit dem Windows-Logo. Dieser als MCE-Taste bekannte Knopf ermöglicht es, die Media Center-Oberfläche zu starten und von einem beliebigen Ort im Menü der Oberfläche zum Ausgangspunkt zurückzukehren. Die markante grüne Taste auf den MCE-Fernbedienungen wird des Öfteren auch als Logo des Media Centers begriffen und somit von verschiedenen Internet-Gemeinschaften in Name und Logo eingebaut. Ein gutes Beispiel dafür ist das englischsprachige MCE-Forum „The Green Button“ (deutsch: „Der grüne Knopf“). Da nach der Einführung von Windows Vista das Windows Media Center auch für den Endverbraucher zugänglich wurde, war eine Fernbedienung nicht mehr zwingend an die PC-Systeme gekoppelt. Die Fernbedienung, die mit einem Vista-System ausgeliefert wird, hat eine runde MCE-Taste.\n\n\"Codename: „Freestyle“\"\n\nEnthielt neben anderen kleinen Änderungen die Unterstützung für Radioempfang.\n\n\"Codename: „Harmony“\"\n\nMit der Version 2004 wurde die MCE in einigen europäischen Ländern erstmals vertrieben. Private Kunden konnten diese Version jedoch zunächst ausschließlich als Bestandteil eines Komplettsystem erwerben. Die Hardware-Unterstützung (z. B. Grafikkarten, TV-Karten) war jedoch noch recht eingeschränkt. Trotzdem wurde bereits diese Version in vielen Systemen als Multimedia-Wohnzimmer-Anwendung eingesetzt. Die Version 2004 wird seit dem Oktober 2004 nicht mehr vertrieben.\n\n\"Codename: „Symphony“\"\n\nDiese Version der MCE basiert auf Windows XP Professional mit integriertem Service Pack 2. Diese Version der MCE war erstmals auch mit einer System-Builder-Lizenz einzeln erhältlich, womit der Bau von eigenen Multimediacomputern auf Basis von Windows endlich möglich wurde.\n\nAm 14. Oktober 2005 erschien außerdem ein Updatepaket namens „Windows XP Media Center Edition 2005 Rollup 2“ (Codename: „Emerald“). Es enthält neben einigen kleineren Aktualisierungen vor allem die Unterstützung für die Kommunikation mit der Xbox 360.\n\nDie Media Center Edition 2005 kann im Gegensatz zu vorherigen Versionen keiner Domäne beitreten, was die einzige Einschränkung gegenüber Windows XP Professional darstellt. Wird jedoch eine bestehende Installation auf MCE 2005 aktualisiert, bleibt der Computer in der Domäne.\n\n\"Codename: „Diamond“\"\n\nDas Windows Media Center ist Bestandteil der Windows-Vista-Versionen Ultimate und Home Premium. Diese überarbeitete Version enthielt erstmals eine neue Art des \"Setups\" (Einrichtung des Programms), welche eine schnellere Inbetriebnahme der Software erlaubt.\nDie Benutzeroberfläche wurde neu organisiert und dem Design der Aero-Oberfläche von Windows Vista angepasst.\nDie neuen Menüs, in denen man nun auch horizontal navigieren konnte, ermöglichten eine bessere Nutzung von Breitbildschirmen im 16:10 und Breitbild-Fernsehern im 16:9 Format. Neu war auch die Möglichkeit, das Hauptmenü während der TV- oder Video-Wiedergabe transparent über das Bild legen zu lassen. Dadurch konnten Einstellungen vorgenommen werden, während die Wiedergabe im Hintergrund weiter lief.\n\n\"Codename: „Fiji“\"\n\nDas \"Windows Media Center TV Pack 2008\" ist ein Update ausschließlich für Windows Vista SP1 basierende Systeme. Die neueste Version von Windows Media Center wurde am 16. Juli 2008 durch Microsoft freigegeben. Windows Media Center TV Pack 2008 war nur für sogenannte OEMs verfügbar – d. h. für Hersteller, die auf Basis von Windows Media Center komplette Geräte inklusive Hardware und Software herstellen. Für bestehende PCs, die Media Center unter Windows Vista nutzten, war ein Aufrüsten auf das TV Pack 2008 lizenztechnisch nicht möglich. Ebenso enthielten neue erworbene oder neu installierte Versionen von Windows Vista Service Pack 1 das TV Pack 2008 nicht (außer es handelte sich um die erwähnten OEM-Geräte) und es gab keine Möglichkeit zum (legalen) Download von Media Center TV Pack 2008.\nDie offizielle Ankündigung des Media Center TV Pack 2008 durch Microsoft erfolgte am 28. August 2008 im Rahmen der IFA Messe in Berlin. Microsoft bestätigte die Existenz des Produktes nur indirekt durch einen Artikel in der Microsoft Support-Datenbank.\n\nNeuerungen in Windows Media Center TV Pack 2008 waren unter anderem:\n\nWeiterhin nicht unterstützt wurde auch im Media Center TV Pack 2008 hochauflösendes Fernsehen (HDTV) nach dem H.264 Standard.\n\nDas WMC in Windows 7 besitzt dieselben Funktionen des \"TV Pack 2008\", enthält jedoch angepasste Animationen, neue Klänge und Fehlerkorrekturen, die in einer spürbaren Leistungssteigerung erkennbar sind. Zudem ist es nun möglich, TV Karten nach dem DVB-S2 Standard zu betreiben und somit HDTV im H.264 Codec zu betrachten. Obwohl der mitgelieferte Windows Media Player 12 mit vielen fremden Formaten arbeiten kann, können bestimmte Formate wie z. B. Matroska und Ogg-Dateien nicht sofort abgespielt werden. WMC erkennt zwar die Dateien und auch deren Typ, jedoch müssen erst die entsprechenden Codecs von Drittanbietern installiert werden, um sie auch abspielen zu können. Neu hinzugekommen ist auch die Funktion \"Filmbibliothek\", mit der es möglich ist, die eigene Filmsammlung auf Festplatte optisch im WMC darzustellen. Verglichen wird diese Option auch gern mit einer virtuellen Videothek.\n\nFür Besitzer von Windows 8 Pro war das Media Center bis zum 31. Januar 2013 kostenlos erhältlich. In diesem Zeitraum musste der per E-Mail versendete Product Key auch aktiviert worden sein, sonst verlor er seine Gültigkeit. Seitdem kann das Media Center Pack zu einem Preis von ca. 10 Euro erworben werden. DVB-C wird weiterhin nicht direkt unterstützt und kann nur durch den Kauf von Drittanbietersoftware (z. B. \"DVBLink\" des Herstellers \"DVBLogic\") genutzt werden.\n\nFür Windows 10 bietet Microsoft selbst das Windows Media Center nicht mehr an. Jedoch hat eine Programmierergruppe das Media Center für Windows 10 portiert, so dass es auch dort lauffähig ist. Die Freeware ist funktional mit der Version aus Windows 8.1 identisch. Zudem unterstützt die neue Version nicht nur eine Darstellung im Vollbild-, sondern auch im Fenstermodus. Das Programm ist auch auf Deutsch verfügbar.\n\n\nMedia Center Communitys:\n"}
{"id": "4171490", "url": "https://de.wikipedia.org/wiki?curid=4171490", "title": "1chipMSX", "text": "1chipMSX\n\nDer 1chipMSX oder das ESE MSX System 3 ist ein MSX-2-System von \"D4 Enterprise\". Es wurde 2006 in Japan auf den Markt gebracht.\n\nDer \"1chipMSX\" ist eine neue Implementierung des MSX-2-Standards in einen Altera Cyclone EP1C12Q240C8N FPGA Chip. Das System hat ein transparentes blaues Gehäuse, sowie zwei Slots für MSX-Cartridges und zwei Joystick-Schnittstellen. Das Gerät verfügt über diverse Video-Ausgänge (S-Video, Composite Video, VGA), einen PS/2-Tastatur-Eingang sowie zwei USB-Schnittstellen und ein SD/MCC-Karten-Slot. Vorbereitet sind zwei Audio-Ausgänge für eine Stereo-Ton-Implementierung bei einer späteren Generation.\n\nDas fehlende Diskettenlaufwerk wird durch eine Emulation von Disk-Files auf den SD/MMC-Karten kompensiert. Es ist auch möglich per SD/MMC-Karten zu booten.\n\nDie japanische Version konnte wegen RoHS nicht in Europa verkauft werden. Eine europäische Version ist in Planung, ist bisher aber nicht auf den Markt gekommen.\n\n\n"}
{"id": "4172265", "url": "https://de.wikipedia.org/wiki?curid=4172265", "title": "Canon V-20", "text": "Canon V-20\n\nDer Canon V-20 ist ein MSX-Heimcomputer von Canon, der 1983 auf den Markt gebracht wurde.\n\nCanon hat sich erst später dem MSX-Standard angeschlossen und mit ihren MSX-Rechnern vor allem ihre hauseigene Designlinie umgesetzt. Die Canon-Modelle ragten von ihrer Ausrüstung nicht besonders aus der Masse der MSX-Rechner anderer Anbieter hervor.\n\nVon Canon wurde weiters eine Hardwareerweiterung hergestellt, welche Aufnahmedaten wie Blende, Belichtungszeit oder Aufnahmedatum vom Datenrückteil der Spiegelreflexkamera Canon T90 empfangen konnte. Wegen der Standardisierung von MSX war die Erweiterung auch auf Computern anderer Hersteller lauffähig.\n\nVon dem gleichzeitig erschienenen Canon V-10 (weißes Gehäuse) unterscheidet sich dieses Modell vor allem durch die dunkelgraue Gehäusefarbe und den größeren Speicher. Typisch für die Modelle sind die relativ großen Cursortasten, die vor allem für Spiele ideal sind.\n\nDer Rechner ist mit einem Zilog-Z80A-Prozessor mit einer Taktfrequenz von 3,58 MHz ausgestattet und verfügt über 64 KByte RAM, erweiterbar auf 128 KByte plus 16 KByte Video-RAM und 32 KByte ROM. Er hat zwei Modulports, eine Centronics-Parallelschnittstelle (Drucker), einen Kassetten-Anschluss zur Datenspeicherung, einen Audio/Video-Ausgang, einen HF-Antenneanschluss und zwei Joystick-Ports.\n\nText wird mit 40 * 24 Zeichen dargestellt, die Grafikauflösung beträgt 256 × 192 Pixel bei 16 Farben, als Grafikchip kommt ein TMS9918A von Texas Instruments zum Einsatz. Der Audiochip General Instrument AY-3-8910 liefert drei Stimmen mit je acht Oktaven.\n\n\n"}
{"id": "4172371", "url": "https://de.wikipedia.org/wiki?curid=4172371", "title": "Philips VG-8020", "text": "Philips VG-8020\n\nPhilips VG-8020 ist ein MSX-Heimcomputer von Philips, der 1984 auf den Markt gebracht wurde.\n\nDie Tastatur des Computers bestand im Gegensatz zu seinem Vorgänger Philips VG-8010 aus qualitativ hochwertigen Schreibmaschinentasten. Die Cursortasten wurden als separater Block angeordnet. Das Tastenschema entsprach dem amerikanischen ASCII-Standard QWERTY. Es gab keine Tasten für Umlaute. Die Funktionstasten wurden auf fünf beschränkt.\n\nDas Netzteil wurde in das Gehäuse integriert. Der Computer ist mit einem Zilog Z80A-Prozessor mit einer Taktfrequenz von 3,58 MHz ausgestattet und hat über 64 KByte RAM wovon 29 Kbyte aus dem BASIC adressierbar waren. Der Speicher war erweiterbar auf 128 KByte und bestand zusätzlich aus 16 KByte Video-RAM und 32 KByte ROM. Er hatte somit doppelt so viel Arbeitsspeicher, wie der fast gleichzeitig erschienene Phillips VG-8010.\n\nZu seinen Schnittstellen gehörten zwei Modulports, die sich unter einer rauchfarbenen Abdeckung befanden. Zu seinen Schnittstellen gehörten zudem eine Centronics-Schnittstelle, ein Kassettenanschluss, ein Video-Ausgang, ein HF-Antennen-Anschluss und zwei Joystick-Ports an der Gerätefront.\n\nFür diese Computermodelle von Philips gab es umfangreiches Zubehör wie Monitore, Drucker, Floppy-Disk-Station, Joysticks, Kassettenrekorder (D-6600/60P) und Speichererweiterungen bis zu 144 Kbyte.\n\nDer Computer war aufgrund eines Bugs nicht völlig kompatibel zu anderen MSX-1-Systemen, konnte aber mit einem Workaround (codice_1) vor dem Starten eines Programms korrigiert werden.\n\n\n\n"}
{"id": "4173243", "url": "https://de.wikipedia.org/wiki?curid=4173243", "title": "Daewoo CPC-300", "text": "Daewoo CPC-300\n\nDaewoo CPC-300 ist ein MSX-2-Heimcomputer von der koreanischen Elektronikfirma Daewoo und wurde 1989 auf den Markt gebracht.\n\nNach den MSX-1-Modellen CPC-100 und CPC-200 erschien dieser MSX-2-Computer im Jahr 1989. Das Gerät wird von einem Zilog Z80A-Prozessor mit einer Taktfrequenz von 3,58 MHz betrieben.\n\nDer Computer hat eine Video-Schnittstelle (NTSC) und eine Audio-Schnittstelle (Mono). Man kann zwischen schwarzweiß und farbig umstellen. Ebenso besitzt der Computer einen Cartridge-Port, eine Druckerschnittstelle und eine Kassettenrekorderschnittstelle. Die benötigte Netzspannung beträgt 110 oder 220 V bei 50 Hertz und kann per Schalter an der Unterseite des Computers eingestellt werden.\n\nDer Computer wurde in zwei Versionen hergestellt: CPC-300 und CPC-300E. Beim CPC-300E handelte es sich um eine abgespeckte Variante. Ebenso wurde CPC-300 in einer koreanischen und einer internationalen Variante hergestellt. Die koreanische Tastatur beinhaltet Hangeul-Zeichen. Der Splash Screen wurde gegenüber den MSX-Rechner verändert und „IQ 2000“ in Hangeul wird angezeigt.\n\nCPC-300 hat 128 KByte Arbeitsspeicher und 128 Kbyte Video-RAM, eine batteriegetriebene Uhr und zwei Joystick-Ports, sowie eine Lichtgriffel-Schnittstelle und ein eingebautes Programm „MSX-TUTOR“. Der CPC-300E hat nur 64 KByte Arbeitsspeicher und keine batteriegetriebene Uhr und es fehlen die Schnittstellen für Joystick und Lichtgriffel.\n\n"}
{"id": "4175148", "url": "https://de.wikipedia.org/wiki?curid=4175148", "title": "HijackThis", "text": "HijackThis\n\nHijackThis, manchmal als HJT abgekürzt, ist ein freies Werkzeug für Microsoft Windows zur Diagnose und Entfernung von Malware-Befall. Ursprünglich von Merijn Bellekom erstellt, wurde es später an Trend Micro verkauft. Das Programm verfolgt einen heuristischen Ansatz, um Schadsoftware zu erkennen: Anstatt sich auf eine Datenbank mit bekannter Malware zu verlassen, scannt es einen Computer, erzeugt eine Liste von Unterschieden zu einer bekannten malwarefreien Umgebung und erlaubt dem Benutzer zu entscheiden, was auf dieser Liste vom System entfernt werden soll.\n\nJüngere Versionen von HijackThis beinhalten zusätzliche Werkzeuge wie einen Task-Manager, einen Editor für die hosts-Datei und einen Scanner für Alternate Data Streams.\n\nHijackThis wird vorrangig zur Diagnose von Schadsoftware-Befall eingesetzt, da sorgloser Umgang mit seinen Entfernungsoptionen erheblichen Softwareschaden an einem Computer anrichten kann.\n\nAm 18. Januar 2012 hat Trend Micro den Quellcode unter der GPLv2 auf Sourceforge veröffentlicht. Bis dahin war HijackThis Freeware.\n\nHijackThis generiert Plain-Text-Logdateien mit detaillierten Einträgen zu allen Funden. Die meisten dieser Einträge können von HijackThis entfernt oder ausgeschaltet werden. Unerfahrenen Nutzern wird geraten, mit letzter Option vorsichtig umzugehen, da HijackThis nicht zwischen erwünschten und unerwünschten Einträgen unterscheidet. So kann der Benutzer unabsichtlich wichtige Programme blockieren, wodurch das Betriebssystem oder Peripheriegeräte funktionsuntüchtig gemacht werden können. HijackThis versucht allerdings, Sicherungskopien von Dateien oder Registrierungseinträgen, die es entfernt, anzulegen, die zur Wiederherstellung des Systems nach einem Fehler verwendet werden können.\n\nOft werden die Logdateien in Internetforen gestellt, wo erfahrenere Benutzer helfen zu entscheiden, welche Einträge entfernt werden müssen. Es existieren auch Werkzeuge, die die Logdateien analysieren und versuchen, sie automatisch zu bereinigen oder dem Benutzer diesbezügliche Vorschläge zu machen (siehe unter Weblinks). Die Verwendung dieser Werkzeuge wird jedoch manchmal als potentiell gefährlich für unerfahrene Benutzer sowie als zu unzuverlässig und zu ungenau kritisiert, um eine Analyse durch einen Menschen zu ersetzen.\n\nHijackThis wird nicht mehr weiterentwickelt und ist deshalb in Malwarebekämpfungsforen wie trojaner-board.de oder bleepingcomputer.com durch informativere und umfassendere Scanprogramme wie Farbar Recovery Scan Tool (FRST) ersetzt worden.\n\n"}
{"id": "4176788", "url": "https://de.wikipedia.org/wiki?curid=4176788", "title": "Rapunzel – Neu verföhnt", "text": "Rapunzel – Neu verföhnt\n\nRapunzel – Neu verföhnt (Originaltitel: \"Tangled;\" eng. für: Verheddert) ist ein US-amerikanischer Computeranimationsfilm von Nathan Greno und Byron Howard aus dem Jahr 2010. Er enthält Motive des Märchens \"Rapunzel\" der Brüder Grimm und gilt als der 50. Film der Disney Animation Studios im Meisterwerkekanon. Er wird mit Produktionskosten von mindestens 260 Millionen US-Dollar auf Platz zwei hinter Fluch der Karibik 3, dem teuersten Film aller Zeiten, gesehen. Inflationsbereinigt liegt er auf Platz 11 und somit gleichauf mit Filmen wie Cleopatra und Titanic. Demgegenüber steht ein weltweites Einspielergebnis von rund 570 Millionen US-Dollar.\n\nGothel ist eine sehr alte Hexe, die sich mithilfe einer magischen Blume regelmäßig verjüngt. Durch einen Zufall wird mit der Blume ein Heiltrank für die todkranke, schwangere Königin hergestellt. Die Königin wird gesund; die Verjüngungskraft geht dabei auf die kurz darauf geborene Rapunzel über. Beim Singen eines bestimmten Liedes beginnen ihre Haare zu leuchten und entfalten dieselbe Wunderkraft wie zuvor die Blume. Da die magische Wirkung nicht in den Haaren, sondern in Rapunzel selber liegt, entführt Gothel das Kind und zieht es in einem geheimen, einsamen Turm als eigene Tochter auf, um auch weiterhin die verjüngende Wirkung nutzen zu können.\n\nDer Dieb Flynn Rider, der zusammen mit den Brüdern Stabbington die Krone der verschwundenen Prinzessin gestohlen hat, betrügt seine Kumpane auf der Flucht vor der Palastwache und flieht in Rapunzels Turm, den er durch Zufall findet. Es gelingt der nun 17-jährigen Rapunzel, den Eindringling zu überwältigen und die Krone zu verstecken. Sie überredet den Fremden, ihr bei der Ergründung eines Himmelsphänomens zu helfen, wofür sie den Turm verlassen muss. Im Gegenzug erhält Rider die Krone zurück, deren Bedeutung Rapunzel unbekannt ist. Rapunzel schickt Gothel, die zwischendurch auftaucht und vor deren Augen Flynn Rider versteckt gehalten wird, unter einem Vorwand auf eine Drei-Tages-Reise in den Wald. Dann zieht sie zusammen mit Rider und dem Chamäleon Pascal (das ihr immer zur Seite steht) los, später auch begleitet von dem Pferd Maximus, das sich aber eher wie ein Hund verhält.\n\nDa das Himmelsphänomen, eigentlich Himmelslaternen, die vom Königspaar jährlich zu Rapunzels Geburtstag steigengelassen werden, vom königlichen Schloss ausgeht, fürchtet Rider den eigentlichen Zweck der Reise. Er führt Rapunzel in ein zwielichtiges Lokal, aus dem beide beim Auftauchen der Palastwache durch einen Geheimtunnel fliehen können. Auf der Flucht offenbart Rapunzel die Heilkraft ihrer Haare.\n\nGothel, die inzwischen die Flucht Rapunzels bemerkt und im Turm die Krone und einen Steckbrief Flynns gefunden hat, trifft auf der Suche nach Rapunzel die Stabbington-Brüder. Sie überredet die Brüder, ihr bei einem Plan zu helfen, durch den sie sich an Flynn Rider rächen können und der Rapunzel zurück in ihr Gewahrsam bringt. Zunächst versucht sie noch einmal erfolglos, Rapunzel zur Rückkehr zu bewegen. Gothel weist darauf hin, dass Rider nur hinter der Krone her sei. Damit Rapunzel es selber überprüfen kann, übergibt Gothel ihr die Krone.\n\nAm nächsten Tag betreten Rapunzel und Rider das Schloss. Am Abend schauen sie sich die Laternen vom See aus an. Bei dieser Gelegenheit übergibt Rapunzel die Krone. Rider sieht am Ufer die Stabbington-Brüder und will die Krone zurückgeben. Während Rapunzel im Boot wartet, schlagen die Brüder Rider bewusstlos, fesseln ihn an ein Segelboot und lassen ihn zusammen mit der Krone auf den Steg des Schlosses zutreiben. Rapunzel gegenüber beschreiben sie es als „Betrug“ Riders gegenüber Rapunzel. Als die Brüder versuchen, Rapunzel zu entführen, überwältigt Gothel die beiden und bringt Rapunzel zurück in den Turm. Währenddessen wird Rider zum Tode verurteilt.\n\nRider wird von den zwielichtigen Gestalten aus dem früher besuchten Lokal gerettet. Aufgrund verschiedener Hinweise, die sie während der Reise gesammelt hat, schließt Rapunzel, dass sie die gesuchte Prinzessin ist, und versucht, aus dem Turm zu fliehen. Gothel fesselt sie, und als Rider zur Rettung kommt, sticht Gothel ihn nieder. Rapunzel will seine Wunde heilen, was Gothel erst zulässt, als Rapunzel ihr im Gegenzug verspricht, danach mit ihr zu gehen und nie wieder zu fliehen. Bevor es dazu kommt, schneidet der sterbende Rider Rapunzels Haare ab, die daraufhin ihre magische Wirkung verlieren. Gothel altert wieder und gerät dabei so sehr außer sich, dass sie aus dem Fenster des Turmes stürzt und stirbt. Als Rider in Rapunzels Armen stirbt, weint sie. Die Tränen enthalten jedoch noch die magischen Fähigkeiten, eine trifft Riders Wange und belebt ihn wieder. Rapunzel und der geheilte Rider kehren in das Schloss zurück, wo sie feierlich empfangen werden und Jahre später heiraten.\n\nIm Jahr 2012 produzierte Disney den siebenminütigen Kurzfilm \"Rapunzel - Verföhnt, Verlobt, Verheiratet\" (engl. Originaltitel: \"Tangled Ever After\"), in dem die Hochzeit von Rapunzel und Rider dargestellt wird. Unmittelbar vor der Ringübergabe verlieren die Trauzeugen Maximus und Pascal die Eheringe, die die Schlosstreppen herab in die darunter liegende Stadt fallen. Die beiden Tiere hetzen hinterher und richten bei der Jagd einen immensen Schaden an, doch sie können den Eheleuten pünktlich die Ringe geben und sind selig − bis Maximus unabsichtlich die Hochzeitstorte anstößt und sie wiederum die Schlosstreppe herabfällt.\n\n\"Rapunzel – Neu verföhnt\" ist der 50. Disney-Animationsfilm. Er entstand als 3D-Computeranimation.\n\nDer Film erlebte seine US-Premiere am 24. November 2010. In Deutschland kam er am 9. Dezember 2010 in die Kinos. In Österreich wurde er wegen des gesetzlichen Feiertags bereits am 8. Dezember gestartet.\n\nDer optische Stil des Films orientiert sich an Gemälden des im 18. Jahrhundert lebenden französischen Rokoko-Künstlers Jean-Honoré Fragonard, insbesondere an dem Gemälde \"Die Schaukel\". Dieser Stil wird als „romantisch und üppig“ bezeichnet.\n\nAnimation Supervisor Glen Keane wollte diesen Film wie einen traditionellen, handgezeichneten Disney-Klassiker in 3D wirken lassen. Aus diesem Grund führte er ein Seminar mit dem Namen „The Best of Both Worlds“ (engl. für „Das Beste beider Welten“) durch, bei dem er mit 50 Animatoren die Vor- und Nachteile von CGI-Animationen und Handzeichnungen diskutierte und bei dem darüber gesprochen wurde, wie man die Stärken beider Genres in einem Film vereinen könnte. Der Film wurde sowohl von klassischen Zeichnern als auch von Computeranimatoren entwickelt. Um den Film nach Keanes Vorstellungen zu gestalten, wurden speziell für diesen Film neue Techniken entwickelt. Die handgezeichneten Figuren wurden mit computeranimierten 3D-Bildern gefüllt. Um die Wirkung einer Zeichnung zu erzeugen, verwendete man nicht-fotorealistische Render-Techniken, was die Oberfläche wie gemalt aussehen lässt und trotzdem Tiefe und Dimensionen erzeugt.\n\nDie Liedauswahl im Abspann wurde zum Teil den Ländern angepasst, in denen der Film jeweils aufgeführt wird. So wird in der deutschen Version während der Abschlusstitel nach \"Something That I Want\" von Grace Potter \"Endlich sehe ich das Licht\" von Monrose gespielt.\n\nDie musikalische Leitung der Aufgabe, die Originaltexte ins Deutsche einzusingen, übernahm Thomas Amper. Die weiteren Synchronarbeiten fanden bei der FFS Film- und Fernseh-Synchron GmbH statt, Katrin Fröhlich schrieb das Dialogbuch und führte die Dialogregie. Der ursprüngliche Name des Diebes „Bastion“ wurde im aktuellen Drehbuch in „Flynn“ abgeändert. Außerdem wurden alle Hauptrollen umbesetzt, so dass Mandy Moore, Zachary Levi und Donna Murphy die ursprünglich als Sprecher angekündigten Kristin Chenoweth, Dan Fogler und Grey DeLisle ersetzten.\n\n2016 produzierte Disney die gleichnamige Serie nebst dem Pilotfilm \"Rapunzel - Für immer verföhnt\". Seit 2017 läuft diese in Disney Channel. Eine zweite Staffel hat Disney bereits vor Start der Serie in Auftrag gegeben. \n\n\"Rapunzel – Neu verföhnt\" war 2011 für einen Golden Globe Award in den Kategorien bester Filmsong \"(I See the Light)\" und bester Animationsfilm nominiert, konnte aber keinen der beiden Preise gewinnen. Im selben Jahr folgte eine Oscar-Nominierung für \"I See the Light\".\n\nBei den Tokyo Anime Awards 2012 wurde der Film als bester ausländischer Animationsfilm ausgezeichnet.\n\n"}
{"id": "4193276", "url": "https://de.wikipedia.org/wiki?curid=4193276", "title": "RUN (Zeitschrift)", "text": "RUN (Zeitschrift)\n\nRUN war eine von Juni 1984 bis Januar 1988 erschienene Computerzeitschrift.\n\nDas nach dem BASIC-Befehl für einen Programmstart benannte, nach eigenen Angaben \"unabhängige Commodore Computermagazin\" erschien monatlich, die Erstausgabe kostete 4,50 D-Mark (heute inflationsbereinigt €). Einen Monat nach der Einstellung gab die gleiche Redaktion das Computermagazin „Amiga-Welt“ heraus.\nBeide Magazine wurden vom \"CW-Publikationen Verlag\" veröffentlicht, einem Tochterverlag des IDG Verlag.\n\nDie RUN beinhaltete Informationen zu Hard- und Software (auch Spiele), Unterhaltung, Tipps und Tricks, Programmierkurse sowie Programmlistings zu verschiedenen Commodore-Computersystemen wie C16, VC20, C116, C128, Plus/4 oder dem Amiga, wobei der Commodore 64 den Schwerpunkt darstellte. Zur Erleichterung des Abtippens der Programmlistings gab es ab 1985 den „Superkorrektor“ für BASIC-Listings (für die hier erwähnten Heimcomputer), sowie den „Checker für Maschinenprogramme“.\n\nVon 1984 bis Dezember 1992 gab es auch eine eigenständige englischsprachige RUN, die vom Verlag CWC Publication bzw. später von IDG Communicators in Nordamerika und Großbritannien herausgegeben wurde. Sie erschien zunächst monatlich und in den letzten zwei Jahren zweimonatlich.\n\n"}
{"id": "4195982", "url": "https://de.wikipedia.org/wiki?curid=4195982", "title": "RAMSIS", "text": "RAMSIS\n\nRAMSIS ist ein 3D-Menschmodell in Form einer Software zur ergonomischen Analyse von CAD-Konstruktionen. Der Begriff RAMSIS ist ein Akronym für die Bezeichnung Rechnergestütztes Anthropometrisch-Mathematisches System zur Insassen-Simulation.\n\nRAMSIS dient zur ergonomischen Analyse und der Auslegungsunterstützung von Produkten und Arbeitsplätzen während der Konstruktionsphase auf Basis von CAD-Geometrie. Es ergänzt die dreidimensionalen Konstruktionsmodelle von Bauteilen mit einem skalierbaren funktionalen geometrischen Modell des Menschen. Auf dieser Grundlage ist es möglich, die jeweilige Mensch-Maschine-Schnittstelle eines späteren technischen Produktes detailliert zu überprüfen. RAMSIS repräsentiert dabei die Eigenschaften und Bedürfnisse der unterschiedlichen Menschen im Umgang mit technischen Produkten.\nDies kann bereits vor der Anfertigung eines ersten 1:1-Modelles oder eines Prototyps in der konstruktiven Entstehungsphase im Computer erfolgen. Der Einsatz von RAMSIS erfolgt vor allem in der Entwicklung aller Arten von Luft- und Boden-Fahrzeugen und Baumaschinen. Die besondere Domäne von RAMSIS ist die Überprüfung der Insassenbedingungen von Fahrzeugen bei der Benutzung durch die Fahrer und Passagiere. Im Bereich der Produktentwicklung von Pkw stellt RAMSIS quasi einen Standard für die Ergonomieauslegung dar und wird von ca. 90 % der Hersteller weltweit eingesetzt. Die Auslegung von Arbeitsplätzen in Produktion und Büro ist ein erweitertes Einsatzgebiet.\n\nRAMSIS kann das Manikin entsprechend den realen Körperabmessungen skalieren und nutzt dazu eine multidimensionale statistische Datenbank. Damit ist es nicht nur möglich, die ergonomisch relevanten Körperabmessungen der Frauen, Männer und Kinder hinsichtlich einer sehr kleinen und sehr großen Körperlänge wiederzugeben. Auch die charakteristischen Proportionsverhältnisse, welche unter den Begriffen „Sitzriese“ und „Sitzzwerg“ bekannt sind, sowie die Korpulenzvariationen kann RAMSIS geometrisch darstellen. Das Zusatzmodul ‚RAMSIS Body Builder’ ermöglicht eine detaillierte multidimensionale Einstellung von 18 Körpermaßen innerhalb der statistischen Grenzen. Da die Körpermaße einer Bevölkerung einer Zunahme von durchschnittlich etwa 10 mm pro Dekade unterliegen, der sogenannten säkularen Akzeleration, können die Körpermaße für die Zukunft statistisch prognostiziert werden.\n\nRAMSIS besitzt ein anatomisches Beweglichkeitsmodell für seine Körpergelenke und ein Haltungsmodell, das speziell die eingenommene Körperhaltung im jeweiligen Fahrzeug nach Komfortaspekten simulieren und bewerten kann. Auf dieser Grundlage können mit RAMSIS die Positionen und Körperhaltungen, der Raumbedarf, die Erreichbarkeiten, der Bewegungsraum, die Betätigungen, die Handhabungskräfte und die Sicht prognostiziert werden. Im Modul 'RAMSIS kognitiv' ermöglichen mehrere Funktionen für die Simulation der Sichtbedingungen des Menschen Prognosen der visuellen Wahrnehmung.\n\nDie Industrie nutzt diese Ergonomie-Software intensiv um die ergonomische Gestaltung eines Produktes leichter, schneller und kostengünstiger durchzuführen, als dies mit iterativer Optimierung eines Prototyps mittels Überprüfung durch Versuchspersonen erfolgen kann. RAMSIS existiert nicht nur als eigenständiges Stand-alone-Computerprogramm, sondern auch als Modul eingebettet in das CAD-System CATIA V5. Dadurch kann die Notwendigkeit von aufwändigen Datenkonvertierungen bei rekursiven Analyse- und Konstruktionsfolgen reduziert werden. RAMSIS ist als Analyse- und Simulationsprogramm dem computer-aided engineering (CAE) zuzurechnen, obwohl es unmittelbar gemeinsam mit CAD eingesetzt wird.\n\nRAMSIS wurde in den Jahren 1987 bis 1994 im Rahmen eines Forschungsprojektes der Forschungsvereinigung Automobiltechnik (FAT) des Verband der Automobilindustrie (VDA) entwickelt. Anlass für die Entwicklung digitaler Menschmodelle seit Ende der 1960er und vor allem in den 1970er Jahren war der Bedarf für die Konstruktion komplexer Produkte, wie der eines Fahrzeugs, auch innerhalb der CAD-Konstruktionsumgebung ein adäquates Auslegungswerkzeug für ergonomische Belange einzusetzen. Die bis dahin verwendeten zweidimensionalen Körperumrissschablonen wie die SAE-Schablone oder die „Kieler Puppe“ erfüllten diesen Anspruch nicht.\n\nDie wissenschaftliche Erarbeitung der ergonomischen Grundlagen des Systems erfolgte überwiegend am Lehrstuhl für Ergonomie (LfE) der Technischen Universität München (TUM) unter Professor Heiner Bubb. Die Softwareentwicklung und der internationale Vertrieb erfolgt durch die Firma Human Solutions GmbH in Kaiserslautern (vormals Tecmath AG).\n\nEigentümer von RAMSIS wurde nach Abschluss des FAT-Projektes ein Konsortium der deutschen Automobilindustrie bestehend aus den Herstellerfirmen AUDI AG, BMW AG, Daimler AG, Ford-Werke GmbH, Adam Opel GmbH, Dr. Ing. h. c. F. Porsche AG, Volkswagen AG sowie des Automobilzulieferers Johnson Controls (JCI Beteiligungsgesellschaft GmbH).\n\nNach Abschluss des Forschungsprojektes wurde RAMSIS in ein kommerzielles Produkt überführt und wird seit 2002 durch die Human Solutions GmbH als Simulationssoftware auch anderen Nutzern außerhalb der deutschen Automobilindustrie angeboten. Seitdem erfährt RAMSIS kontinuierlich eine weitere Verbreitung auf der ganzen Welt. Die funktionale Entwicklung ging ebenfalls stetig weiter. Spezialfunktionen für die unterschiedlichsten Anwendungsfälle führen zu einer andauernden Erweiterung der Software.\n\nSeit dem Jahr 2007 wird der RAMSIS Excellence Award vergeben, ein wissenschaftlicher Preis für Fahrzeug-Ergonomie, der nach dem Menschmodell benannt ist.\n\n\n"}
{"id": "4200477", "url": "https://de.wikipedia.org/wiki?curid=4200477", "title": "CuneiForm", "text": "CuneiForm\n\nCuneiForm (engl. für \"keilschriftlich\") ist eine Texterkennungssoftware für gedruckte Erkennungsvorlagen des russischen Unternehmens \"Cognitive Technologies\", die mittlerweile als freie Software verfügbar ist.\n\nCuneiForm erkennt gedruckte Vorlagen, jedoch keine Handschrift oder ähnliches, mit Sprachmodellen für über 20 verschiedene Sprachen. Gut funktioniert auch die Erkennung komplizierter Tabellenstrukturen. Ergebnisse können in RTF, HTML oder als ASCII-Text gespeichert oder direkt an die Textverarbeitung \"Word\" oder die Tabellenkalkulation \"Excel\" exportiert werden. Es erhält Dokumentstruktur und Schriftarten und ermöglicht Stapelverarbeitung.\n\nCuneiForm war einst marktführend in Russland (in Konkurrenz zu FineReader des Unternehmens ABBYY) und wurde mit einigen Scannern mitgeliefert.\n\n1993 ging Cognitive Technologies einen OEM-Vertrag mit der kanadischen Corel Corporation ein, der die Einbindung der Erkennungsbibliothek in das Corel-Draw-Paket erlaubte, das ab Version 3.0 diese enthielt.\n\n1996 wurde OCR CuneiForm'96 veröffentlicht. Es war das erste Texterkennungspaket, das mit einer adaptiven Erkennungsmethode arbeitete, d. h. einer Methode, die Multifont- und Omnifont-Erkennung verbindet: Es erfolgt eine interne Nachbildung der in der Erkennungsvorlage verwendeten Fonts (engl. für \"Schriftarten\") aus Zeichen, die in erkennbarer Qualität abgebildet sind. Dadurch wird im Anschluss auch die Erkennung von schlechter abgebildeten Zeichen möglich, da sich die Software bei der Erkennung dynamisch anpasst. Mit dieser Erkennungsmethode wird die Erkennungsgenauigkeit wesentlich gesteigert.\n\n1997 wurde die Nutzung neuronaler Netze bei der Erkennung eingeführt.\n\nSeit 1999 kann die Software das Aussehen der Vorlage erhalten, indem die Anordnung der Elemente in der Ausgabe nachgebaut wird.\n\nIm Rahmen eines Programmes, das erklärtermaßen Texterkennungstechnologie für jedermann verfügbar machen soll, hat Cognitive Technologies am 2. April 2008 angekündigt, die Software letzten Endes komplett als freie Software verfügbar zu machen. Als erster Schritt wurde nach einigen Jahren ohne Entwicklungsfortschritte am 12. Dezember 2007 eine Freeware-Version veröffentlicht.\nWeiterhin wurde im Juni 2008 ein kostenloser Texterkennungsdienst im World Wide Web eingerichtet.\n\nAls Investor und Projektkoordinator will Cognitive Technologies die Entwicklung einer neuen Version der Software fördern.\nSeit Anfang April 2008 ist der Kern der Erkennungsengine unter der vereinfachten BSD-Lizenz frei verfügbar, um auch eine kommerzielle Verwendung zu ermöglichen. Am 30. August 2009 wurde auch die originale Benutzerschnittstelle offengelegt.\n\nJussi Pakkanen hat eine plattformunabhängig kompilierbare Version der Software erstellt, die auf Linux, BSD, macOS und Windows läuft. Diese unabhängigen Entwicklungen sollen schließlich in den Hauptzweig von Cognitive Technologies integriert werden. Es handelt sich um eine reine Kommandozeilen-Version, die mittels der Einbindung von ImageMagick das Lesen einer Vielzahl von Dateiformaten erlaubt, während sonst einzig unkomprimiertes Windows Bitmap (BMP) unterstützt wird.\nAb Version 0.5 kann die Software auch in die Beschreibungssprache hOCR ausgeben.\n\n\nMittels eines Skripts (\"xsane2cunei\") kann CuneiForm auch in die Scan-Software \"XSane\" eingebunden werden. Aus der hOCR-Ausgabe von CuneiForm können mittels des Kommandozeilenprogrammes \"hocr2pdf\" Bilder-PDF-Dateien maschinell durchsuchbar gemacht werden. Die Kommandozeilenwerkzeuge pdfsandwich oder pdfocr automatisieren diesen Prozess.\nAuch das Dokumentenmanagementsystem \"Archivista\" macht mittels CuneiForm und hocr2pdf PDFs maschinell durchsuchbar.\n\n"}
