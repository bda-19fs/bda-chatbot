{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config IPCompleter.greedy=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'ionesoft_tickets.csv'\n",
    "encoding = 'ansi'\n",
    "sep = ','"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticket</th>\n",
       "      <th>type</th>\n",
       "      <th>client</th>\n",
       "      <th>issuer</th>\n",
       "      <th>inquiry</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.019031e+15</td>\n",
       "      <td>Fehler in der App</td>\n",
       "      <td>SVBA</td>\n",
       "      <td>3</td>\n",
       "      <td>ich kann die bilder in übungen nicht bearbeite...</td>\n",
       "      <td>Fehlerbericht senden</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.019030e+15</td>\n",
       "      <td>Fehler in der App</td>\n",
       "      <td>FOMA</td>\n",
       "      <td>3</td>\n",
       "      <td>lückentexte weg daten verloren gegangen von pp...</td>\n",
       "      <td>Fehlerbericht senden</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.019023e+15</td>\n",
       "      <td>Anmeldung / Aktivierung</td>\n",
       "      <td>AGVS</td>\n",
       "      <td>3</td>\n",
       "      <td>bitte deaktivieren sie dieses gerät. vielen dank.</td>\n",
       "      <td>Gerät entfernt, neu starten</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.019023e+15</td>\n",
       "      <td>Geräteanzahl überschritten</td>\n",
       "      <td>BEOOK</td>\n",
       "      <td>3</td>\n",
       "      <td>ich habe einen neuen laptop und möchte den alt...</td>\n",
       "      <td>Gerät entfernt, neu starten</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.019023e+15</td>\n",
       "      <td>Geräteanzahl überschritten</td>\n",
       "      <td>BEOOK</td>\n",
       "      <td>3</td>\n",
       "      <td>offenbar habe ich die geräteanzahl überschritt...</td>\n",
       "      <td>Gerät entfernt, neu starten</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ticket                        type client  issuer  \\\n",
       "0  2.019031e+15           Fehler in der App   SVBA       3   \n",
       "1  2.019030e+15           Fehler in der App   FOMA       3   \n",
       "2  2.019023e+15     Anmeldung / Aktivierung   AGVS       3   \n",
       "3  2.019023e+15  Geräteanzahl überschritten  BEOOK       3   \n",
       "4  2.019023e+15  Geräteanzahl überschritten  BEOOK       3   \n",
       "\n",
       "                                             inquiry  \\\n",
       "0  ich kann die bilder in übungen nicht bearbeite...   \n",
       "1  lückentexte weg daten verloren gegangen von pp...   \n",
       "2  bitte deaktivieren sie dieses gerät. vielen dank.   \n",
       "3  ich habe einen neuen laptop und möchte den alt...   \n",
       "4  offenbar habe ich die geräteanzahl überschritt...   \n",
       "\n",
       "                        answer  \n",
       "0         Fehlerbericht senden  \n",
       "1         Fehlerbericht senden  \n",
       "2  Gerät entfernt, neu starten  \n",
       "3  Gerät entfernt, neu starten  \n",
       "4  Gerät entfernt, neu starten  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "extract = 'ionesoft_tickets_corrected_grammar.csv'\n",
    "names=['ticket', 'type', 'client', 'issuer', 'inquiry', 'answer']\n",
    "\n",
    "corpus = pd.read_csv(file, sep=sep, encoding=encoding, names=names, skiprows=[0])\n",
    "corpus.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pickle dump column as flatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = 'inquiries.p'\n",
    "col = 'inquiry'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "column = corpus[col].values.flatten().tolist()\n",
    "pickle.dump(column, open(out, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default NLP Pipeline\n",
    "1. Normalisation\n",
    "1. Tokenisation\n",
    "1. Remove stop words\n",
    "1. Stemming / Lemming\n",
    "1. TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "\n",
    "def normalize_document(doc):\n",
    "    double_spaces = r' +'\n",
    "    not_in_swiss_alphabet = r'[^\\u00C0-\\u017Fa-zA-Z\\s]'\n",
    "    \n",
    "    doc = re.sub(not_in_swiss_alphabet, ' ', doc, re.I | re.A)\n",
    "    doc = re.sub(double_spaces, ' ', doc, re.I | re.A)\n",
    "    \n",
    "    doc = doc.lower()\n",
    "    return doc.strip()\n",
    "\n",
    "normalize = np.vectorize(normalize_document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lückentexte acht aber weg daten tester terterin tester in verloren gegangen'\n",
      " 'äöü éàè']\n"
     ]
    }
   ],
   "source": [
    "test = [\n",
    "    'lückentexte acht aber weg daten tester/terterin tester/-in verloren gegangen',\n",
    "    'äöü!#@éàè'\n",
    "]\n",
    "\n",
    "test = normalize(test)\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_doc(doc):\n",
    "    return [sent.split(' ') for sent in doc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['lückentexte', 'acht', 'aber', 'weg', 'daten', 'tester', 'terterin', 'tester', 'in', 'verloren', 'gegangen'], ['äöü', 'éàè']]\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenize_doc(test)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.  Remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(tokens, stop_words):\n",
    "    return [token for token in tokens if token not in stop_words]\n",
    "\n",
    "def remove_stopwords_doc(token_doc, stop_words):\n",
    "    return list(map(lambda l: remove_stopwords(l, stop_words), token_doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test stop word removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['lückentexte', 'daten', 'tester', 'terterin', 'tester', 'verloren', 'gegangen'], ['äöü', 'éàè']]\n"
     ]
    }
   ],
   "source": [
    "stop_words = pickle.load(open('./res/custom_ch_stopwords.p', 'rb'))\n",
    "\n",
    "clean = remove_stopwords_doc(tokens, stop_words)\n",
    "print(clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. stemming / lemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "def stemm(tokens, stemmer):\n",
    "    return [stemmer.stem(token) for token in tokens]\n",
    "    \n",
    "def stemm_doc(token_doc, stemmer):\n",
    "    return list(map(lambda l: stemm(l, stemmer), token_doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['luckentext', 'dat', 'test', 'terterin', 'test', 'verlor', 'gegang'], ['aou', 'éàè']]\n"
     ]
    }
   ],
   "source": [
    "snowball = SnowballStemmer('german', ignore_stopwords=True)\n",
    "stemmed = stemm_doc(clean, snowball)\n",
    "print(stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
