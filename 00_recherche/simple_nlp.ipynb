{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import tokenize\n",
    "from nltk import tag\n",
    "from nltk import chunk\n",
    "\n",
    "satz = 'Ich bin ein Student an der Hochschule Luzern. Ich liebe IT.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ich bin ein Student an der Hochschule Luzern.', 'Ich liebe IT.']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize.sent_tokenize(satz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ich',\n",
       " 'bin',\n",
       " 'ein',\n",
       " 'Student',\n",
       " 'an',\n",
       " 'der',\n",
       " 'Hochschule',\n",
       " 'Luzern',\n",
       " '.',\n",
       " 'Ich',\n",
       " 'liebe',\n",
       " 'IT',\n",
       " '.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_token = tokenize.word_tokenize(satz)\n",
    "word_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Ich', 'NNP'),\n",
       " ('bin', 'NN'),\n",
       " ('ein', 'NN'),\n",
       " ('Student', 'NNP'),\n",
       " ('an', 'DT'),\n",
       " ('der', 'NN'),\n",
       " ('Hochschule', 'NNP'),\n",
       " ('Luzern', 'NNP'),\n",
       " ('.', '.'),\n",
       " ('Ich', 'NNP'),\n",
       " ('liebe', 'VBD'),\n",
       " ('IT', 'NNP'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged_pos = tag.pos_tag(word_token)\n",
    "tagged_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (GPE Ich/NNP)\n",
      "  bin/NN\n",
      "  ein/NN\n",
      "  Student/NNP\n",
      "  an/DT\n",
      "  der/NN\n",
      "  (PERSON Hochschule/NNP Luzern/NNP)\n",
      "  ./.\n",
      "  (PERSON Ich/NNP)\n",
      "  liebe/VBD\n",
      "  IT/NNP\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "tree = chunk.ne_chunk(tagged_pos)\n",
    "print(tree)\n",
    "tree.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p: ich\n",
      "f: ich\n",
      "s: ich\n",
      "p: bin\n",
      "f: bin\n",
      "s: bin\n",
      "p: ein\n",
      "f: ein\n",
      "s: ein\n",
      "p: student\n",
      "f: stud\n",
      "s: student\n",
      "p: an\n",
      "f: an\n",
      "s: an\n",
      "p: der\n",
      "f: der\n",
      "s: der\n",
      "p: hochschul\n",
      "f: hochsch\n",
      "s: hochschul\n",
      "p: luzern\n",
      "f: luzern\n",
      "s: luz\n",
      "p: .\n",
      "f: .\n",
      "s: .\n",
      "p: ich\n",
      "f: ich\n",
      "s: ich\n",
      "p: lieb\n",
      "f: lieb\n",
      "s: lieb\n",
      "p: IT\n",
      "f: it\n",
      "s: it\n",
      "p: .\n",
      "f: .\n",
      "s: .\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "porter = PorterStemmer()\n",
    "lancaster = LancasterStemmer()\n",
    "snowball = SnowballStemmer('german', ignore_stopwords=True)\n",
    "\n",
    "for t in word_token:\n",
    "    print(f'p: {porter.stem(t)}')\n",
    "    print(f'f: {lancaster.stem(t)}')\n",
    "    print(f's: {snowball.stem(t)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['einiges', 'allen', 'auch', 'Ã¼ber', 'meinen']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = list(set(stopwords.words('german')))\n",
    "stop_words[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ich', 'Student', 'Hochschule', 'Luzern', '.', 'Ich', 'liebe', 'IT', '.']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[t for t in word_token if not t in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
